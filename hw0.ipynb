{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihIqEIPtHQK6"
      },
      "source": [
        "Welcome to **Homework 0**, interactive edition! In this assignment, we will introduce you to PyTorch and review dynamic programming.\n",
        "\n",
        "Throughout 6.864, we will use **Google Colab** for the programming and modeling parts of homework assignments. Colab notebooks are just like regular IPython notebooks, but they are executed on Google servers instead of on your local machine. The nice part about this is that Colab offers free GPU support, allowing you to quickly edit-refresh-run  machine learning code without your own CUDA-compatible GPU.\n",
        "\n",
        "**We suggest you run this homework assignment on a GPU.** To do so, follow these steps:\n",
        "1. Under the \"Runtime\" menu, select \"Change runtime type\"\n",
        "1. In the \"Hardware accelerator\" dropdown select \"GPU\"\n",
        "1. Rerun any cells you ran previously in the notebook.\n",
        "\n",
        "Then you're good to go! The next two cells install some dependencies for this project and configure it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSlk4f-UkW8c"
      },
      "source": [
        "!pip install torch matplotlib seaborn sklearn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mj30yTFUBeVv"
      },
      "source": [
        "import torch\n",
        "from torch import cuda\n",
        "\n",
        "if cuda.is_available():\n",
        "  device = 'cuda'\n",
        "else:\n",
        "  print('WARNING: you are running this assignment on a cpu!')\n",
        "  device = 'cpu'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUlQZU-K5poX"
      },
      "source": [
        "# Part 1: PyTorch\n",
        "\n",
        "We'll first cover the basics of [PyTorch](https://pytorch.org/), a popular machine learning framework. All programming assignments in this class will use PyTorch for modeling and optimization. Before you proceed with this section, we strongly suggest you complete the [60 minute deep learning tutorial](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html) provided by the PyTorch developers. That tutorial is relatively comprehensive; we'll cover only the most essential concepts from the library.\n",
        "\n",
        "A simple way to think about PyTorch is that it's numpy with extra stuff. The most important of the \"extra stuff\" is **automatic differentiation**: PyTorch can keep track of all the computations performed on data and can automatically compute the gradient of any output (e.g., the value of an objective function) with respect to any of the inputs (usually, the model parameters). This makes optimization easy, as all the user (that's you!) has to do is define the model, feed it inputs, and compute the objective function.\n",
        "\n",
        "We'll walk through how to do each of these things for a simple **linear regresion model**. Obviously, this model won't have many moving parts. The focus here is to expose you to the library and the classes and functions that you'll use all the time.\n",
        "\n",
        "Toward that end, the exercises that follow will show you how to:\n",
        "1. represent and manipulate data using **tensors**,\n",
        "1. define a model as a torch **module**,\n",
        "1. and optimize the model using **automatic differentiation**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUu0zTFq6UHr"
      },
      "source": [
        "## Loading and preprocesing the data\n",
        "\n",
        "Let's start by loading some data. To keep things simple, we'll use the tiny  **Iris Dataset** (Fisher 1936), which contains 150 samples of iris flower measurements. The four measurements are sepal height, sepal width, petal height, and petal width. Each iris falls under one of three categories: setosa, virginica, and versicolor.\n",
        "\n",
        "This dataset is small enough that it is built into the scikit-learn library. We'll grab it from there."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6G32HYEO2z4"
      },
      "source": [
        "from sklearn import datasets\n",
        "\n",
        "A, b = datasets.load_iris(return_X_y=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sDmm-OCpNIo"
      },
      "source": [
        "The data we just loaded is stored in a numpy array. The `A` array contains the flower measurements and hence has size $150 \\times 4$. The `b` contains the 150 class labels encoded as integers: 0 means setosa, 1 means virginica, and 2 means versicolor.\n",
        "\n",
        "We need to convert this data into a format that Torch understands.\n",
        "Torch talks about data in terms of **tensors.** [Tensors](https://pytorch.org/docs/stable/tensors.html) are very similar to numpy arrays--in fact, many of the operations you can perform on Tensors are exactly the same as operations you can perform on numpy arrays. You'll want to familiarize yourself with  Tensor methods because you'll be using them a lot.\n",
        "\n",
        "Importantly, to run computations with a tensor on a GPU, you'll have to move the tensor data to the GPU. You can do this right away when you construct the tensor by specifying the `device='cuda'` keyword argument, or you can call `my_tensor.to('cuda')`.\n",
        "\n",
        "Your first task is to construct and perform some basic manipulations on tensors.\n",
        "\n",
        "#### **TASK 1**\n",
        "1. Convert `A` and `b` to floating point tensors and move them to `device` (see top of notebook).\n",
        "1. Drop the last feature stored in `A`.\n",
        "1. Append a column of ones to the end of `A`. This will allow our model to have a bias term.\n",
        "1. Transform `b` to have values in $\\{-1, 0, 1\\}$ instead of in $\\{0, 1, 2\\}$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0xbBZl0pTGx"
      },
      "source": [
        "# TODO: Your code here."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zguf8sPU6d_B"
      },
      "source": [
        "## Linear regression using matrix math\n",
        "\n",
        "Our next task is to classify iris type based on their measurements. To do this, we'll perform **least squares regression** to find a linear separator between the points.\n",
        "\n",
        "Let $A$ be the matrix of measurements and $b$ be the vector of class labels as in the previous section. In least squares regression, we solve the following optimization problem:\n",
        "$$\n",
        "\\hat{x} = \\underset{x}{\\textrm{argmin}}||Ax - b||_2^2\n",
        "$$\n",
        "\n",
        "Solving this optimization problem amounts to finding a linear separator between points of each class. Luckily, least squares is simple enough that there is a closed form solution, given by the **normal equations**:\n",
        "\n",
        "$$A^\\top A\\hat{x} = A^\\top b$$\n",
        "\n",
        "When $A$ is overconstrained--as it is here--then $A^\\top A$ is invertible. Go ahead and compute $\\hat{x}$ using the normal equations below. This won't require any fancy PyTorch tools, just matrix operations. Later on, we'll compare this solution to one we find using gradient descent.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LET6-QaonX_"
      },
      "source": [
        "#### **TASK 2**\r\n",
        "\r\n",
        "Using Tensor methods only, compute the solution to the normal equations below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Kmx5mjFU1fC"
      },
      "source": [
        "# TODO: Your code here."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiQbxRKG3OEe"
      },
      "source": [
        "The 4-dimensional vector $\\hat{x}$ defines a 3D plane that separates the data points by class. As a sanity check, let's visualize that plane.\n",
        "\n",
        "The helper function below plots all the data points on the same grid as the plane. Go ahead and call the helper function on your solution from above. You should see one class of flower _below_ the plane, one _on_ the plane, and another _above_ the plane."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNKnTUpcqlGA"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "sns.set()\n",
        "\n",
        "def visualize(x):\n",
        "  \"\"\"Plot the linear separator alongside the data points.\"\"\"\n",
        "  fig = plt.figure()\n",
        "  ax = fig.gca(projection='3d')\n",
        "\n",
        "  # Plot the data points.\n",
        "  for start, end, color, marker in (\n",
        "      (0, 50, 'b', 'x'),\n",
        "      (50, 100, 'k', 'o'),\n",
        "      (100, 150, 'r', 'v')):\n",
        "    ax.scatter(\n",
        "        A[start:end, 0].cpu(),\n",
        "        A[start:end, 1].cpu(),\n",
        "        A[start:end, 2].cpu(),\n",
        "        c=color,\n",
        "        marker=marker)\n",
        "\n",
        "  fig.legend(('setosa', 'virginica', 'versicolor'))\n",
        "  fig.tight_layout()\n",
        "\n",
        "  # Compute a bunch of plane (x, y, z) points.\n",
        "  min_x, min_y, *_ = A.min(dim=0).values.tolist()\n",
        "  max_x, max_y, *_ = A.max(dim=0).values.tolist()\n",
        "  xs, ys = torch.meshgrid(\n",
        "      torch.linspace(min_x, max_x, 100),\n",
        "      torch.linspace(min_y, max_y, 100))\n",
        "\n",
        "  w_x, w_y, w_z, bias = x.squeeze().tolist()\n",
        "  zs = (-bias - w_x * xs - w_y * ys) / w_z\n",
        "\n",
        "  ax.plot3D(\n",
        "      xs.flatten().numpy(),\n",
        "      ys.flatten().numpy(),\n",
        "      zs.flatten().numpy(),\n",
        "      alpha=.5,\n",
        "      color='y')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNgZ7ad9yYL5"
      },
      "source": [
        "# TODO: Your code here."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fEOEchDk2OI"
      },
      "source": [
        "How good is our separating plane as a classifier? Let's measure its classification accuracy on the training data. Given a data point $a$, we will compute the classifier's prediction for the $i$-th sample as\n",
        "\n",
        "$$\\hat{b_i} = \\textrm{round}(A_{i} x)$$\n",
        "\n",
        "where $A_i$ is the $i$-th row of $A$ and the $\\textrm{round}$ function rounds its input to the nearest integer. We treat the classification as correct if $\\hat{b}_i = b_i$.\n",
        "\n",
        "#### **TASK 3**\n",
        "Fill in the function below so that it computes the accuracy of $\\hat{x}$ on $A$. Call the function on the $\\hat{x}$ you computed in Task 2. You should obtain > 95% accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4uk68Aasoqu"
      },
      "source": [
        "# TODO: Your code here.\n",
        "def accuracy(x):\n",
        "  \"\"\"Return the accuracy of the linear separator x on the training data.\"\"\"\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePJu4fyE7PPY"
      },
      "source": [
        "## Linear regression using SGD\n",
        "\n",
        "Look, I would love to live in a world where every machine learning problem looks like least squares regression. But in reality, most models do not have a closed form solution. Instead, they require some kind of iterative optimization. Let's pretend for a second that least squares falls under that umbrella and recompute $\\hat{x}$ using **stochastic gradient descent**.\n",
        "\n",
        "The first step is to define the model we want to optimize. The term for \"something you can optimize\" in PyTorch is a **module**. At a high level, modules (a) store the parameters of the model and (b) know how to map inputs to outputs.\n",
        "\n",
        "To define a torch module, you must create a class that inherits from `torch.nn.Module`. The class should have an `__init__` function, which initializes any submodules or parameters, and a `forward` function, which takes any number of arguments and performs the computation, returning the result. We provide a skeleton for the linear regression module below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9MXhhIkottM"
      },
      "source": [
        "#### **TASK 4**\r\n",
        "\r\n",
        "Fill in the LinearRegressionModel class and instantiate it.\r\n",
        "\r\n",
        "**Hint**: You should only need to add two lines of code to the class. Check out the `torch.nn.Linear` module, and remember we already accounted for a bias term when we augmented $A$ it Task 1, so you'll want to set `bias=False`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6zDoQT4-0VJ"
      },
      "source": [
        "from torch import nn\n",
        "\n",
        "class LinearRegressionModel(nn.Module):\n",
        "\n",
        "  def __init__(self, n_features):\n",
        "    super().__init__()\n",
        "    # TODO: Your code here.\n",
        "  \n",
        "  def forward(self, features):\n",
        "    # TODO: Your code here.\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwZYyP-DoBmN"
      },
      "source": [
        "Now PyTorch knows about our model. Implicitly, torch also knows what **learnable parameters** belong to our model--if you used the `torch.nn.Linear` module in your implementation, the sole parameter is the weight vector stored in `the_linear_module.weight.data`. But I don't have to tell you this: you can obtain a dictionary mapping parameter name to value by calling `model.parameters()`.\n",
        "\n",
        "When we feed inputs to our model, we call it like a function: `outputs = model(inputs)`. Under the hood, torch will record the operations (such as matrix multiplication, vector addition, etc.) that were used to create `outputs` from `inputs`. This allows torch to compute gradients, which will in turn let us perform gradient descent on the parameters of the model.\n",
        "\n",
        "Before we can do that, though, we need an objective function, since the gradient we want to compute is the gradient of the _loss_ with respect to the _parameters_. In this case, our objective is the least squares error shown before Task 3. And surprise! It's built into PyTorch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6_HEXByYAi1"
      },
      "source": [
        "criterion = nn.MSELoss().to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oP18mNao2VCj"
      },
      "source": [
        "**Aside:** Notice we pulled the loss function from the `torch.nn` module. That's because loss functions are themselves torch modules, although most of them don't have any parameters or data associated with them. Of course, our loss functions don't have to be torch modules: we could just as well have computed our loss to be `outputs.sum()` or any other scalar value computed from the model outputs. For this class, though, you should probably always use built in loss functions because they often apply some numerical stability tricks under the hood.\n",
        "\n",
        "Now we have a way to compute a loss from model outputs and target labels. The last important step is to tell torch what to optimize and what optimization method to use. To do this, we must instantiate an `Optimizer` object from the `torch.optim` package. Once you have an `Optimizer`, you can use the magic incantation during training:\n",
        "```\n",
        "loss = ...\n",
        "loss.backward()\n",
        "optimizer.step()\n",
        "optimizer.zero_grad()\n",
        "```\n",
        "where, as before, `loss` is a scalar tensor computed from your model's outputs. The `loss.backward()` call computes the gradient of the loss with respect to the model parameters used to produce it. The `optimizer.step()` takes the gradient step on the parameters. The `optimizer.zero_grad()` step is not obvious: by default, PyTorch accumulates gradients as you continue to perform computations. We don't want this behavior after we've already taken a gradient step, so we clear the cache each time. Don't forget to do this.\n",
        "\n",
        "#### **TASK 5**\n",
        "\n",
        "Instantiate an `Optimizer` below. There are many kinds of optimizers, but for this assignment we'll use basic no-bells no-whistles stochastic gradient descent: `torch.optim.SGD`. Use a learning rate of .01"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNf_IGN1X6bO"
      },
      "source": [
        "# TODO: Your code here."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a3U_qIOp7DQ"
      },
      "source": [
        "Finally, in the spirit of _stochastic_ gradient descent, we will process our training data in batches. This is, of course, _absurdly unnecessary_ for a dataset with only 150 points, but the data utilities in PyTorch are quite useful, and you should know how to use them.\n",
        "\n",
        "The two classes you'll see over and over again are `torch.utils.data.Dataset` and `torch.utils.data.DataLoader`.\n",
        "\n",
        "The [Dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) class defines an abstract interface for datasets: you must define `__getitem__`, which returns the features and class label of one item in the dataset, and `__len__`, which returns the number of items in the dataset.\n",
        "\n",
        "The [DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) class wraps a given dataset, allowing you iterate over the dataset in batches. No need to worry about indexing or collating the data because it does that for you.\n",
        "\n",
        "#### **TASK 6**\n",
        "\n",
        "Fill in the `IrisDataset` skeleton below. Instantiate the dataset using `A` and `b` from the previous section, and also instantiate a `DataLoader` with a batch size of 50."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Jo-GQI5ogOy"
      },
      "source": [
        "from torch.utils import data\n",
        "\n",
        "class IrisDataset(data.Dataset):\n",
        "\n",
        "  def __init__(self, features, labels):\n",
        "    # TODO: Your code here.\n",
        "    pass\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    # TODO: Your code here.\n",
        "    pass\n",
        "\n",
        "  def __len__(self):\n",
        "    # TODO: Your code here.\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYSOlcRkqX7A"
      },
      "source": [
        "Now we have all the pieces we need to train a least squares classifier with SGD. \n",
        "\n",
        "#### **TASK 7**\n",
        "Using the optimizer, criterion, and data loader you just created, train the least squares classifier for 2500 epochs. By **epoch**, we mean making a full pass through the dataset, i.e. iterating over the `DataLoader` one full time. \n",
        "\n",
        "Visualize the learned plane and print the classification accuracy using the utilities from before. Does the result look the same as before?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvVLvDVps7wv"
      },
      "source": [
        "# TODO: Your code here."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOTJVgrd5vxF"
      },
      "source": [
        "# Part 2: Dynamic Programming\n",
        "\n",
        "In the second part of this assignment, we will review **dynamic programming**. Dynamic programming (DP) is a paradigm for optimizing recursive algorithms by memoizing and reusing solutions to smaller subproblems.\n",
        "\n",
        "Many algorithms we will see in this class (e.g., the forward-backward algorithms of HMMs, the CKY parsing algorithm, etc.) rely on dynamic programming for efficient implementation. For that reason, it is important that you are familiar with DP and know how to design and implement new dynamic programs.\n",
        "\n",
        "## Problem statement\n",
        "\n",
        "This problem concerns the **edit distance** between strings. Specifically, given two strings like \"masaachusets\" and \"massachusetts\", how many characters do we have to add, delete, or change in the first string to make it equal to the second? In this example, the answer should be 2: the second string can be obtained by changing the second \"a\" to \"s\" and inserting a \"t\" near the end.\n",
        "\n",
        "Formally let $s = s_1 s_2 \\dots s_k$ and $t = t_1 t_2 \\dots t_\\ell$ be strings with $k$ and $\\ell$ characters, respectively. The question is: what is the fewest number of character-level edits you can make to $s$ to make it equal to $t$? We will count only four kinds of edits:\n",
        "1. **Insertionn:** Add a character to the string. Example: $abc \\to aabc$.\n",
        "1. **Deletio:** Delete a character in the string. Example: $abc \\to ac$.\n",
        "1. **Substitutiom:** Replace one character in the string for any other character. Example: $abc \\to abd$.\n",
        "1. **Transpsoition:** Swap two consecutive characters. Example: $abc \\to acb$. \n",
        "Importantly, we will assume each character is transposed at most one time. So, for example, we will not consider two step transpositions like the following: $abc \\to acb \\to cab$. Instead, computing edit distance for the previous example would rely on substitutions only\n",
        "\n",
        "**Caveat:** we will consider some classes of edits as more expensive than others. There are practical reasons to do this. For example, you could imagine using edit distance in a spell checker system, where some kinds of edits are more unlikely (higher cost) than others. In this assignment, we will say that substituting or transposing a consonant with a vowel, or vice versa, is twice as expensive as substituting/transposing within character class, e.g. editing $abc \\to abd$ counts as 1 edit, but editing $abc \\to aba$ counts as 2.\n",
        "\n",
        "Write an explicit recurrence relation that solves this problem. Then implement the recurrence using dynamic programming inside the `distance` function below.\n",
        "\n",
        "**Hint:** Imagine you already know the edit distance between every length $i < k$ prefix of $s$ and length $j < \\ell$ prefix of $t$. Can you write the distance between the full $s$ and $t$ in terms of those distances?\n",
        "\n",
        "**P.S.:** Yes, edit distance is a well known and well studied DP problem. You can probably find solutions online, but **please don't do this.** This assignment is intended to help prepare you for the harder DP assignments coming later in the semester."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BqNPo9wZ_7-"
      },
      "source": [
        "# TODO: Your code here.\n",
        "def distance(s, t):\n",
        "  \"\"\"Return the edit distance between strings s and t.\"\"\"\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cszm5C9L9yF5"
      },
      "source": [
        "Here are some test cases to help you out. Feel free to add more."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peq2Nl2r9xie"
      },
      "source": [
        "assert distance('masaachusets', 'massachusetts') == 3\n",
        "assert distance('foo', 'oof') == 2\n",
        "assert distance('foobarbaz', 'fobabrazr') == 3\n",
        "assert distance('dpisfun', 'dpnotfun') == 3\n",
        "print('You did it!')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}