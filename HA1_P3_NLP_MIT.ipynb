{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HA1_P3_NLP_MIT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cicattzo/mit_advanced_nlp/blob/main/HA1_P3_NLP_MIT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N655YeL2eEUC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14ac7ce9-e193-42b4-9910-f587909486a8"
      },
      "source": [
        "%%bash\n",
        "!(stat -t /usr/local/lib/*/dist-packages/google/colab > /dev/null 2>&1) && exit \n",
        "rm -rf 6864-hw1\n",
        "git clone https://github.com/mit-6864/hw1.git"
      ],
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'hw1' already exists and is not an empty directory.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5R8vijdeKgl"
      },
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/hw1\")\n",
        "\n",
        "import csv\n",
        "import itertools as it\n",
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "\n",
        "import torch\n",
        "from scipy.special import logsumexp\n",
        "import scipy\n",
        "\n",
        "import lab_util"
      ],
      "execution_count": 256,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaoYiysseNIH"
      },
      "source": [
        "## Hidden Markov Models\n",
        "\n",
        "In the remaining part of the lab (containing part 3) you'll use the Baum--Welch algorithm to learn _categorical_ representations of words in your vocabulary. Answers to questions in this lab should go in the same report as the initial release.\n",
        "\n",
        "As before, we'll start by loading up a dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUn-q_pIeuAV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e481c0f4-02e3-42fe-9426-979774c7a6f3"
      },
      "source": [
        "data = []\n",
        "n_positive = 0\n",
        "n_disp = 0\n",
        "with open(\"/content/hw1/reviews.csv\") as reader:\n",
        "  csvreader = csv.reader(reader)\n",
        "  next(csvreader)\n",
        "  for id, review, label in csvreader:\n",
        "    label = int(label)\n",
        "\n",
        "    # hacky class balancing\n",
        "    if label == 1:\n",
        "      if n_positive == 2000:\n",
        "        continue\n",
        "      n_positive += 1\n",
        "    if len(data) == 4000:\n",
        "      break\n",
        "\n",
        "    data.append((review, label))\n",
        "    \n",
        "    if n_disp > 5:\n",
        "      continue\n",
        "    n_disp += 1\n",
        "    print(\"review:\", review)\n",
        "    print(\"rating:\", label, \"(good)\" if label == 1 else \"(bad)\")\n",
        "    print()\n",
        "\n",
        "print(f\"Read {len(data)} total reviews.\")\n",
        "np.random.shuffle(data)\n",
        "reviews, labels = zip(*data)\n",
        "train_reviews = reviews[:3000]\n",
        "train_labels = labels[:3000]\n",
        "val_reviews = reviews[3000:3500]\n",
        "val_labels = labels[3000:3500]\n",
        "test_reviews = reviews[3500:]\n",
        "test_labels = labels[3500:]"
      ],
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "review: I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most.\n",
            "rating: 1 (good)\n",
            "\n",
            "review: Product arrived labeled as Jumbo Salted Peanuts...the peanuts were actually small sized unsalted. Not sure if this was an error or if the vendor intended to represent the product as \"Jumbo\".\n",
            "rating: 0 (bad)\n",
            "\n",
            "review: This is a confection that has been around a few centuries.  It is a light, pillowy citrus gelatin with nuts - in this case Filberts. And it is cut into tiny squares and then liberally coated with powdered sugar.  And it is a tiny mouthful of heaven.  Not too chewy, and very flavorful.  I highly recommend this yummy treat.  If you are familiar with the story of C.S. Lewis' \"The Lion, The Witch, and The Wardrobe\" - this is the treat that seduces Edmund into selling out his Brother and Sisters to the Witch.\n",
            "rating: 1 (good)\n",
            "\n",
            "review: If you are looking for the secret ingredient in Robitussin I believe I have found it.  I got this in addition to the Root Beer Extract I ordered (which was good) and made some cherry soda.  The flavor is very medicinal.\n",
            "rating: 0 (bad)\n",
            "\n",
            "review: Great taffy at a great price.  There was a wide assortment of yummy taffy.  Delivery was very quick.  If your a taffy lover, this is a deal.\n",
            "rating: 1 (good)\n",
            "\n",
            "review: I got a wild hair for taffy and ordered this five pound bag. The taffy was all very enjoyable with many flavors: watermelon, root beer, melon, peppermint, grape, etc. My only complaint is there was a bit too much red/black licorice-flavored pieces (just not my particular favorites). Between me, my kids, and my husband, this lasted only two weeks! I would recommend this brand of taffy -- it was a delightful treat.\n",
            "rating: 1 (good)\n",
            "\n",
            "Read 4000 total reviews.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2qlqRHoe3y-"
      },
      "source": [
        "Next, implement the forward--backward algorithm for HMMs like we saw in class.\n",
        "\n",
        "**IMPORTANT NOTE**: if you directly multiply probabilities as shown on the class slides, you'll get underflow errors. You'll probably want to work in the log domain (remember that `log(ab) = log(a) + log(b)`, `log(exp(a) + exp(b)) = logaddexp(a, b)`). In general, we recommend either `np.logaddexp` or `scipy.special.logsumexp` as safe ways to compute the necessary quantities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Q56nZA7KbTJ",
        "outputId": "d8fc68d4-35b1-414a-d94f-627b76366e63"
      },
      "source": [
        "logsumexp(np.array([1,2]))"
      ],
      "execution_count": 257,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.313261687518223"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 257
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sp-qdTD1KeNg",
        "outputId": "f0835498-ab18-48dc-ecf6-c2951a3499c8"
      },
      "source": [
        "np.log(np.exp(1)+np.exp(2))"
      ],
      "execution_count": 237,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.3132616875182226"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 237
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjhShiLeKjWV"
      },
      "source": [
        "a = np.array([1,2])"
      ],
      "execution_count": 238,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Y3BUR2k7EVn",
        "outputId": "462e8d51-6b93-4d82-8c0d-4ca8a9e76516"
      },
      "source": [
        "a = np.zeros((10, 10))\n",
        "a"
      ],
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 208
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wVf4QVIfBdc"
      },
      "source": [
        "# hmm model\n",
        "class HMM(object):\n",
        "    def __init__(self, num_states, num_words):\n",
        "        self.num_states = num_states\n",
        "        self.num_words = num_words\n",
        "\n",
        "        self.states = range(num_states)\n",
        "        self.symbols = range(num_words)\n",
        "\n",
        "        \"\"\"\n",
        "        Initialize the matrix A with random transition probabilities p(j|i)\n",
        "        A should be a matrix of size `num_states x num_states` with rows that\n",
        "        sum to 1.\n",
        "        \"\"\"\n",
        "         # your code here\n",
        "        self.A = np.random.rand(self.num_states,self.num_states)\n",
        "        self.A = self.A/self.A.sum(axis=1)[:,None]\n",
        "        # self.A = torch.from_numpy(self.A)\n",
        "        \"\"\"\n",
        "        Initialize the matrix B with random emission probabilities p(o|i). B \n",
        "        should be a matrix of size `num_states x num_words` with rows that sum \n",
        "        to 1.\n",
        "        \"\"\"\n",
        "       # your code here\n",
        "        self.B = np.random.rand(self.num_states,self.num_words) \n",
        "        self.B = self.B/self.B.sum(axis=1)[:,None]\n",
        "        # self.B = torch.from_numpy(self.B)\n",
        "        \"\"\"\n",
        "        Initialize the vector pi with a random starting distribution. pi should\n",
        "        be a vector of size `num_states` with entries that sum to 1.\n",
        "        \"\"\"\n",
        "        # your code here\n",
        "        self.pi = np.random.random(self.num_states)\n",
        "        self.pi /= self.pi.sum()\n",
        "        # self.pi = torch.from_numpy(self.pi)\n",
        "        # self.pi = None ```\n",
        "\n",
        "\n",
        "    def generate(self, n):\n",
        "        \"\"\"randomly sample the HMM to generate a sequence.\n",
        "        \"\"\"\n",
        "        # we'll give you this one\n",
        "\n",
        "        sequence = []\n",
        "        # initialize the first state\n",
        "        state = np.random.choice(self.states, p=self.pi)\n",
        "        for i in range(n):\n",
        "            # get the emission probs for this state\n",
        "            b = self.B[state, :]\n",
        "            # emit a word\n",
        "            word = np.random.choice(self.symbols, p=b)\n",
        "            sequence.append(word)\n",
        "            # get the transition probs for this state\n",
        "            a = self.A[state, :]\n",
        "            # update the state\n",
        "            state = np.random.choice(self.states, p=a)\n",
        "        return sequence\n",
        "\n",
        "    def forward(self, obs):\n",
        "        \"\"\"\n",
        "        Runs the forward algorithm. This function should return a \n",
        "        `len(obs) x  num_states` matrix where the (t, i)th entry contains \n",
        "        log p(obs[:t], hidden_state_t = i)\n",
        "        \"\"\"\n",
        "\n",
        "        # your code here!\n",
        "\n",
        "        alpha = np.zeros((len(obs), self.num_states))\n",
        "\n",
        "        for s in self.states:\n",
        "          alpha[0][s] = np.log(self.pi[s])+np.log(self.B[s][obs[0]])\n",
        "\n",
        "        for t in range(1, len(obs)):\n",
        "\n",
        "          for s in self.states:\n",
        "\n",
        "            temp = []\n",
        "\n",
        "            for s_i in self.states:\n",
        "\n",
        "              temp.append(alpha[t-1][s_i] + np.log(self.A[s_i][s]) + np.log(self.B[s][obs[t]]))\n",
        "\n",
        "            temp = np.array(temp)\n",
        "\n",
        "            # alpha[t][s] = sum((alpha[t-1][s_i] * self.A[s_i][s] * self.B[s][obs[t]]) for s_i in self.states)\n",
        "\n",
        "            alpha[t][s] = logsumexp(temp)\n",
        "\n",
        "        # alpha = np.log(alpha)\n",
        "\n",
        "        return alpha\n",
        "\n",
        "    def backward(self, obs):\n",
        "        \"\"\"\n",
        "        Run the backward algorithm. This function should return a\n",
        "        `len(obs) x num_states` matrix where the (t, i)th entry contains\n",
        "        log p(obs[t+1:] | hidden_state_t = i)\n",
        "        \"\"\"\n",
        "\n",
        "        beta = np.zeros((len(obs), self.num_states))\n",
        "\n",
        "        # your code here!\n",
        "\n",
        "        for s in self.states:\n",
        "          beta[len(obs)-1][s] = 1.0\n",
        "\n",
        "        for t in reversed(range(len(obs)-1)):\n",
        "\n",
        "          for s in self.states:\n",
        "\n",
        "            # temp = []\n",
        "\n",
        "            # for s_i in self.states:\n",
        "              \n",
        "              # temp.append(np.log(beta[t+1][s_i]) + np.log(self.A[s][s_i]) + np.log(self.B[s_i][obs[t+1]]))\n",
        "\n",
        "            # temp = np.array(temp)\n",
        "\n",
        "            beta[t][s] = sum((beta[t+1][s_i] * self.A[s][s_i] * self.B[s_i][obs[t+1]]) for s_i in self.states)\n",
        "\n",
        "            # beta[t][s] = logsumexp(temp)\n",
        "\n",
        "        beta = np.log(beta)\n",
        "\n",
        "        return beta\n",
        "        \n",
        "    def forward_backward(self, obs):\n",
        "        \"\"\"\n",
        "        Compute forward-backward scores\n",
        "\n",
        "        logprob is the total log-probability of the sequence obs (marginalizing\n",
        "        over hidden states).\n",
        "\n",
        "        gamma is a matrix of size `len(obs) x num_states1. It contains the\n",
        "        marginal probability of being in state i at time t\n",
        "\n",
        "        xi is a tensor of size `len(obs) x num_states x num_states`. It contains\n",
        "        the marginal probability of transitioning from i to j at t.\n",
        "        \"\"\"\n",
        "        #create forward and backward\n",
        "        forward = self.forward(obs)\n",
        "        forward_exp = np.exp(forward)\n",
        "        backward = self.backward(obs)\n",
        "        backward_exp = np.exp(backward)\n",
        "\n",
        "        #calculate logprob\n",
        "        logprob = logsumexp(np.array(forward[len(obs)-1]))\n",
        "        #calculate gamma\n",
        "        gamma = np.zeros((len(obs), self.num_states))\n",
        "\n",
        "        xi = np.zeros((len(obs), self.num_states, self.num_states))\n",
        "\n",
        "        for t in range(len(obs)):\n",
        "\n",
        "          for s in self.states:\n",
        "\n",
        "            gamma[t][s] = np.exp(forward[t][s]+backward[t][s]-logprob)\n",
        "\n",
        "            if t == len(obs)-1:\n",
        "              continue\n",
        "\n",
        "            for s_i in self.states:\n",
        "\n",
        "              xi[t][s][s_i] = np.exp(forward[t][s] + np.log(self.A[s][s_i]) + np.log(self.B[s_i][obs[t+1]]) + backward[t+1][s_i] - logprob)\n",
        "\n",
        "        # your code here!\n",
        "\n",
        "        return logprob, xi, gamma\n",
        "\n",
        "        \"\"\"\n",
        "        SANITY CHECK\n",
        "\n",
        "        The most straightforward way of implementing the forward, backward, and \n",
        "        forward_backward methods would be to iterate through all the values and \n",
        "        use the formulas in the slides to calculate the corresponding values.\n",
        "\n",
        "        However, this may not be fast enough. If your model is taking too long\n",
        "        to train, consider how you may speed up your code by reducing the number\n",
        "        of for loops involved. How can you reformulate your code using matrix\n",
        "        operations?\n",
        "\n",
        "        Hint: we were able to implement each of the forward, backward, and\n",
        "        forward_backward operations using only one for loop.\n",
        "        \"\"\"\n",
        "\n",
        "    def learn_unsupervised(self, corpus, num_iters, print_every=10):\n",
        "        \"\"\"Run the Baum Welch EM algorithm\n",
        "        \n",
        "        corpus: the data to learn from\n",
        "        num_iters: the number of iterations to run the algorithm\n",
        "        print_every: how often to print the log-likelihood while the model is\n",
        "        updating its parameters.\n",
        "        \"\"\"\n",
        "\n",
        "        for i_iter in range(num_iters):\n",
        "            \n",
        "\n",
        "\n",
        "            # expected_si = None # your code here\n",
        "            # expected_sij = None # your code here\n",
        "            # expected_sjwk = None # your code here\n",
        "            # expected_q1 = None # your code here\n",
        "\n",
        "            A_new = np.zeros((self.num_states,self.num_states))\n",
        "            B_new = np.zeros((self.num_states,self.num_words))\n",
        "            pi_new = np.zeros(self.num_states)\n",
        "\n",
        "            expected_si = np.zeros(self.num_states) \n",
        "            expected_sij = np.zeros((self.num_states, self.num_states)) \n",
        "            expected_sjwk = np.zeros((self.num_states, self.num_words)) \n",
        "            expected_q1 = np.zeros(self.num_states)\n",
        "            expected_number_of_times_sj = np.zeros(self.num_states)\n",
        "\n",
        "            total_logprob = 0\n",
        "            \n",
        "            for review in corpus:\n",
        "\n",
        "                logprob, xi, gamma = self.forward_backward(review)\n",
        "\n",
        "                total_logprob = total_logprob+logprob \n",
        "\n",
        "\n",
        "                for s_i in self.states:\n",
        "\n",
        "                  expected_q1[s_i] += gamma[0, s_i]\n",
        "\n",
        "                  expected_number_of_times_sj[s_i] += sum(gamma[t, s_i] for t in range(len(review)))\n",
        "\n",
        "                  for s_j in self.states:\n",
        "\n",
        "                    expected_sij[s_i,s_j] += sum(xi[t][s_i][s_j] for t in range(len(review)-1))\n",
        "\n",
        "                  expected_si[s_i] += sum(expected_sij[s_i, s_x] for s_x in self.states)\n",
        "\n",
        "                  for t in range(len(review)):\n",
        "\n",
        "                    expected_sjwk[s_i, review[t]] += gamma[t,s_i]\n",
        "\n",
        "\n",
        "                for s_i in self.states:\n",
        "\n",
        "                  pi_new[s_i] = expected_q1[s_i]\n",
        "                  \n",
        "                  for s_j in self.states:\n",
        "                    \n",
        "                    A_new[s_i, s_j] = expected_sij[s_i,s_j] / expected_si[s_i]\n",
        "\n",
        "                  for w in range(self.num_words):\n",
        "\n",
        "                    B_new[s_i, w] = expected_sjwk[s_i, w] / expected_number_of_times_sj[s_i]\n",
        "            \n",
        "            A_row_sums = A_new.sum(axis=1)\n",
        "            A_new = A_new / A_row_sums[:, np.newaxis]\n",
        "\n",
        "            B_row_sums = B_new.sum(axis=1)\n",
        "            B_new = B_new / B_row_sums[:, np.newaxis]\n",
        "\n",
        "            pi_sum = sum(pi_new)\n",
        "\n",
        "            pi_new = pi_new / pi_sum\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            if i_iter % print_every == 0:\n",
        "              print(\"log-likelihood\", total_logprob)\n",
        "\n",
        "            \"\"\"\n",
        "            The following variables should be the new values of self.A, self.B,\n",
        "            and self.pi after the values are updated.\n",
        "            \"\"\"\n",
        "            # A_new = None # your code here\n",
        "            # B_new = None # your code here\n",
        "            # pi_new = None # your code here\n",
        "\n",
        "            self.A = A_new\n",
        "            self.B = B_new\n",
        "            self.pi = pi_new"
      ],
      "execution_count": 290,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 868
        },
        "id": "d9p_n5-8Lajr",
        "outputId": "5ea68dde-ae77-4f1a-f4c8-80e8d57d5a32"
      },
      "source": [
        "init_test()\n",
        "forward_test()\n",
        "backward_test()\n",
        "# forward_backward_test()\n",
        "# baum_welch_update_test()\n",
        "# end_to_end_test()\n",
        "\n",
        "\"\"\"\n",
        "Note: The end_to_end_test is not as robustg due to it using random starts. Try\n",
        "running the test case a few times to see if you get a good result at least a few\n",
        "times before deciding that your code is buggy.\n",
        "\"\"\""
      ],
      "execution_count": 291,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The result of the forward function should be [[ -2.96913  -3.43382]\n",
            " [ -4.66005  -9.19418]\n",
            " [ -7.35001  -7.89695]\n",
            " [ -9.65069  -9.95363]\n",
            " [-11.25815 -14.27392]\n",
            " [-18.14079 -14.4781 ]\n",
            " [-16.89275 -18.62696]\n",
            " [-19.45549 -20.17289]\n",
            " [-21.53772 -23.283  ]\n",
            " [-23.4927  -26.69119]\n",
            " [-25.84891 -26.73817]\n",
            " [-28.12237 -29.92402]]\n",
            "Your value of alpha is: [[ -2.96913  -3.43382]\n",
            " [ -4.66005  -9.19418]\n",
            " [ -7.35001  -7.89695]\n",
            " [ -9.65069  -9.95363]\n",
            " [-11.25815 -14.27392]\n",
            " [-18.14079 -14.4781 ]\n",
            " [-16.89275 -18.62697]\n",
            " [-19.45549 -20.17289]\n",
            " [-21.53772 -23.283  ]\n",
            " [-23.4927  -26.69119]\n",
            " [-25.84891 -26.73817]\n",
            " [-28.12237 -29.92402]]\n",
            "The result of the backward function should be [[-25.42937 -25.58918]\n",
            " [-23.32164 -23.19959]\n",
            " [-21.11007 -21.02033]\n",
            " [-18.82215 -18.94381]\n",
            " [-16.78523 -16.33951]\n",
            " [-13.42847 -13.51924]\n",
            " [-11.24815 -11.19161]\n",
            " [ -8.88679  -8.96441]\n",
            " [ -6.57374  -6.70985]\n",
            " [ -4.51873  -4.47419]\n",
            " [ -2.44529  -2.51463]\n",
            " [  0.        0.     ]]\n",
            "Your value of beta is: [[-25.42937 -25.58918]\n",
            " [-23.32164 -23.19959]\n",
            " [-21.11007 -21.02033]\n",
            " [-18.82215 -18.94381]\n",
            " [-16.78523 -16.33951]\n",
            " [-13.42847 -13.51924]\n",
            " [-11.24815 -11.19161]\n",
            " [ -8.88679  -8.96441]\n",
            " [ -6.57374  -6.70985]\n",
            " [ -4.51873  -4.47419]\n",
            " [ -2.44529  -2.51463]\n",
            " [  0.        0.     ]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nNote: The end_to_end_test is not as robustg due to it using random starts. Try\\nrunning the test case a few times to see if you get a good result at least a few\\ntimes before deciding that your code is buggy.\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 291
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6PvF4PHSpxX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3gRZ_r_vLav"
      },
      "source": [
        "def init_test():\r\n",
        "\r\n",
        "    num_states = np.random.randint(100)\r\n",
        "    num_words = np.random.randint(100)\r\n",
        "    model = HMM(num_states, num_words)\r\n",
        "\r\n",
        "    assert model.A.shape == (num_states, num_states)\r\n",
        "    assert model.B.shape == (num_states, num_words)\r\n",
        "    assert model.pi.shape == (num_states, )\r\n",
        "\r\n",
        "    assert np.linalg.norm(np.sum(model.A, axis=1) - np.ones(num_states)) < 1e-10\r\n",
        "    assert np.linalg.norm(np.sum(model.B, axis=1) - np.ones(num_states)) < 1e-10\r\n",
        "    assert np.linalg.norm(np.sum(model.pi) - 1) < 1e-10\r\n",
        "\r\n",
        "def forward_test():\r\n",
        "    model = HMM(2, 10)\r\n",
        "    model.A = np.array([[0.79034887, 0.20965113],\r\n",
        "                        [0.66824331, 0.33175669]])\r\n",
        "    model.B = np.array([[0.08511814, 0.06627238, 0.08487461, 0.15607959, 0.00124582, 0.12984083, 0.11164849, 0.11591902, 0.15232716, 0.09667395],\r\n",
        "                        [0.18425462, 0.14326559, 0.14026994, 0.0215989,  0.17687124, 0.04681278, 0.05857451, 0.17451212, 0.00473382, 0.04910648]])\r\n",
        "    model.pi = np.array([0.77480039, 0.22519961])\r\n",
        "    obs = [1, 8, 0, 0, 3, 4, 5, 2, 6, 3, 7, 9]\r\n",
        "    alpha = model.forward(obs)\r\n",
        "\r\n",
        "    print(\"The result of the forward function should be\", np.array([[-2.96913, -3.43382],\r\n",
        "                                                                    [ -4.66005, -9.19418],\r\n",
        "                                                                    [ -7.35001, -7.89695],\r\n",
        "                                                                    [ -9.65069, -9.95363],\r\n",
        "                                                                    [-11.25815, -14.27392],\r\n",
        "                                                                    [-18.14079, -14.4781 ],\r\n",
        "                                                                    [-16.89275, -18.62696],\r\n",
        "                                                                    [-19.45549, -20.17289],\r\n",
        "                                                                    [-21.53772, -23.283  ],\r\n",
        "                                                                    [-23.4927, -26.69119],\r\n",
        "                                                                    [-25.84891, -26.73817],\r\n",
        "                                                                    [-28.12237, -29.92402]]))\r\n",
        "    print(\"Your value of alpha is:\", np.round(alpha, 5))\r\n",
        "\r\n",
        "def backward_test():\r\n",
        "    model = HMM(2, 10)\r\n",
        "    model.A = np.array([[0.79034887, 0.20965113],\r\n",
        "                        [0.66824331, 0.33175669]])\r\n",
        "    model.B = np.array([[0.08511814, 0.06627238, 0.08487461, 0.15607959, 0.00124582, 0.12984083, 0.11164849, 0.11591902, 0.15232716, 0.09667395],\r\n",
        "                        [0.18425462, 0.14326559, 0.14026994, 0.0215989,  0.17687124, 0.04681278, 0.05857451, 0.17451212, 0.00473382, 0.04910648]])\r\n",
        "    model.pi = np.array([0.77480039, 0.22519961])\r\n",
        "    obs = [1, 8, 0, 0, 3, 4, 5, 2, 6, 3, 7, 9]\r\n",
        "    beta = model.backward(obs)\r\n",
        "\r\n",
        "    print(\"The result of the backward function should be\", np.array([[-25.42937, -25.58918], \r\n",
        "                                                                     [-23.32164, -23.19959],\r\n",
        "                                                                     [-21.11007, -21.02033],\r\n",
        "                                                                     [-18.82215, -18.94381],\r\n",
        "                                                                     [-16.78523, -16.33951],\r\n",
        "                                                                     [-13.42847, -13.51924],\r\n",
        "                                                                     [-11.24815, -11.19161],\r\n",
        "                                                                     [ -8.88679,  -8.96441],\r\n",
        "                                                                     [ -6.57374,  -6.70985],\r\n",
        "                                                                     [ -4.51873,  -4.47419],\r\n",
        "                                                                     [ -2.44529,  -2.51463],\r\n",
        "                                                                     [  0, 0]]))\r\n",
        "\r\n",
        "    print(\"Your value of beta is:\", np.round(beta, 5))\r\n",
        "\r\n",
        "\r\n",
        "def forward_backward_test():\r\n",
        "    model = HMM(2, 10)\r\n",
        "    model.A = np.array([[0.79034887, 0.20965113],\r\n",
        "                        [0.66824331, 0.33175669]])\r\n",
        "    model.B = np.array([[0.08511814, 0.06627238, 0.08487461, 0.15607959, 0.00124582, 0.12984083, 0.11164849, 0.11591902, 0.15232716, 0.09667395],\r\n",
        "                        [0.18425462, 0.14326559, 0.14026994, 0.0215989,  0.17687124, 0.04681278, 0.05857451, 0.17451212, 0.00473382, 0.04910648]])\r\n",
        "    model.pi = np.array([0.77480039, 0.22519961])\r\n",
        "    obs = [1, 8, 0, 0, 3, 4, 5, 2, 6, 3, 7, 9]\r\n",
        "    logprob, xi, gamma = model.forward_backward(obs)\r\n",
        "\r\n",
        "    print(\"The value of logprob should be:\", -27.9693)\r\n",
        "    print(\"Your value of logprob is:\", np.round(logprob, 5))\r\n",
        "\r\n",
        "    print(\"The value of xi should be:\", np.array([[[0.64523, 0.00601],\r\n",
        "                                                  [0.34278, 0.00598]],\r\n",
        "\r\n",
        "                                                 [[0.60684, 0.38117],\r\n",
        "                                                  [0.00551, 0.00648]],\r\n",
        "\r\n",
        "                                                 [[0.40595, 0.2064 ],\r\n",
        "                                                  [0.19863, 0.18902]],\r\n",
        "\r\n",
        "                                                 [[0.5718,  0.03278],\r\n",
        "                                                  [0.35711, 0.03831]],\r\n",
        "\r\n",
        "                                                 [[0.02625, 0.90266],\r\n",
        "                                                  [0.00109, 0.07   ]],\r\n",
        "\r\n",
        "                                                 [[0.02482, 0.00251],\r\n",
        "                                                  [0.81777, 0.15489]],\r\n",
        "\r\n",
        "                                                 [[0.59943, 0.24316],\r\n",
        "                                                  [0.08947, 0.06793]],\r\n",
        "\r\n",
        "                                                 [[0.6143,  0.07461],\r\n",
        "                                                  [0.25347, 0.05762]],\r\n",
        "\r\n",
        "                                                 [[0.8357,  0.03207],\r\n",
        "                                                  [0.12337, 0.00886]],\r\n",
        "\r\n",
        "                                                 [[0.69872, 0.26034],\r\n",
        "                                                  [0.02412, 0.01682]],\r\n",
        "\r\n",
        "                                                 [[0.63701, 0.08583],\r\n",
        "                                                  [0.22134, 0.05582]]]))\r\n",
        "    print(\"Your value of xi is:\", np.round(xi, 5))\r\n",
        "\r\n",
        "    print(\"The value of gamma should be:\", np.array([[0.65124, 0.34876],\r\n",
        "                                                    [0.98802, 0.01198],\r\n",
        "                                                    [0.61235, 0.38765],\r\n",
        "                                                    [0.60458, 0.39542],\r\n",
        "                                                    [0.92891, 0.07109],\r\n",
        "                                                    [0.02733, 0.97267],\r\n",
        "                                                    [0.8426,  0.1574 ],\r\n",
        "                                                    [0.68891, 0.31109],\r\n",
        "                                                    [0.86777, 0.13223],\r\n",
        "                                                    [0.95906, 0.04094],\r\n",
        "                                                    [0.72284, 0.27716],\r\n",
        "                                                    [0.85835, 0.14165]]))\r\n",
        "\r\n",
        "    print(\"Your value of gamma is:\", np.round(gamma, 5))\r\n",
        "\r\n",
        "def baum_welch_update_test():\r\n",
        "    model = HMM(4, 10)\r\n",
        "    \r\n",
        "    model.A = np.array([[0.05263151, 0.62161178, 0.06683182, 0.25892489],\r\n",
        "                        [0.26993274, 0.13114741, 0.32305468, 0.27586517],\r\n",
        "                        [0.2951958,  0.14576492, 0.22474111, 0.33429817],\r\n",
        "                        [0.29586018, 0.26065884, 0.1977772,  0.24570378]])\r\n",
        "    \r\n",
        "    model.B = np.array([[0.01800425, 0.09767131, 0.17824799, 0.12586453, 0.19514548, 0.05433139, 0.01995667, 0.12985343, 0.01884263, 0.16208232],\r\n",
        "                        [0.04512782, 0.09469685, 0.1426164,  0.13851362, 0.08717793, 0.17152532, 0.08746939, 0.04900339, 0.05315859, 0.13071069],\r\n",
        "                        [0.11055806, 0.10592473, 0.0051817,  0.07721441, 0.21761783, 0.20323146, 0.18881598, 0.00584989, 0.00682669, 0.07877924],\r\n",
        "                        [0.08711377, 0.16703645, 0.0706214,  0.05297571, 0.10486868, 0.16794587, 0.13562053, 0.15729142, 0.03345308, 0.02307309]])\r\n",
        "    \r\n",
        "    model.pi = np.array([0.21186864, 0.27156561, 0.37188523, 0.14468051])\r\n",
        "    \r\n",
        "    corpus = np.array([[7,3,2,5,0,3,2,9,4,2], [7,3,2,4,2,8,7,5,0,8], [7,3,2,3,1,7,3,8,6,7], [7,3,2,6,4,4,3,4,0,0]])\r\n",
        "\r\n",
        "    model.learn_unsupervised(corpus, 200)\r\n",
        "\r\n",
        "    print(\"hmm.A should be\", np.array([[0, 1, 0, 0], \r\n",
        "                                     [0.14122, 0, 0.27099, 0.58779], \r\n",
        "                                     [0.20671, 0, 0, 0.79329], \r\n",
        "                                     [0, 0.90909, 0.09091, 0]]))\r\n",
        "    print(\"Your implementation has hmm.A to be\", np.round(model.A, 5))\r\n",
        "\r\n",
        "    print(\"hmm.B should be\", np.array([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\r\n",
        "                                              [0.0625, 0, 0, 0.5, 0, 0.125, 0.125, 0, 0.125, 0.0625],\r\n",
        "                                              [0, 0.20671, 0, 0, 0.79329, 0, 0, 0, 0, 0],\r\n",
        "                                              [0.24667, 0, 0.57555, 0, 0.09556, 0, 0, 0, 0.08222, 0]]))\r\n",
        "    print(\"Your implementation has hmm.B to be\", np.round(model.B, 5))\r\n",
        "\r\n",
        "    print(\"hmm.pi should be\", np.array([1, 0, 0, 0]))\r\n",
        "\r\n",
        "    print(\"Your implementation has hmm.pi to be\", np.round(model.pi, 5))\r\n",
        "\r\n",
        "def end_to_end_test():\r\n",
        "    # Test Case 1\r\n",
        "\r\n",
        "    corpus = np.array([[0,3,0,3,0,3,0,3,0,3,0,3], [0,2,0,2,0,2,0,2,0,2,0,2,0], [1,2,1,2,1,2,1,2,1,2,1,2],[1,3,1,3,1,3,1,3,1,3]])\r\n",
        "    hmm = HMM(num_states=2,num_words=4)\r\n",
        "    hmm.learn_unsupervised(corpus, 10)\r\n",
        "    print(\"After this test case, hmm.A should either be approximately,\",  np.array([[0, 1], [1, 0]]))\r\n",
        "    print(\"This is your current value of hmm.A: \", np.round(hmm.A, 5))\r\n",
        "\r\n",
        "    print(\"After this test case, hmm.B should either be approximately,\", np.array([[0, 0, 0.5, 0.5], [0.5, 0.5, 0, 0]]), \" or it should be \", np.array([[0.5, 0.5, 0, 0], [0, 0, 0.5, 0.5]]))\r\n",
        "    print(\"This is your current value of hmm.B: \", np.round(hmm.B, 5))\r\n",
        "\r\n",
        "    # Test Case 2\r\n",
        "\r\n",
        "    corpus = np.array([[0,0,0,0,0,0,0,0,0,0], [1,1,1,1,1,1,1,1,1,1], [2,2,2,2,2,2,2,2,2,2]])\r\n",
        "    hmm = HMM(num_states=3, num_words=3)\r\n",
        "    hmm.learn_unsupervised(corpus, 100)\r\n",
        "    print(\"After this test case, hmm.A should be the identity matrix\", np.eye(3))\r\n",
        "    print(\"This is your current value of hmm.A: \", np.round(hmm.A, 5))\r\n",
        "\r\n",
        "    print(\"After this test case, hmm.B should be some 3 by 3 permutation matrix\")\r\n",
        "    print(\"This is your current value of hmm.B: \", np.round(hmm.B, 5))"
      ],
      "execution_count": 275,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yFF2rKuuh6U"
      },
      "source": [
        "## Test Cases\r\n",
        "\r\n",
        "The following are test cases that are meant to help you debug your code. The code involves six test suites - an initialization test, a forward test, a backward test, a forward_backward test, a baum_welch_update test, and a final end_to_end test."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmewPFV2MPlS"
      },
      "source": [
        "## Test\r\n",
        "\r\n",
        "To actually run the test cases, run the cell below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFtzZ9W9MVKu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d28088aa-cd85-4dcd-d6f5-8823003bb1eb"
      },
      "source": [
        "init_test()\r\n",
        "forward_test()\r\n",
        "backward_test()\r\n",
        "forward_backward_test()\r\n",
        "baum_welch_update_test()\r\n",
        "end_to_end_test()\r\n",
        "\r\n",
        "\"\"\"\r\n",
        "Note: The end_to_end_test is not as robustg due to it using random starts. Try\r\n",
        "running the test case a few times to see if you get a good result at least a few\r\n",
        "times before deciding that your code is buggy.\r\n",
        "\"\"\""
      ],
      "execution_count": 292,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The result of the forward function should be [[ -2.96913  -3.43382]\n",
            " [ -4.66005  -9.19418]\n",
            " [ -7.35001  -7.89695]\n",
            " [ -9.65069  -9.95363]\n",
            " [-11.25815 -14.27392]\n",
            " [-18.14079 -14.4781 ]\n",
            " [-16.89275 -18.62696]\n",
            " [-19.45549 -20.17289]\n",
            " [-21.53772 -23.283  ]\n",
            " [-23.4927  -26.69119]\n",
            " [-25.84891 -26.73817]\n",
            " [-28.12237 -29.92402]]\n",
            "Your value of alpha is: [[ -2.96913  -3.43382]\n",
            " [ -4.66005  -9.19418]\n",
            " [ -7.35001  -7.89695]\n",
            " [ -9.65069  -9.95363]\n",
            " [-11.25815 -14.27392]\n",
            " [-18.14079 -14.4781 ]\n",
            " [-16.89275 -18.62697]\n",
            " [-19.45549 -20.17289]\n",
            " [-21.53772 -23.283  ]\n",
            " [-23.4927  -26.69119]\n",
            " [-25.84891 -26.73817]\n",
            " [-28.12237 -29.92402]]\n",
            "The result of the backward function should be [[-25.42937 -25.58918]\n",
            " [-23.32164 -23.19959]\n",
            " [-21.11007 -21.02033]\n",
            " [-18.82215 -18.94381]\n",
            " [-16.78523 -16.33951]\n",
            " [-13.42847 -13.51924]\n",
            " [-11.24815 -11.19161]\n",
            " [ -8.88679  -8.96441]\n",
            " [ -6.57374  -6.70985]\n",
            " [ -4.51873  -4.47419]\n",
            " [ -2.44529  -2.51463]\n",
            " [  0.        0.     ]]\n",
            "Your value of beta is: [[-25.42937 -25.58918]\n",
            " [-23.32164 -23.19959]\n",
            " [-21.11007 -21.02033]\n",
            " [-18.82215 -18.94381]\n",
            " [-16.78523 -16.33951]\n",
            " [-13.42847 -13.51924]\n",
            " [-11.24815 -11.19161]\n",
            " [ -8.88679  -8.96441]\n",
            " [ -6.57374  -6.70985]\n",
            " [ -4.51873  -4.47419]\n",
            " [ -2.44529  -2.51463]\n",
            " [  0.        0.     ]]\n",
            "The value of logprob should be: -27.9693\n",
            "Your value of logprob is: -27.96963\n",
            "The value of xi should be: [[[0.64523 0.00601]\n",
            "  [0.34278 0.00598]]\n",
            "\n",
            " [[0.60684 0.38117]\n",
            "  [0.00551 0.00648]]\n",
            "\n",
            " [[0.40595 0.2064 ]\n",
            "  [0.19863 0.18902]]\n",
            "\n",
            " [[0.5718  0.03278]\n",
            "  [0.35711 0.03831]]\n",
            "\n",
            " [[0.02625 0.90266]\n",
            "  [0.00109 0.07   ]]\n",
            "\n",
            " [[0.02482 0.00251]\n",
            "  [0.81777 0.15489]]\n",
            "\n",
            " [[0.59943 0.24316]\n",
            "  [0.08947 0.06793]]\n",
            "\n",
            " [[0.6143  0.07461]\n",
            "  [0.25347 0.05762]]\n",
            "\n",
            " [[0.8357  0.03207]\n",
            "  [0.12337 0.00886]]\n",
            "\n",
            " [[0.69872 0.26034]\n",
            "  [0.02412 0.01682]]\n",
            "\n",
            " [[0.63701 0.08583]\n",
            "  [0.22134 0.05582]]]\n",
            "Your value of xi is: [[[0.64523 0.00601]\n",
            "  [0.34278 0.00598]]\n",
            "\n",
            " [[0.60684 0.38117]\n",
            "  [0.00551 0.00648]]\n",
            "\n",
            " [[0.40595 0.2064 ]\n",
            "  [0.19863 0.18902]]\n",
            "\n",
            " [[0.5718  0.03278]\n",
            "  [0.35711 0.03831]]\n",
            "\n",
            " [[0.02625 0.90266]\n",
            "  [0.00109 0.07   ]]\n",
            "\n",
            " [[0.02482 0.00251]\n",
            "  [0.81777 0.15489]]\n",
            "\n",
            " [[0.59943 0.24316]\n",
            "  [0.08947 0.06793]]\n",
            "\n",
            " [[0.6143  0.07461]\n",
            "  [0.25347 0.05762]]\n",
            "\n",
            " [[0.8357  0.03207]\n",
            "  [0.12337 0.00886]]\n",
            "\n",
            " [[0.69872 0.26034]\n",
            "  [0.02412 0.01682]]\n",
            "\n",
            " [[0.63701 0.08583]\n",
            "  [0.22134 0.05582]]\n",
            "\n",
            " [[0.      0.     ]\n",
            "  [0.      0.     ]]]\n",
            "The value of gamma should be: [[0.65124 0.34876]\n",
            " [0.98802 0.01198]\n",
            " [0.61235 0.38765]\n",
            " [0.60458 0.39542]\n",
            " [0.92891 0.07109]\n",
            " [0.02733 0.97267]\n",
            " [0.8426  0.1574 ]\n",
            " [0.68891 0.31109]\n",
            " [0.86777 0.13223]\n",
            " [0.95906 0.04094]\n",
            " [0.72284 0.27716]\n",
            " [0.85835 0.14165]]\n",
            "Your value of gamma is: [[0.65124 0.34876]\n",
            " [0.98802 0.01198]\n",
            " [0.61235 0.38765]\n",
            " [0.60458 0.39542]\n",
            " [0.92891 0.07109]\n",
            " [0.02733 0.97267]\n",
            " [0.8426  0.1574 ]\n",
            " [0.68891 0.31109]\n",
            " [0.86777 0.13223]\n",
            " [0.95906 0.04094]\n",
            " [0.72284 0.27716]\n",
            " [0.85835 0.14165]]\n",
            "log-likelihood -96.33989919755487\n",
            "log-likelihood -61.83737464303461\n",
            "log-likelihood -59.65907612785483\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:71: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:81: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:164: RuntimeWarning: divide by zero encountered in log\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "log-likelihood -58.78006168233148\n",
            "log-likelihood -58.05638622904577\n",
            "log-likelihood -58.047573998672526\n",
            "log-likelihood -58.04756024248874\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:123: RuntimeWarning: divide by zero encountered in log\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "log-likelihood -58.04756014666195\n",
            "log-likelihood -58.04756014303575\n",
            "log-likelihood -58.04756014156317\n",
            "log-likelihood -58.04756014077336\n",
            "log-likelihood -58.047560140347095\n",
            "log-likelihood -58.04756014011701\n",
            "log-likelihood -58.04756013999282\n",
            "log-likelihood -58.047560139925785\n",
            "log-likelihood -58.04756013988958\n",
            "log-likelihood -58.04756013987006\n",
            "log-likelihood -58.04756013985951\n",
            "log-likelihood -58.04756013985383\n",
            "log-likelihood -58.04756013985076\n",
            "hmm.A should be [[0.      1.      0.      0.     ]\n",
            " [0.14122 0.      0.27099 0.58779]\n",
            " [0.20671 0.      0.      0.79329]\n",
            " [0.      0.90909 0.09091 0.     ]]\n",
            "Your implementation has hmm.A to be [[0.      1.      0.      0.     ]\n",
            " [0.14122 0.      0.27099 0.58779]\n",
            " [0.20671 0.      0.      0.79329]\n",
            " [0.      0.90909 0.09091 0.     ]]\n",
            "hmm.B should be [[0.      0.      0.      0.      0.      0.      0.      1.      0.\n",
            "  0.     ]\n",
            " [0.0625  0.      0.      0.5     0.      0.125   0.125   0.      0.125\n",
            "  0.0625 ]\n",
            " [0.      0.20671 0.      0.      0.79329 0.      0.      0.      0.\n",
            "  0.     ]\n",
            " [0.24667 0.      0.57555 0.      0.09556 0.      0.      0.      0.08222\n",
            "  0.     ]]\n",
            "Your implementation has hmm.B to be [[0.      0.      0.      0.      0.      0.      0.      1.      0.\n",
            "  0.     ]\n",
            " [0.0625  0.      0.      0.5     0.      0.125   0.125   0.      0.125\n",
            "  0.0625 ]\n",
            " [0.      0.20671 0.      0.      0.79329 0.      0.      0.      0.\n",
            "  0.     ]\n",
            " [0.24667 0.      0.57555 0.      0.09556 0.      0.      0.      0.08222\n",
            "  0.     ]]\n",
            "hmm.pi should be [1 0 0 0]\n",
            "Your implementation has hmm.pi to be [1. 0. 0. 0.]\n",
            "log-likelihood -67.2524391439169\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:165: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:246: RuntimeWarning: invalid value encountered in double_scalars\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:250: RuntimeWarning: invalid value encountered in double_scalars\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "After this test case, hmm.A should either be approximately, [[0 1]\n",
            " [1 0]]\n",
            "This is your current value of hmm.A:  [[1. 0.]\n",
            " [0. 1.]]\n",
            "After this test case, hmm.B should either be approximately, [[0.  0.  0.5 0.5]\n",
            " [0.5 0.5 0.  0. ]]  or it should be  [[0.5 0.5 0.  0. ]\n",
            " [0.  0.  0.5 0.5]]\n",
            "This is your current value of hmm.B:  [[0.52    0.      0.24    0.24   ]\n",
            " [0.      0.5     0.27273 0.22727]]\n",
            "log-likelihood -33.18143000254254\n",
            "log-likelihood -15.77248611716074\n",
            "log-likelihood -15.772486116083343\n",
            "log-likelihood -15.772486116083344\n",
            "log-likelihood -15.772486116083344\n",
            "log-likelihood -15.772486116083344\n",
            "log-likelihood -15.772486116083344\n",
            "log-likelihood -15.772486116083343\n",
            "log-likelihood -15.772486116083343\n",
            "log-likelihood -15.772486116083343\n",
            "After this test case, hmm.A should be the identity matrix [[1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]]\n",
            "This is your current value of hmm.A:  [[1.      0.      0.     ]\n",
            " [0.      0.47804 0.52196]\n",
            " [0.      0.92646 0.07354]]\n",
            "After this test case, hmm.B should be some 3 by 3 permutation matrix\n",
            "This is your current value of hmm.B:  [[0.  1.  0. ]\n",
            " [0.5 0.  0.5]\n",
            " [0.5 0.  0.5]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nNote: The end_to_end_test is not as robustg due to it using random starts. Try\\nrunning the test case a few times to see if you get a good result at least a few\\ntimes before deciding that your code is buggy.\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 292
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eF-l7WucpCBP"
      },
      "source": [
        "## Training\r\n",
        "\r\n",
        "Train a model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTWXUt15pDg4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d786ef38-16a2-4f5b-dea4-1a8f09d7fe85"
      },
      "source": [
        "tokenizer = lab_util.Tokenizer()\n",
        "tokenizer.fit(train_reviews)\n",
        "train_reviews_tk = tokenizer.tokenize(train_reviews)\n",
        "print(tokenizer.vocab_size)\n",
        "\n",
        "hmm = HMM(num_states=10, num_words=tokenizer.vocab_size)\n",
        "hmm.learn_unsupervised(train_reviews_tk, 10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2006\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:123: RuntimeWarning: divide by zero encountered in log\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiCwE05xqXmI"
      },
      "source": [
        "Let's look at some of the words associated with each hidden state:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXhMoLUFqbn_"
      },
      "source": [
        "for i in range(hmm.num_states):\n",
        "    most_probable = np.argsort(hmm.B[i, :])[:10]\n",
        "    print(f\"state {i}\")\n",
        "    for o in most_probable:\n",
        "        print(tokenizer.token_to_word[o], hmm.B[i, o])\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAQ_PmASwdFz"
      },
      "source": [
        "We can also look at some samples from the model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tj1eT3s3wgFJ"
      },
      "source": [
        "for i in range(10):\n",
        "    print(tokenizer.de_tokenize([hmm.generate(10)]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9Qk9adNr7lQ"
      },
      "source": [
        "Finally, let's repeat the classification experiment from Parts 1 and 2, using the _vector of expected hidden state counts_ as a sentence representation.\n",
        "\n",
        "(Warning! results may not be the same as in earlier versions of this experiment.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mL6JQXLJspyA"
      },
      "source": [
        "def train_model(xs_featurized, ys):\n",
        "  import sklearn.linear_model\n",
        "  model = sklearn.linear_model.LogisticRegression()\n",
        "  model.fit(xs_featurized, ys)\n",
        "  return model\n",
        "\n",
        "def eval_model(model, xs_featurized, ys):\n",
        "  pred_ys = model.predict(xs_featurized)\n",
        "  print(\"test accuracy\", np.mean(pred_ys == ys))\n",
        "\n",
        "def training_experiment(name, featurizer, n_train):\n",
        "    print(f\"{name} features, {n_train} examples\")\n",
        "    train_xs = np.array([\n",
        "        hmm_featurizer(review) \n",
        "        for review in tokenizer.tokenize(train_reviews[:n_train])\n",
        "    ])\n",
        "    train_ys = train_labels[:n_train]\n",
        "    test_xs = np.array([\n",
        "        hmm_featurizer(review)\n",
        "        for review in tokenizer.tokenize(test_reviews)\n",
        "    ])\n",
        "    test_ys = test_labels\n",
        "    model = train_model(train_xs, train_ys)\n",
        "    eval_model(model, test_xs, test_ys)\n",
        "    print()\n",
        "\n",
        "def hmm_featurizer(review):\n",
        "    _, _, gamma = hmm.forward_backward(review)\n",
        "    return gamma.sum(axis=0)\n",
        "\n",
        "training_experiment(\"hmm\", hmm_featurizer, n_train=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6DI4otm0YHe"
      },
      "source": [
        "## Experiments for Part 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovbrCIUT0NGy"
      },
      "source": [
        "# Your code here!"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}