{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HA1_P3_NLP_MIT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cicattzo/mit_advanced_nlp/blob/main/HA1_P3_NLP_MIT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N655YeL2eEUC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10f8e1de-d5c6-4d15-8be7-84ada6b3e650"
      },
      "source": [
        "%%bash\n",
        "!(stat -t /usr/local/lib/*/dist-packages/google/colab > /dev/null 2>&1) && exit \n",
        "rm -rf 6864-hw1\n",
        "git clone https://github.com/mit-6864/hw1.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'hw1'...\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5R8vijdeKgl"
      },
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/hw1\")\n",
        "\n",
        "import csv\n",
        "import itertools as it\n",
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "\n",
        "import torch\n",
        "\n",
        "import lab_util"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaoYiysseNIH"
      },
      "source": [
        "## Hidden Markov Models\n",
        "\n",
        "In the remaining part of the lab (containing part 3) you'll use the Baum--Welch algorithm to learn _categorical_ representations of words in your vocabulary. Answers to questions in this lab should go in the same report as the initial release.\n",
        "\n",
        "As before, we'll start by loading up a dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUn-q_pIeuAV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "702cbaeb-8125-4e17-c66e-1a9696854816"
      },
      "source": [
        "data = []\n",
        "n_positive = 0\n",
        "n_disp = 0\n",
        "with open(\"/content/hw1/reviews.csv\") as reader:\n",
        "  csvreader = csv.reader(reader)\n",
        "  next(csvreader)\n",
        "  for id, review, label in csvreader:\n",
        "    label = int(label)\n",
        "\n",
        "    # hacky class balancing\n",
        "    if label == 1:\n",
        "      if n_positive == 2000:\n",
        "        continue\n",
        "      n_positive += 1\n",
        "    if len(data) == 4000:\n",
        "      break\n",
        "\n",
        "    data.append((review, label))\n",
        "    \n",
        "    if n_disp > 5:\n",
        "      continue\n",
        "    n_disp += 1\n",
        "    print(\"review:\", review)\n",
        "    print(\"rating:\", label, \"(good)\" if label == 1 else \"(bad)\")\n",
        "    print()\n",
        "\n",
        "print(f\"Read {len(data)} total reviews.\")\n",
        "np.random.shuffle(data)\n",
        "reviews, labels = zip(*data)\n",
        "train_reviews = reviews[:3000]\n",
        "train_labels = labels[:3000]\n",
        "val_reviews = reviews[3000:3500]\n",
        "val_labels = labels[3000:3500]\n",
        "test_reviews = reviews[3500:]\n",
        "test_labels = labels[3500:]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "review: I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most.\n",
            "rating: 1 (good)\n",
            "\n",
            "review: Product arrived labeled as Jumbo Salted Peanuts...the peanuts were actually small sized unsalted. Not sure if this was an error or if the vendor intended to represent the product as \"Jumbo\".\n",
            "rating: 0 (bad)\n",
            "\n",
            "review: This is a confection that has been around a few centuries.  It is a light, pillowy citrus gelatin with nuts - in this case Filberts. And it is cut into tiny squares and then liberally coated with powdered sugar.  And it is a tiny mouthful of heaven.  Not too chewy, and very flavorful.  I highly recommend this yummy treat.  If you are familiar with the story of C.S. Lewis' \"The Lion, The Witch, and The Wardrobe\" - this is the treat that seduces Edmund into selling out his Brother and Sisters to the Witch.\n",
            "rating: 1 (good)\n",
            "\n",
            "review: If you are looking for the secret ingredient in Robitussin I believe I have found it.  I got this in addition to the Root Beer Extract I ordered (which was good) and made some cherry soda.  The flavor is very medicinal.\n",
            "rating: 0 (bad)\n",
            "\n",
            "review: Great taffy at a great price.  There was a wide assortment of yummy taffy.  Delivery was very quick.  If your a taffy lover, this is a deal.\n",
            "rating: 1 (good)\n",
            "\n",
            "review: I got a wild hair for taffy and ordered this five pound bag. The taffy was all very enjoyable with many flavors: watermelon, root beer, melon, peppermint, grape, etc. My only complaint is there was a bit too much red/black licorice-flavored pieces (just not my particular favorites). Between me, my kids, and my husband, this lasted only two weeks! I would recommend this brand of taffy -- it was a delightful treat.\n",
            "rating: 1 (good)\n",
            "\n",
            "Read 4000 total reviews.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2qlqRHoe3y-"
      },
      "source": [
        "Next, implement the forward--backward algorithm for HMMs like we saw in class.\n",
        "\n",
        "**IMPORTANT NOTE**: if you directly multiply probabilities as shown on the class slides, you'll get underflow errors. You'll probably want to work in the log domain (remember that `log(ab) = log(a) + log(b)`, `log(exp(a) + exp(b)) = logaddexp(a, b)`). In general, we recommend either `np.logaddexp` or `scipy.special.logsumexp` as safe ways to compute the necessary quantities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Y3BUR2k7EVn",
        "outputId": "17154c47-f1ac-4e57-c59c-dcc6c15c23ad"
      },
      "source": [
        "np.log(a)"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.88840068, -1.13435068, -1.3202967 ],\n",
              "       [-0.50665062, -0.93098973, -5.70587665],\n",
              "       [-0.577294  , -2.11251348, -1.14680236]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51azxKrm7EZ7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wVf4QVIfBdc"
      },
      "source": [
        "# hmm model\n",
        "class HMM(object):\n",
        "    def __init__(self, num_states, num_words):\n",
        "        self.num_states = num_states\n",
        "        self.num_words = num_words\n",
        "\n",
        "        self.states = range(num_states)\n",
        "        self.symbols = range(num_words)\n",
        "\n",
        "        \"\"\"\n",
        "        Initialize the matrix A with random transition probabilities p(j|i)\n",
        "        A should be a matrix of size `num_states x num_states` with rows that\n",
        "        sum to 1.\n",
        "        \"\"\"\n",
        "         # your code here\n",
        "        self.A = np.random.rand(self.num_states,self.num_states)\n",
        "        self.A = self.A/self.A.sum(axis=1)[:,None]\n",
        "        # self.A = torch.from_numpy(self.A)\n",
        "        \"\"\"\n",
        "        Initialize the matrix B with random emission probabilities p(o|i). B \n",
        "        should be a matrix of size `num_states x num_words` with rows that sum \n",
        "        to 1.\n",
        "        \"\"\"\n",
        "       # your code here\n",
        "        self.B = np.random.rand(self.num_states,self.num_words) \n",
        "        self.B = self.B/self.B.sum(axis=1)[:,None]\n",
        "        # self.B = torch.from_numpy(self.B)\n",
        "        \"\"\"\n",
        "        Initialize the vector pi with a random starting distribution. pi should\n",
        "        be a vector of size `num_states` with entries that sum to 1.\n",
        "        \"\"\"\n",
        "        # your code here\n",
        "        self.pi = np.random.random(self.num_states)\n",
        "        self.pi /= self.pi.sum()\n",
        "        # self.pi = torch.from_numpy(self.pi)\n",
        "        # self.pi = None ```\n",
        "\n",
        "\n",
        "    def generate(self, n):\n",
        "        \"\"\"randomly sample the HMM to generate a sequence.\n",
        "        \"\"\"\n",
        "        # we'll give you this one\n",
        "\n",
        "        sequence = []\n",
        "        # initialize the first state\n",
        "        state = np.random.choice(self.states, p=self.pi)\n",
        "        for i in range(n):\n",
        "            # get the emission probs for this state\n",
        "            b = self.B[state, :]\n",
        "            # emit a word\n",
        "            word = np.random.choice(self.symbols, p=b)\n",
        "            sequence.append(word)\n",
        "            # get the transition probs for this state\n",
        "            a = self.A[state, :]\n",
        "            # update the state\n",
        "            state = np.random.choice(self.states, p=a)\n",
        "        return sequence\n",
        "\n",
        "    def forward(self, obs):\n",
        "        \"\"\"\n",
        "        Runs the forward algorithm. This function should return a \n",
        "        `len(obs) x  num_states` matrix where the (t, i)th entry contains \n",
        "        log p(obs[:t], hidden_state_t = i)\n",
        "        \"\"\"\n",
        "\n",
        "        # your code here!\n",
        "\n",
        "        alpha = np.zeros((len(obs), self.num_states))\n",
        "\n",
        "        for s in self.states:\n",
        "          alpha[0][s] = self.pi[s]*self.B[s][obs[0]]\n",
        "\n",
        "        for t in range(1, len(obs)):\n",
        "\n",
        "          for s in self.states:\n",
        "\n",
        "            alpha[t][s] = sum((alpha[t-1][s_i] * self.A[s_i][s] * self.B[s][obs[t]]) for s_i in self.states)\n",
        "\n",
        "\n",
        "        alpha = np.log(alpha)\n",
        "\n",
        "        return alpha\n",
        "\n",
        "    def backward(self, obs):\n",
        "        \"\"\"\n",
        "        Run the backward algorithm. This function should return a\n",
        "        `len(obs) x num_states` matrix where the (t, i)th entry contains\n",
        "        log p(obs[t+1:] | hidden_state_t = i)\n",
        "        \"\"\"\n",
        "\n",
        "        beta = np.zeros((len(obs), self.num_states))\n",
        "\n",
        "        # your code here!\n",
        "\n",
        "        for s in self.states:\n",
        "          beta[len(obs)-1][s] = 1.0\n",
        "\n",
        "        for t in reversed(range(len(obs)-1)):\n",
        "\n",
        "          for s in self.states:\n",
        "\n",
        "            beta[t][s] = sum((beta[t+1][s_i] * self.A[s][s_i] * self.B[s_i][obs[t+1]]) for s_i in self.states)\n",
        "\n",
        "        beta = np.log(beta)\n",
        "\n",
        "        return beta\n",
        "        \n",
        "    def forward_backward(self, obs):\n",
        "        \"\"\"\n",
        "        Compute forward-backward scores\n",
        "\n",
        "        logprob is the total log-probability of the sequence obs (marginalizing\n",
        "        over hidden states).\n",
        "\n",
        "        gamma is a matrix of size `len(obs) x num_states1. It contains the\n",
        "        marginal probability of being in state i at time t\n",
        "\n",
        "        xi is a tensor of size `len(obs) x num_states x num_states`. It contains\n",
        "        the marginal probability of transitioning from i to j at t.\n",
        "        \"\"\"\n",
        "\n",
        "        logprob = None\n",
        "        xi = None\n",
        "        gamma = None\n",
        "        # your code here!\n",
        "\n",
        "        return logprob, xi, gamma\n",
        "\n",
        "        \"\"\"\n",
        "        SANITY CHECK\n",
        "\n",
        "        The most straightforward way of implementing the forward, backward, and \n",
        "        forward_backward methods would be to iterate through all the values and \n",
        "        use the formulas in the slides to calculate the corresponding values.\n",
        "\n",
        "        However, this may not be fast enough. If your model is taking too long\n",
        "        to train, consider how you may speed up your code by reducing the number\n",
        "        of for loops involved. How can you reformulate your code using matrix\n",
        "        operations?\n",
        "\n",
        "        Hint: we were able to implement each of the forward, backward, and\n",
        "        forward_backward operations using only one for loop.\n",
        "        \"\"\"\n",
        "\n",
        "    def learn_unsupervised(self, corpus, num_iters, print_every=10):\n",
        "        \"\"\"Run the Baum Welch EM algorithm\n",
        "        \n",
        "        corpus: the data to learn from\n",
        "        num_iters: the number of iterations to run the algorithm\n",
        "        print_every: how often to print the log-likelihood while the model is\n",
        "        updating its parameters.\n",
        "        \"\"\"\n",
        "\n",
        "        for i_iter in range(num_iters):\n",
        "            \"\"\"\n",
        "            expected_si: a vector of size (num_states,) where the i-th entry is\n",
        "            the expected number of times a sentence is transitioning from state \n",
        "            i to some other state.\n",
        "\n",
        "            expected_sij: an array of size (num_states, num_states) where the\n",
        "            (i,j)-th entry represents the expected number of state transitions\n",
        "            between state i and state j.\n",
        "\n",
        "            expected_sjwk: an array of size (num_states, num_words) where the \n",
        "            (j,k)-th entry represents the expected number of times the word w_k \n",
        "            appears when at state j.\n",
        "\n",
        "            expected_q1: a vector of size (num_states,) where the i-th entry is \n",
        "            the expected number of times state i is the first state.\n",
        "\n",
        "            total_logprob: The log of the probability of the corpus being\n",
        "            generated with the current parameters of the HMM.\n",
        "            \"\"\"\n",
        "            expected_si = None # your code here\n",
        "            expected_sij = None # your code here\n",
        "            expected_sjwk = None # your code here\n",
        "            expected_q1 = None # your code here\n",
        "            total_logprob = 0\n",
        "            \n",
        "            for review in corpus:\n",
        "                logprob, xi, gamma = self.forward_backward(review)\n",
        "                # your code here \n",
        "            if i_iter % print_every == 0:\n",
        "              print(\"log-likelihood\", total_logprob)\n",
        "\n",
        "            \"\"\"\n",
        "            The following variables should be the new values of self.A, self.B,\n",
        "            and self.pi after the values are updated.\n",
        "            \"\"\"\n",
        "            A_new = None # your code here\n",
        "            B_new = None # your code here\n",
        "            pi_new = None # your code here\n",
        "\n",
        "            self.A = A_new\n",
        "            self.B = B_new\n",
        "            self.pi = pi_new"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "c04sNBIqqs8N",
        "outputId": "dc8f6217-c08a-4e99-daaa-0b151efe2c8f"
      },
      "source": [
        "init_test()\n",
        "forward_test()\n",
        "backward_test()\n",
        "forward_backward_test()\n",
        "baum_welch_update_test()\n",
        "end_to_end_test()\n",
        "\n",
        "\"\"\"\n",
        "Note: The end_to_end_test is not as robustg due to it using random starts. Try\n",
        "running the test case a few times to see if you get a good result at least a few\n",
        "times before deciding that your code is buggy.\n",
        "\"\"\""
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The result of the forward function should be [[ -2.96913  -3.43382]\n",
            " [ -4.66005  -9.19418]\n",
            " [ -7.35001  -7.89695]\n",
            " [ -9.65069  -9.95363]\n",
            " [-11.25815 -14.27392]\n",
            " [-18.14079 -14.4781 ]\n",
            " [-16.89275 -18.62696]\n",
            " [-19.45549 -20.17289]\n",
            " [-21.53772 -23.283  ]\n",
            " [-23.4927  -26.69119]\n",
            " [-25.84891 -26.73817]\n",
            " [-28.12237 -29.92402]]\n",
            "Your value of alpha is: [[ -2.96913  -3.43382]\n",
            " [ -4.66005  -9.19418]\n",
            " [ -7.35001  -7.89695]\n",
            " [ -9.65069  -9.95363]\n",
            " [-11.25815 -14.27392]\n",
            " [-18.14079 -14.4781 ]\n",
            " [-16.89275 -18.62697]\n",
            " [-19.45549 -20.17289]\n",
            " [-21.53772 -23.283  ]\n",
            " [-23.4927  -26.69119]\n",
            " [-25.84891 -26.73817]\n",
            " [-28.12237 -29.92402]]\n",
            "The result of the backward function should be [[-25.42937 -25.58918]\n",
            " [-23.32164 -23.19959]\n",
            " [-21.11007 -21.02033]\n",
            " [-18.82215 -18.94381]\n",
            " [-16.78523 -16.33951]\n",
            " [-13.42847 -13.51924]\n",
            " [-11.24815 -11.19161]\n",
            " [ -8.88679  -8.96441]\n",
            " [ -6.57374  -6.70985]\n",
            " [ -4.51873  -4.47419]\n",
            " [ -2.44529  -2.51463]\n",
            " [  0.        0.     ]]\n",
            "Your value of beta is: [[-25.42937 -25.58918]\n",
            " [-23.32164 -23.19959]\n",
            " [-21.11007 -21.02033]\n",
            " [-18.82215 -18.94381]\n",
            " [-16.78523 -16.33951]\n",
            " [-13.42847 -13.51924]\n",
            " [-11.24815 -11.19161]\n",
            " [ -8.88679  -8.96441]\n",
            " [ -6.57374  -6.70985]\n",
            " [ -4.51873  -4.47419]\n",
            " [ -2.44529  -2.51463]\n",
            " [  0.        0.     ]]\n",
            "The value of logprob should be: -27.9693\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-113-2561e295b88b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mforward_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbackward_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mforward_backward_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mbaum_welch_update_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mend_to_end_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-6474f77d16c6>\u001b[0m in \u001b[0;36mforward_backward_test\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The value of logprob should be:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m27.9693\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Your value of logprob is:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogprob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     print(\"The value of xi should be:\", np.array([[[0.64523, 0.00601],\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mround_\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mround_\u001b[0;34m(a, decimals, out)\u001b[0m\n\u001b[1;32m   3635\u001b[0m     \u001b[0maround\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mequivalent\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0msee\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdetails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3636\u001b[0m     \"\"\"\n\u001b[0;32m-> 3637\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0maround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecimals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecimals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3639\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36maround\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36maround\u001b[0;34m(a, decimals, out)\u001b[0m\n\u001b[1;32m   3260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3261\u001b[0m     \"\"\"\n\u001b[0;32m-> 3262\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'round'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecimals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecimals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbound\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapit\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mwrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'NoneType' and 'float'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yFF2rKuuh6U"
      },
      "source": [
        "## Test Cases\r\n",
        "\r\n",
        "The following are test cases that are meant to help you debug your code. The code involves six test suites - an initialization test, a forward test, a backward test, a forward_backward test, a baum_welch_update test, and a final end_to_end test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3gRZ_r_vLav"
      },
      "source": [
        "def init_test():\r\n",
        "\r\n",
        "    num_states = np.random.randint(100)\r\n",
        "    num_words = np.random.randint(100)\r\n",
        "    model = HMM(num_states, num_words)\r\n",
        "\r\n",
        "    assert model.A.shape == (num_states, num_states)\r\n",
        "    assert model.B.shape == (num_states, num_words)\r\n",
        "    assert model.pi.shape == (num_states, )\r\n",
        "\r\n",
        "    assert np.linalg.norm(np.sum(model.A, axis=1) - np.ones(num_states)) < 1e-10\r\n",
        "    assert np.linalg.norm(np.sum(model.B, axis=1) - np.ones(num_states)) < 1e-10\r\n",
        "    assert np.linalg.norm(np.sum(model.pi) - 1) < 1e-10\r\n",
        "\r\n",
        "def forward_test():\r\n",
        "    model = HMM(2, 10)\r\n",
        "    model.A = np.array([[0.79034887, 0.20965113],\r\n",
        "                        [0.66824331, 0.33175669]])\r\n",
        "    model.B = np.array([[0.08511814, 0.06627238, 0.08487461, 0.15607959, 0.00124582, 0.12984083, 0.11164849, 0.11591902, 0.15232716, 0.09667395],\r\n",
        "                        [0.18425462, 0.14326559, 0.14026994, 0.0215989,  0.17687124, 0.04681278, 0.05857451, 0.17451212, 0.00473382, 0.04910648]])\r\n",
        "    model.pi = np.array([0.77480039, 0.22519961])\r\n",
        "    obs = [1, 8, 0, 0, 3, 4, 5, 2, 6, 3, 7, 9]\r\n",
        "    alpha = model.forward(obs)\r\n",
        "\r\n",
        "    print(\"The result of the forward function should be\", np.array([[-2.96913, -3.43382],\r\n",
        "                                                                    [ -4.66005, -9.19418],\r\n",
        "                                                                    [ -7.35001, -7.89695],\r\n",
        "                                                                    [ -9.65069, -9.95363],\r\n",
        "                                                                    [-11.25815, -14.27392],\r\n",
        "                                                                    [-18.14079, -14.4781 ],\r\n",
        "                                                                    [-16.89275, -18.62696],\r\n",
        "                                                                    [-19.45549, -20.17289],\r\n",
        "                                                                    [-21.53772, -23.283  ],\r\n",
        "                                                                    [-23.4927, -26.69119],\r\n",
        "                                                                    [-25.84891, -26.73817],\r\n",
        "                                                                    [-28.12237, -29.92402]]))\r\n",
        "    print(\"Your value of alpha is:\", np.round(alpha, 5))\r\n",
        "\r\n",
        "def backward_test():\r\n",
        "    model = HMM(2, 10)\r\n",
        "    model.A = np.array([[0.79034887, 0.20965113],\r\n",
        "                        [0.66824331, 0.33175669]])\r\n",
        "    model.B = np.array([[0.08511814, 0.06627238, 0.08487461, 0.15607959, 0.00124582, 0.12984083, 0.11164849, 0.11591902, 0.15232716, 0.09667395],\r\n",
        "                        [0.18425462, 0.14326559, 0.14026994, 0.0215989,  0.17687124, 0.04681278, 0.05857451, 0.17451212, 0.00473382, 0.04910648]])\r\n",
        "    model.pi = np.array([0.77480039, 0.22519961])\r\n",
        "    obs = [1, 8, 0, 0, 3, 4, 5, 2, 6, 3, 7, 9]\r\n",
        "    beta = model.backward(obs)\r\n",
        "\r\n",
        "    print(\"The result of the backward function should be\", np.array([[-25.42937, -25.58918], \r\n",
        "                                                                     [-23.32164, -23.19959],\r\n",
        "                                                                     [-21.11007, -21.02033],\r\n",
        "                                                                     [-18.82215, -18.94381],\r\n",
        "                                                                     [-16.78523, -16.33951],\r\n",
        "                                                                     [-13.42847, -13.51924],\r\n",
        "                                                                     [-11.24815, -11.19161],\r\n",
        "                                                                     [ -8.88679,  -8.96441],\r\n",
        "                                                                     [ -6.57374,  -6.70985],\r\n",
        "                                                                     [ -4.51873,  -4.47419],\r\n",
        "                                                                     [ -2.44529,  -2.51463],\r\n",
        "                                                                     [  0, 0]]))\r\n",
        "\r\n",
        "    print(\"Your value of beta is:\", np.round(beta, 5))\r\n",
        "\r\n",
        "\r\n",
        "def forward_backward_test():\r\n",
        "    model = HMM(2, 10)\r\n",
        "    model.A = np.array([[0.79034887, 0.20965113],\r\n",
        "                        [0.66824331, 0.33175669]])\r\n",
        "    model.B = np.array([[0.08511814, 0.06627238, 0.08487461, 0.15607959, 0.00124582, 0.12984083, 0.11164849, 0.11591902, 0.15232716, 0.09667395],\r\n",
        "                        [0.18425462, 0.14326559, 0.14026994, 0.0215989,  0.17687124, 0.04681278, 0.05857451, 0.17451212, 0.00473382, 0.04910648]])\r\n",
        "    model.pi = np.array([0.77480039, 0.22519961])\r\n",
        "    obs = [1, 8, 0, 0, 3, 4, 5, 2, 6, 3, 7, 9]\r\n",
        "    logprob, xi, gamma = model.forward_backward(obs)\r\n",
        "\r\n",
        "    print(\"The value of logprob should be:\", -27.9693)\r\n",
        "    print(\"Your value of logprob is:\", np.round(logprob, 5))\r\n",
        "\r\n",
        "    print(\"The value of xi should be:\", np.array([[[0.64523, 0.00601],\r\n",
        "                                                  [0.34278, 0.00598]],\r\n",
        "\r\n",
        "                                                 [[0.60684, 0.38117],\r\n",
        "                                                  [0.00551, 0.00648]],\r\n",
        "\r\n",
        "                                                 [[0.40595, 0.2064 ],\r\n",
        "                                                  [0.19863, 0.18902]],\r\n",
        "\r\n",
        "                                                 [[0.5718,  0.03278],\r\n",
        "                                                  [0.35711, 0.03831]],\r\n",
        "\r\n",
        "                                                 [[0.02625, 0.90266],\r\n",
        "                                                  [0.00109, 0.07   ]],\r\n",
        "\r\n",
        "                                                 [[0.02482, 0.00251],\r\n",
        "                                                  [0.81777, 0.15489]],\r\n",
        "\r\n",
        "                                                 [[0.59943, 0.24316],\r\n",
        "                                                  [0.08947, 0.06793]],\r\n",
        "\r\n",
        "                                                 [[0.6143,  0.07461],\r\n",
        "                                                  [0.25347, 0.05762]],\r\n",
        "\r\n",
        "                                                 [[0.8357,  0.03207],\r\n",
        "                                                  [0.12337, 0.00886]],\r\n",
        "\r\n",
        "                                                 [[0.69872, 0.26034],\r\n",
        "                                                  [0.02412, 0.01682]],\r\n",
        "\r\n",
        "                                                 [[0.63701, 0.08583],\r\n",
        "                                                  [0.22134, 0.05582]]]))\r\n",
        "    print(\"Your value of xi is:\", np.round(xi, 5))\r\n",
        "\r\n",
        "    print(\"The value of gamma should be:\", np.array([[0.65124, 0.34876],\r\n",
        "                                                    [0.98802, 0.01198],\r\n",
        "                                                    [0.61235, 0.38765],\r\n",
        "                                                    [0.60458, 0.39542],\r\n",
        "                                                    [0.92891, 0.07109],\r\n",
        "                                                    [0.02733, 0.97267],\r\n",
        "                                                    [0.8426,  0.1574 ],\r\n",
        "                                                    [0.68891, 0.31109],\r\n",
        "                                                    [0.86777, 0.13223],\r\n",
        "                                                    [0.95906, 0.04094],\r\n",
        "                                                    [0.72284, 0.27716],\r\n",
        "                                                    [0.85835, 0.14165]]))\r\n",
        "\r\n",
        "    print(\"Your value of gamma is:\", np.round(gamma, 5))\r\n",
        "\r\n",
        "def baum_welch_update_test():\r\n",
        "    model = HMM(4, 10)\r\n",
        "    \r\n",
        "    model.A = np.array([[0.05263151, 0.62161178, 0.06683182, 0.25892489],\r\n",
        "                        [0.26993274, 0.13114741, 0.32305468, 0.27586517],\r\n",
        "                        [0.2951958,  0.14576492, 0.22474111, 0.33429817],\r\n",
        "                        [0.29586018, 0.26065884, 0.1977772,  0.24570378]])\r\n",
        "    \r\n",
        "    model.B = np.array([[0.01800425, 0.09767131, 0.17824799, 0.12586453, 0.19514548, 0.05433139, 0.01995667, 0.12985343, 0.01884263, 0.16208232],\r\n",
        "                        [0.04512782, 0.09469685, 0.1426164,  0.13851362, 0.08717793, 0.17152532, 0.08746939, 0.04900339, 0.05315859, 0.13071069],\r\n",
        "                        [0.11055806, 0.10592473, 0.0051817,  0.07721441, 0.21761783, 0.20323146, 0.18881598, 0.00584989, 0.00682669, 0.07877924],\r\n",
        "                        [0.08711377, 0.16703645, 0.0706214,  0.05297571, 0.10486868, 0.16794587, 0.13562053, 0.15729142, 0.03345308, 0.02307309]])\r\n",
        "    \r\n",
        "    model.pi = np.array([0.21186864, 0.27156561, 0.37188523, 0.14468051])\r\n",
        "    \r\n",
        "    corpus = np.array([[7,3,2,5,0,3,2,9,4,2], [7,3,2,4,2,8,7,5,0,8], [7,3,2,3,1,7,3,8,6,7], [7,3,2,6,4,4,3,4,0,0]])\r\n",
        "\r\n",
        "    model.learn_unsupervised(corpus, 200)\r\n",
        "\r\n",
        "    print(\"hmm.A should be\", np.array([[0, 1, 0, 0], \r\n",
        "                                     [0.14122, 0, 0.27099, 0.58779], \r\n",
        "                                     [0.20671, 0, 0, 0.79329], \r\n",
        "                                     [0, 0.90909, 0.09091, 0]]))\r\n",
        "    print(\"Your implementation has hmm.A to be\", np.round(model.A, 5))\r\n",
        "\r\n",
        "    print(\"hmm.B should be\", np.array([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\r\n",
        "                                              [0.0625, 0, 0, 0.5, 0, 0.125, 0.125, 0, 0.125, 0.0625],\r\n",
        "                                              [0, 0.20671, 0, 0, 0.79329, 0, 0, 0, 0, 0],\r\n",
        "                                              [0.24667, 0, 0.57555, 0, 0.09556, 0, 0, 0, 0.08222, 0]]))\r\n",
        "    print(\"Your implementation has hmm.B to be\", np.round(model.B, 5))\r\n",
        "\r\n",
        "    print(\"hmm.pi should be\", np.array([1, 0, 0, 0]))\r\n",
        "\r\n",
        "    print(\"Your implementation has hmm.pi to be\", np.round(model.pi, 5))\r\n",
        "\r\n",
        "def end_to_end_test():\r\n",
        "    # Test Case 1\r\n",
        "\r\n",
        "    corpus = np.array([[0,3,0,3,0,3,0,3,0,3,0,3], [0,2,0,2,0,2,0,2,0,2,0,2,0], [1,2,1,2,1,2,1,2,1,2,1,2],[1,3,1,3,1,3,1,3,1,3]])\r\n",
        "    hmm = HMM(num_states=2,num_words=4)\r\n",
        "    hmm.learn_unsupervised(corpus, 10)\r\n",
        "    print(\"After this test case, hmm.A should either be approximately,\",  np.array([0, 1], [1, 0]))\r\n",
        "    print(\"This is your current value of hmm.A: \", np.round(hmm.A, 5))\r\n",
        "\r\n",
        "    print(\"After this test case, hmm.B should either be approximately,\", np.array([[0, 0, 0.5, 0.5], [0.5, 0.5, 0, 0]]), \" or it should be \", np.array([[0.5, 0.5, 0, 0], [0, 0, 0.5, 0.5]]))\r\n",
        "    print(\"This is your current value of hmm.B: \", np.round(hmm.B, 5))\r\n",
        "\r\n",
        "    # Test Case 2\r\n",
        "\r\n",
        "    corpus = np.array([[0,0,0,0,0,0,0,0,0,0], [1,1,1,1,1,1,1,1,1,1], [2,2,2,2,2,2,2,2,2,2]])\r\n",
        "    hmm = HMM(num_states=3, num_words=3)\r\n",
        "    hmm.learn_unsupervised(corpus, 100)\r\n",
        "    print(\"After this test case, hmm.A should be the identity matrix\", np.eye(3))\r\n",
        "    print(\"This is your current value of hmm.A: \", np.round(hmm.A, 5))\r\n",
        "\r\n",
        "    print(\"After this test case, hmm.B should be some 3 by 3 permutation matrix\")\r\n",
        "    print(\"This is your current value of hmm.B: \", np.round(hmm.B, 5))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmewPFV2MPlS"
      },
      "source": [
        "## Test\r\n",
        "\r\n",
        "To actually run the test cases, run the cell below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFtzZ9W9MVKu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "outputId": "63c9c076-effa-4edc-d1d5-633c11a66dfa"
      },
      "source": [
        "init_test()\r\n",
        "forward_test()\r\n",
        "backward_test()\r\n",
        "forward_backward_test()\r\n",
        "baum_welch_update_test()\r\n",
        "end_to_end_test()\r\n",
        "\r\n",
        "\"\"\"\r\n",
        "Note: The end_to_end_test is not as robustg due to it using random starts. Try\r\n",
        "running the test case a few times to see if you get a good result at least a few\r\n",
        "times before deciding that your code is buggy.\r\n",
        "\"\"\""
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-2561e295b88b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minit_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mforward_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbackward_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mforward_backward_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbaum_welch_update_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-6474f77d16c6>\u001b[0m in \u001b[0;36minit_test\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnum_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1e-10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1e-10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1e-10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2241\u001b[0m     return _wrapreduction(a, np.add, 'sum', axis, dtype, out, keepdims=keepdims,\n\u001b[0;32m-> 2242\u001b[0;31m                           initial=initial, where=where)\n\u001b[0m\u001b[1;32m   2243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: sum() received an invalid combination of arguments - got (out=NoneType, axis=int, ), but expected one of:\n * (*, torch.dtype dtype)\n      didn't match because some of the keywords were incorrect: out, axis\n * (tuple of names dim, bool keepdim, *, torch.dtype dtype)\n * (tuple of ints dim, bool keepdim, *, torch.dtype dtype)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eF-l7WucpCBP"
      },
      "source": [
        "## Training\r\n",
        "\r\n",
        "Train a model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTWXUt15pDg4"
      },
      "source": [
        "tokenizer = lab_util.Tokenizer()\n",
        "tokenizer.fit(train_reviews)\n",
        "train_reviews_tk = tokenizer.tokenize(train_reviews)\n",
        "print(tokenizer.vocab_size)\n",
        "\n",
        "hmm = HMM(num_states=10, num_words=tokenizer.vocab_size)\n",
        "hmm.learn_unsupervised(train_reviews_tk, 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiCwE05xqXmI"
      },
      "source": [
        "Let's look at some of the words associated with each hidden state:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXhMoLUFqbn_"
      },
      "source": [
        "for i in range(hmm.num_states):\n",
        "    most_probable = np.argsort(hmm.B[i, :])[:10]\n",
        "    print(f\"state {i}\")\n",
        "    for o in most_probable:\n",
        "        print(tokenizer.token_to_word[o], hmm.B[i, o])\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAQ_PmASwdFz"
      },
      "source": [
        "We can also look at some samples from the model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tj1eT3s3wgFJ"
      },
      "source": [
        "for i in range(10):\n",
        "    print(tokenizer.de_tokenize([hmm.generate(10)]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9Qk9adNr7lQ"
      },
      "source": [
        "Finally, let's repeat the classification experiment from Parts 1 and 2, using the _vector of expected hidden state counts_ as a sentence representation.\n",
        "\n",
        "(Warning! results may not be the same as in earlier versions of this experiment.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mL6JQXLJspyA"
      },
      "source": [
        "def train_model(xs_featurized, ys):\n",
        "  import sklearn.linear_model\n",
        "  model = sklearn.linear_model.LogisticRegression()\n",
        "  model.fit(xs_featurized, ys)\n",
        "  return model\n",
        "\n",
        "def eval_model(model, xs_featurized, ys):\n",
        "  pred_ys = model.predict(xs_featurized)\n",
        "  print(\"test accuracy\", np.mean(pred_ys == ys))\n",
        "\n",
        "def training_experiment(name, featurizer, n_train):\n",
        "    print(f\"{name} features, {n_train} examples\")\n",
        "    train_xs = np.array([\n",
        "        hmm_featurizer(review) \n",
        "        for review in tokenizer.tokenize(train_reviews[:n_train])\n",
        "    ])\n",
        "    train_ys = train_labels[:n_train]\n",
        "    test_xs = np.array([\n",
        "        hmm_featurizer(review)\n",
        "        for review in tokenizer.tokenize(test_reviews)\n",
        "    ])\n",
        "    test_ys = test_labels\n",
        "    model = train_model(train_xs, train_ys)\n",
        "    eval_model(model, test_xs, test_ys)\n",
        "    print()\n",
        "\n",
        "def hmm_featurizer(review):\n",
        "    _, _, gamma = hmm.forward_backward(review)\n",
        "    return gamma.sum(axis=0)\n",
        "\n",
        "training_experiment(\"hmm\", hmm_featurizer, n_train=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6DI4otm0YHe"
      },
      "source": [
        "## Experiments for Part 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovbrCIUT0NGy"
      },
      "source": [
        "# Your code here!"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}