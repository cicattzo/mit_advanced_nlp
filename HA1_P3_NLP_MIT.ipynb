{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HA1_P3_NLP_MIT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cicattzo/mit_advanced_nlp/blob/main/HA1_P3_NLP_MIT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N655YeL2eEUC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84c04931-4067-4250-8cd3-4892dce66fdc"
      },
      "source": [
        "%%bash\n",
        "!(stat -t /usr/local/lib/*/dist-packages/google/colab > /dev/null 2>&1) && exit \n",
        "rm -rf 6864-hw1\n",
        "git clone https://github.com/mit-6864/hw1.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'hw1'...\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5R8vijdeKgl"
      },
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/hw1\")\n",
        "\n",
        "import csv\n",
        "import itertools as it\n",
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "\n",
        "import torch\n",
        "from scipy.special import logsumexp\n",
        "import scipy\n",
        "\n",
        "import lab_util"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaoYiysseNIH"
      },
      "source": [
        "## Hidden Markov Models\n",
        "\n",
        "In the remaining part of the lab (containing part 3) you'll use the Baum--Welch algorithm to learn _categorical_ representations of words in your vocabulary. Answers to questions in this lab should go in the same report as the initial release.\n",
        "\n",
        "As before, we'll start by loading up a dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUn-q_pIeuAV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8aaa1c9c-cfbc-492d-b7ed-2df7d4eef0b1"
      },
      "source": [
        "data = []\n",
        "n_positive = 0\n",
        "n_disp = 0\n",
        "with open(\"/content/hw1/reviews.csv\") as reader:\n",
        "  csvreader = csv.reader(reader)\n",
        "  next(csvreader)\n",
        "  for id, review, label in csvreader:\n",
        "    label = int(label)\n",
        "\n",
        "    # hacky class balancing\n",
        "    if label == 1:\n",
        "      if n_positive == 2000:\n",
        "        continue\n",
        "      n_positive += 1\n",
        "    if len(data) == 4000:\n",
        "      break\n",
        "\n",
        "    data.append((review, label))\n",
        "    \n",
        "    if n_disp > 5:\n",
        "      continue\n",
        "    n_disp += 1\n",
        "    print(\"review:\", review)\n",
        "    print(\"rating:\", label, \"(good)\" if label == 1 else \"(bad)\")\n",
        "    print()\n",
        "\n",
        "print(f\"Read {len(data)} total reviews.\")\n",
        "np.random.shuffle(data)\n",
        "reviews, labels = zip(*data)\n",
        "train_reviews = reviews[:3000]\n",
        "train_labels = labels[:3000]\n",
        "val_reviews = reviews[3000:3500]\n",
        "val_labels = labels[3000:3500]\n",
        "test_reviews = reviews[3500:]\n",
        "test_labels = labels[3500:]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "review: I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most.\n",
            "rating: 1 (good)\n",
            "\n",
            "review: Product arrived labeled as Jumbo Salted Peanuts...the peanuts were actually small sized unsalted. Not sure if this was an error or if the vendor intended to represent the product as \"Jumbo\".\n",
            "rating: 0 (bad)\n",
            "\n",
            "review: This is a confection that has been around a few centuries.  It is a light, pillowy citrus gelatin with nuts - in this case Filberts. And it is cut into tiny squares and then liberally coated with powdered sugar.  And it is a tiny mouthful of heaven.  Not too chewy, and very flavorful.  I highly recommend this yummy treat.  If you are familiar with the story of C.S. Lewis' \"The Lion, The Witch, and The Wardrobe\" - this is the treat that seduces Edmund into selling out his Brother and Sisters to the Witch.\n",
            "rating: 1 (good)\n",
            "\n",
            "review: If you are looking for the secret ingredient in Robitussin I believe I have found it.  I got this in addition to the Root Beer Extract I ordered (which was good) and made some cherry soda.  The flavor is very medicinal.\n",
            "rating: 0 (bad)\n",
            "\n",
            "review: Great taffy at a great price.  There was a wide assortment of yummy taffy.  Delivery was very quick.  If your a taffy lover, this is a deal.\n",
            "rating: 1 (good)\n",
            "\n",
            "review: I got a wild hair for taffy and ordered this five pound bag. The taffy was all very enjoyable with many flavors: watermelon, root beer, melon, peppermint, grape, etc. My only complaint is there was a bit too much red/black licorice-flavored pieces (just not my particular favorites). Between me, my kids, and my husband, this lasted only two weeks! I would recommend this brand of taffy -- it was a delightful treat.\n",
            "rating: 1 (good)\n",
            "\n",
            "Read 4000 total reviews.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2qlqRHoe3y-"
      },
      "source": [
        "Next, implement the forward--backward algorithm for HMMs like we saw in class.\n",
        "\n",
        "**IMPORTANT NOTE**: if you directly multiply probabilities as shown on the class slides, you'll get underflow errors. You'll probably want to work in the log domain (remember that `log(ab) = log(a) + log(b)`, `log(exp(a) + exp(b)) = logaddexp(a, b)`). In general, we recommend either `np.logaddexp` or `scipy.special.logsumexp` as safe ways to compute the necessary quantities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wVf4QVIfBdc"
      },
      "source": [
        "# hmm model\n",
        "class HMM(object):\n",
        "    def __init__(self, num_states, num_words):\n",
        "        self.num_states = num_states\n",
        "        self.num_words = num_words\n",
        "\n",
        "        self.states = range(num_states)\n",
        "        self.symbols = range(num_words)\n",
        "\n",
        "        \"\"\"\n",
        "        Initialize the matrix A with random transition probabilities p(j|i)\n",
        "        A should be a matrix of size `num_states x num_states` with rows that\n",
        "        sum to 1.\n",
        "        \"\"\"\n",
        "         # your code here\n",
        "        self.A = np.random.rand(self.num_states,self.num_states)\n",
        "        self.A = self.A/self.A.sum(axis=1)[:,None]\n",
        "        # self.A = torch.from_numpy(self.A)\n",
        "        \"\"\"\n",
        "        Initialize the matrix B with random emission probabilities p(o|i). B \n",
        "        should be a matrix of size `num_states x num_words` with rows that sum \n",
        "        to 1.\n",
        "        \"\"\"\n",
        "       # your code here\n",
        "        self.B = np.random.rand(self.num_states,self.num_words) \n",
        "        self.B = self.B/self.B.sum(axis=1)[:,None]\n",
        "        # self.B = torch.from_numpy(self.B)\n",
        "        \"\"\"\n",
        "        Initialize the vector pi with a random starting distribution. pi should\n",
        "        be a vector of size `num_states` with entries that sum to 1.\n",
        "        \"\"\"\n",
        "        # your code here\n",
        "        self.pi = np.random.random(self.num_states)\n",
        "        self.pi /= self.pi.sum()\n",
        "        # self.pi = torch.from_numpy(self.pi)\n",
        "        # self.pi = None ```\n",
        "\n",
        "\n",
        "    def generate(self, n):\n",
        "        \"\"\"randomly sample the HMM to generate a sequence.\n",
        "        \"\"\"\n",
        "        # we'll give you this one\n",
        "\n",
        "        sequence = []\n",
        "        # initialize the first state\n",
        "        state = np.random.choice(self.states, p=self.pi)\n",
        "        for i in range(n):\n",
        "            # get the emission probs for this state\n",
        "            b = self.B[state, :]\n",
        "            # emit a word\n",
        "            word = np.random.choice(self.symbols, p=b)\n",
        "            sequence.append(word)\n",
        "            # get the transition probs for this state\n",
        "            a = self.A[state, :]\n",
        "            # update the state\n",
        "            state = np.random.choice(self.states, p=a)\n",
        "        return sequence\n",
        "\n",
        "    def forward(self, obs):\n",
        "        \"\"\"\n",
        "        Runs the forward algorithm. This function should return a \n",
        "        `len(obs) x  num_states` matrix where the (t, i)th entry contains \n",
        "        log p(obs[:t], hidden_state_t = i)\n",
        "        \"\"\"\n",
        "\n",
        "        # your code here!\n",
        "\n",
        "        alpha = np.zeros((len(obs), self.num_states))\n",
        "\n",
        "        for s in self.states:\n",
        "          alpha[0][s] = np.log(self.pi[s])+np.log(self.B[s][obs[0]])\n",
        "\n",
        "        for t in range(1, len(obs)):\n",
        "\n",
        "          for s in self.states:\n",
        "\n",
        "            temp = []\n",
        "\n",
        "            for s_i in self.states:\n",
        "\n",
        "              temp.append(alpha[t-1][s_i] + np.log(self.A[s_i][s]) + np.log(self.B[s][obs[t]]))\n",
        "\n",
        "            temp = np.array(temp)\n",
        "\n",
        "            # alpha[t][s] = sum((alpha[t-1][s_i] * self.A[s_i][s] * self.B[s][obs[t]]) for s_i in self.states)\n",
        "\n",
        "            alpha[t][s] = logsumexp(temp)\n",
        "\n",
        "        # alpha = np.log(alpha)\n",
        "\n",
        "        return alpha\n",
        "\n",
        "    def backward(self, obs):\n",
        "        \"\"\"\n",
        "        Run the backward algorithm. This function should return a\n",
        "        `len(obs) x num_states` matrix where the (t, i)th entry contains\n",
        "        log p(obs[t+1:] | hidden_state_t = i)\n",
        "        \"\"\"\n",
        "\n",
        "        beta = np.zeros((len(obs), self.num_states))\n",
        "\n",
        "        # your code here!\n",
        "\n",
        "        for s in self.states:\n",
        "          beta[len(obs)-1][s] = 1.0\n",
        "\n",
        "        for t in reversed(range(len(obs)-1)):\n",
        "\n",
        "          for s in self.states:\n",
        "\n",
        "            # temp = []\n",
        "\n",
        "            # for s_i in self.states:\n",
        "              \n",
        "              # temp.append(np.log(beta[t+1][s_i]) + np.log(self.A[s][s_i]) + np.log(self.B[s_i][obs[t+1]]))\n",
        "\n",
        "            # temp = np.array(temp)\n",
        "\n",
        "            beta[t][s] = sum((beta[t+1][s_i] * self.A[s][s_i] * self.B[s_i][obs[t+1]]) for s_i in self.states)\n",
        "\n",
        "            # beta[t][s] = logsumexp(temp)\n",
        "\n",
        "        beta = np.log(beta)\n",
        "\n",
        "        return beta\n",
        "        \n",
        "    def forward_backward(self, obs):\n",
        "        \"\"\"\n",
        "        Compute forward-backward scores\n",
        "\n",
        "        logprob is the total log-probability of the sequence obs (marginalizing\n",
        "        over hidden states).\n",
        "\n",
        "        gamma is a matrix of size `len(obs) x num_states1. It contains the\n",
        "        marginal probability of being in state i at time t\n",
        "\n",
        "        xi is a tensor of size `len(obs) x num_states x num_states`. It contains\n",
        "        the marginal probability of transitioning from i to j at t.\n",
        "        \"\"\"\n",
        "        #create forward and backward\n",
        "        forward = self.forward(obs)\n",
        "        forward_exp = np.exp(forward)\n",
        "        backward = self.backward(obs)\n",
        "        backward_exp = np.exp(backward)\n",
        "\n",
        "        #calculate logprob\n",
        "        logprob = logsumexp(np.array(forward[len(obs)-1]))\n",
        "        #calculate gamma\n",
        "        gamma = np.zeros((len(obs), self.num_states))\n",
        "\n",
        "        xi = np.zeros((len(obs), self.num_states, self.num_states))\n",
        "\n",
        "        for t in range(len(obs)):\n",
        "\n",
        "          for s in self.states:\n",
        "\n",
        "            gamma[t][s] = np.exp(forward[t][s]+backward[t][s]-logprob)\n",
        "\n",
        "            if t == len(obs)-1:\n",
        "              continue\n",
        "\n",
        "            for s_i in self.states:\n",
        "\n",
        "              xi[t][s][s_i] = np.exp(forward[t][s] + np.log(self.A[s][s_i]) + np.log(self.B[s_i][obs[t+1]]) + backward[t+1][s_i] - logprob)\n",
        "\n",
        "        # your code here!\n",
        "\n",
        "        return logprob, xi, gamma\n",
        "\n",
        "        \"\"\"\n",
        "        SANITY CHECK\n",
        "\n",
        "        The most straightforward way of implementing the forward, backward, and \n",
        "        forward_backward methods would be to iterate through all the values and \n",
        "        use the formulas in the slides to calculate the corresponding values.\n",
        "\n",
        "        However, this may not be fast enough. If your model is taking too long\n",
        "        to train, consider how you may speed up your code by reducing the number\n",
        "        of for loops involved. How can you reformulate your code using matrix\n",
        "        operations?\n",
        "\n",
        "        Hint: we were able to implement each of the forward, backward, and\n",
        "        forward_backward operations using only one for loop.\n",
        "        \"\"\"\n",
        "\n",
        "    def learn_unsupervised(self, corpus, num_iters, print_every=10):\n",
        "        \"\"\"Run the Baum Welch EM algorithm\n",
        "        \n",
        "        corpus: the data to learn from\n",
        "        num_iters: the number of iterations to run the algorithm\n",
        "        print_every: how often to print the log-likelihood while the model is\n",
        "        updating its parameters.\n",
        "        \"\"\"\n",
        "\n",
        "        for i_iter in range(num_iters):\n",
        "            \n",
        "\n",
        "\n",
        "            # expected_si = None # your code here\n",
        "            # expected_sij = None # your code here\n",
        "            # expected_sjwk = None # your code here\n",
        "            # expected_q1 = None # your code here\n",
        "\n",
        "            A_new = np.zeros((self.num_states,self.num_states))\n",
        "            B_new = np.zeros((self.num_states,self.num_words))\n",
        "            pi_new = np.zeros(self.num_states)\n",
        "\n",
        "            expected_si = np.zeros(self.num_states) \n",
        "            expected_sij = np.zeros((self.num_states, self.num_states)) \n",
        "            expected_sjwk = np.zeros((self.num_states, self.num_words)) \n",
        "            expected_q1 = np.zeros(self.num_states)\n",
        "            expected_number_of_times_sj = np.zeros(self.num_states)\n",
        "\n",
        "            total_logprob = 0\n",
        "            \n",
        "            for review in corpus:\n",
        "\n",
        "                logprob, xi, gamma = self.forward_backward(review)\n",
        "\n",
        "                total_logprob = total_logprob+logprob \n",
        "\n",
        "\n",
        "                for s_i in self.states:\n",
        "\n",
        "                  expected_q1[s_i] += gamma[0, s_i]\n",
        "\n",
        "                  expected_number_of_times_sj[s_i] += sum(gamma[t, s_i] for t in range(len(review)))\n",
        "\n",
        "                  for s_j in self.states:\n",
        "\n",
        "                    expected_sij[s_i,s_j] += sum(xi[t][s_i][s_j] for t in range(len(review)-1))\n",
        "\n",
        "                  expected_si[s_i] += sum(expected_sij[s_i, s_x] for s_x in self.states)\n",
        "\n",
        "                  for t in range(len(review)):\n",
        "\n",
        "                    expected_sjwk[s_i, review[t]] += gamma[t,s_i]\n",
        "\n",
        "\n",
        "                for s_i in self.states:\n",
        "\n",
        "                  pi_new[s_i] = expected_q1[s_i]\n",
        "                  \n",
        "                  for s_j in self.states:\n",
        "                    \n",
        "                    A_new[s_i, s_j] = expected_sij[s_i,s_j] / expected_si[s_i]\n",
        "\n",
        "                  for w in range(self.num_words):\n",
        "\n",
        "                    B_new[s_i, w] = expected_sjwk[s_i, w] / expected_number_of_times_sj[s_i]\n",
        "            \n",
        "            A_row_sums = A_new.sum(axis=1)\n",
        "            A_new = A_new / A_row_sums[:, np.newaxis]\n",
        "\n",
        "            B_row_sums = B_new.sum(axis=1)\n",
        "            B_new = B_new / B_row_sums[:, np.newaxis]\n",
        "\n",
        "            pi_sum = sum(pi_new)\n",
        "\n",
        "            pi_new = pi_new / pi_sum\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            if i_iter % print_every == 0:\n",
        "              print(\"log-likelihood\", total_logprob)\n",
        "\n",
        "            \"\"\"\n",
        "            The following variables should be the new values of self.A, self.B,\n",
        "            and self.pi after the values are updated.\n",
        "            \"\"\"\n",
        "            # A_new = None # your code here\n",
        "            # B_new = None # your code here\n",
        "            # pi_new = None # your code here\n",
        "\n",
        "            self.A = A_new\n",
        "            self.B = B_new\n",
        "            self.pi = pi_new\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6PvF4PHSpxX"
      },
      "source": [
        ""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3gRZ_r_vLav"
      },
      "source": [
        "def init_test():\r\n",
        "\r\n",
        "    num_states = np.random.randint(100)\r\n",
        "    num_words = np.random.randint(100)\r\n",
        "    model = HMM(num_states, num_words)\r\n",
        "\r\n",
        "    assert model.A.shape == (num_states, num_states)\r\n",
        "    assert model.B.shape == (num_states, num_words)\r\n",
        "    assert model.pi.shape == (num_states, )\r\n",
        "\r\n",
        "    assert np.linalg.norm(np.sum(model.A, axis=1) - np.ones(num_states)) < 1e-10\r\n",
        "    assert np.linalg.norm(np.sum(model.B, axis=1) - np.ones(num_states)) < 1e-10\r\n",
        "    assert np.linalg.norm(np.sum(model.pi) - 1) < 1e-10\r\n",
        "\r\n",
        "def forward_test():\r\n",
        "    model = HMM(2, 10)\r\n",
        "    model.A = np.array([[0.79034887, 0.20965113],\r\n",
        "                        [0.66824331, 0.33175669]])\r\n",
        "    model.B = np.array([[0.08511814, 0.06627238, 0.08487461, 0.15607959, 0.00124582, 0.12984083, 0.11164849, 0.11591902, 0.15232716, 0.09667395],\r\n",
        "                        [0.18425462, 0.14326559, 0.14026994, 0.0215989,  0.17687124, 0.04681278, 0.05857451, 0.17451212, 0.00473382, 0.04910648]])\r\n",
        "    model.pi = np.array([0.77480039, 0.22519961])\r\n",
        "    obs = [1, 8, 0, 0, 3, 4, 5, 2, 6, 3, 7, 9]\r\n",
        "    alpha = model.forward(obs)\r\n",
        "\r\n",
        "    print(\"The result of the forward function should be\", np.array([[-2.96913, -3.43382],\r\n",
        "                                                                    [ -4.66005, -9.19418],\r\n",
        "                                                                    [ -7.35001, -7.89695],\r\n",
        "                                                                    [ -9.65069, -9.95363],\r\n",
        "                                                                    [-11.25815, -14.27392],\r\n",
        "                                                                    [-18.14079, -14.4781 ],\r\n",
        "                                                                    [-16.89275, -18.62696],\r\n",
        "                                                                    [-19.45549, -20.17289],\r\n",
        "                                                                    [-21.53772, -23.283  ],\r\n",
        "                                                                    [-23.4927, -26.69119],\r\n",
        "                                                                    [-25.84891, -26.73817],\r\n",
        "                                                                    [-28.12237, -29.92402]]))\r\n",
        "    print(\"Your value of alpha is:\", np.round(alpha, 5))\r\n",
        "\r\n",
        "def backward_test():\r\n",
        "    model = HMM(2, 10)\r\n",
        "    model.A = np.array([[0.79034887, 0.20965113],\r\n",
        "                        [0.66824331, 0.33175669]])\r\n",
        "    model.B = np.array([[0.08511814, 0.06627238, 0.08487461, 0.15607959, 0.00124582, 0.12984083, 0.11164849, 0.11591902, 0.15232716, 0.09667395],\r\n",
        "                        [0.18425462, 0.14326559, 0.14026994, 0.0215989,  0.17687124, 0.04681278, 0.05857451, 0.17451212, 0.00473382, 0.04910648]])\r\n",
        "    model.pi = np.array([0.77480039, 0.22519961])\r\n",
        "    obs = [1, 8, 0, 0, 3, 4, 5, 2, 6, 3, 7, 9]\r\n",
        "    beta = model.backward(obs)\r\n",
        "\r\n",
        "    print(\"The result of the backward function should be\", np.array([[-25.42937, -25.58918], \r\n",
        "                                                                     [-23.32164, -23.19959],\r\n",
        "                                                                     [-21.11007, -21.02033],\r\n",
        "                                                                     [-18.82215, -18.94381],\r\n",
        "                                                                     [-16.78523, -16.33951],\r\n",
        "                                                                     [-13.42847, -13.51924],\r\n",
        "                                                                     [-11.24815, -11.19161],\r\n",
        "                                                                     [ -8.88679,  -8.96441],\r\n",
        "                                                                     [ -6.57374,  -6.70985],\r\n",
        "                                                                     [ -4.51873,  -4.47419],\r\n",
        "                                                                     [ -2.44529,  -2.51463],\r\n",
        "                                                                     [  0, 0]]))\r\n",
        "\r\n",
        "    print(\"Your value of beta is:\", np.round(beta, 5))\r\n",
        "\r\n",
        "\r\n",
        "def forward_backward_test():\r\n",
        "    model = HMM(2, 10)\r\n",
        "    model.A = np.array([[0.79034887, 0.20965113],\r\n",
        "                        [0.66824331, 0.33175669]])\r\n",
        "    model.B = np.array([[0.08511814, 0.06627238, 0.08487461, 0.15607959, 0.00124582, 0.12984083, 0.11164849, 0.11591902, 0.15232716, 0.09667395],\r\n",
        "                        [0.18425462, 0.14326559, 0.14026994, 0.0215989,  0.17687124, 0.04681278, 0.05857451, 0.17451212, 0.00473382, 0.04910648]])\r\n",
        "    model.pi = np.array([0.77480039, 0.22519961])\r\n",
        "    obs = [1, 8, 0, 0, 3, 4, 5, 2, 6, 3, 7, 9]\r\n",
        "    logprob, xi, gamma = model.forward_backward(obs)\r\n",
        "\r\n",
        "    print(\"The value of logprob should be:\", -27.9693)\r\n",
        "    print(\"Your value of logprob is:\", np.round(logprob, 5))\r\n",
        "\r\n",
        "    print(\"The value of xi should be:\", np.array([[[0.64523, 0.00601],\r\n",
        "                                                  [0.34278, 0.00598]],\r\n",
        "\r\n",
        "                                                 [[0.60684, 0.38117],\r\n",
        "                                                  [0.00551, 0.00648]],\r\n",
        "\r\n",
        "                                                 [[0.40595, 0.2064 ],\r\n",
        "                                                  [0.19863, 0.18902]],\r\n",
        "\r\n",
        "                                                 [[0.5718,  0.03278],\r\n",
        "                                                  [0.35711, 0.03831]],\r\n",
        "\r\n",
        "                                                 [[0.02625, 0.90266],\r\n",
        "                                                  [0.00109, 0.07   ]],\r\n",
        "\r\n",
        "                                                 [[0.02482, 0.00251],\r\n",
        "                                                  [0.81777, 0.15489]],\r\n",
        "\r\n",
        "                                                 [[0.59943, 0.24316],\r\n",
        "                                                  [0.08947, 0.06793]],\r\n",
        "\r\n",
        "                                                 [[0.6143,  0.07461],\r\n",
        "                                                  [0.25347, 0.05762]],\r\n",
        "\r\n",
        "                                                 [[0.8357,  0.03207],\r\n",
        "                                                  [0.12337, 0.00886]],\r\n",
        "\r\n",
        "                                                 [[0.69872, 0.26034],\r\n",
        "                                                  [0.02412, 0.01682]],\r\n",
        "\r\n",
        "                                                 [[0.63701, 0.08583],\r\n",
        "                                                  [0.22134, 0.05582]]]))\r\n",
        "    print(\"Your value of xi is:\", np.round(xi, 5))\r\n",
        "\r\n",
        "    print(\"The value of gamma should be:\", np.array([[0.65124, 0.34876],\r\n",
        "                                                    [0.98802, 0.01198],\r\n",
        "                                                    [0.61235, 0.38765],\r\n",
        "                                                    [0.60458, 0.39542],\r\n",
        "                                                    [0.92891, 0.07109],\r\n",
        "                                                    [0.02733, 0.97267],\r\n",
        "                                                    [0.8426,  0.1574 ],\r\n",
        "                                                    [0.68891, 0.31109],\r\n",
        "                                                    [0.86777, 0.13223],\r\n",
        "                                                    [0.95906, 0.04094],\r\n",
        "                                                    [0.72284, 0.27716],\r\n",
        "                                                    [0.85835, 0.14165]]))\r\n",
        "\r\n",
        "    print(\"Your value of gamma is:\", np.round(gamma, 5))\r\n",
        "\r\n",
        "def baum_welch_update_test():\r\n",
        "    model = HMM(4, 10)\r\n",
        "    \r\n",
        "    model.A = np.array([[0.05263151, 0.62161178, 0.06683182, 0.25892489],\r\n",
        "                        [0.26993274, 0.13114741, 0.32305468, 0.27586517],\r\n",
        "                        [0.2951958,  0.14576492, 0.22474111, 0.33429817],\r\n",
        "                        [0.29586018, 0.26065884, 0.1977772,  0.24570378]])\r\n",
        "    \r\n",
        "    model.B = np.array([[0.01800425, 0.09767131, 0.17824799, 0.12586453, 0.19514548, 0.05433139, 0.01995667, 0.12985343, 0.01884263, 0.16208232],\r\n",
        "                        [0.04512782, 0.09469685, 0.1426164,  0.13851362, 0.08717793, 0.17152532, 0.08746939, 0.04900339, 0.05315859, 0.13071069],\r\n",
        "                        [0.11055806, 0.10592473, 0.0051817,  0.07721441, 0.21761783, 0.20323146, 0.18881598, 0.00584989, 0.00682669, 0.07877924],\r\n",
        "                        [0.08711377, 0.16703645, 0.0706214,  0.05297571, 0.10486868, 0.16794587, 0.13562053, 0.15729142, 0.03345308, 0.02307309]])\r\n",
        "    \r\n",
        "    model.pi = np.array([0.21186864, 0.27156561, 0.37188523, 0.14468051])\r\n",
        "    \r\n",
        "    corpus = np.array([[7,3,2,5,0,3,2,9,4,2], [7,3,2,4,2,8,7,5,0,8], [7,3,2,3,1,7,3,8,6,7], [7,3,2,6,4,4,3,4,0,0]])\r\n",
        "\r\n",
        "    model.learn_unsupervised(corpus, 200)\r\n",
        "\r\n",
        "    print(\"hmm.A should be\", np.array([[0, 1, 0, 0], \r\n",
        "                                     [0.14122, 0, 0.27099, 0.58779], \r\n",
        "                                     [0.20671, 0, 0, 0.79329], \r\n",
        "                                     [0, 0.90909, 0.09091, 0]]))\r\n",
        "    print(\"Your implementation has hmm.A to be\", np.round(model.A, 5))\r\n",
        "\r\n",
        "    print(\"hmm.B should be\", np.array([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\r\n",
        "                                              [0.0625, 0, 0, 0.5, 0, 0.125, 0.125, 0, 0.125, 0.0625],\r\n",
        "                                              [0, 0.20671, 0, 0, 0.79329, 0, 0, 0, 0, 0],\r\n",
        "                                              [0.24667, 0, 0.57555, 0, 0.09556, 0, 0, 0, 0.08222, 0]]))\r\n",
        "    print(\"Your implementation has hmm.B to be\", np.round(model.B, 5))\r\n",
        "\r\n",
        "    print(\"hmm.pi should be\", np.array([1, 0, 0, 0]))\r\n",
        "\r\n",
        "    print(\"Your implementation has hmm.pi to be\", np.round(model.pi, 5))\r\n",
        "\r\n",
        "def end_to_end_test():\r\n",
        "    # Test Case 1\r\n",
        "\r\n",
        "    corpus = np.array([[0,3,0,3,0,3,0,3,0,3,0,3], [0,2,0,2,0,2,0,2,0,2,0,2,0], [1,2,1,2,1,2,1,2,1,2,1,2],[1,3,1,3,1,3,1,3,1,3]])\r\n",
        "    hmm = HMM(num_states=2,num_words=4)\r\n",
        "    hmm.learn_unsupervised(corpus, 10)\r\n",
        "    print(\"After this test case, hmm.A should either be approximately,\",  np.array([[0, 1], [1, 0]]))\r\n",
        "    print(\"This is your current value of hmm.A: \", np.round(hmm.A, 5))\r\n",
        "\r\n",
        "    print(\"After this test case, hmm.B should either be approximately,\", np.array([[0, 0, 0.5, 0.5], [0.5, 0.5, 0, 0]]), \" or it should be \", np.array([[0.5, 0.5, 0, 0], [0, 0, 0.5, 0.5]]))\r\n",
        "    print(\"This is your current value of hmm.B: \", np.round(hmm.B, 5))\r\n",
        "\r\n",
        "    # Test Case 2\r\n",
        "\r\n",
        "    corpus = np.array([[0,0,0,0,0,0,0,0,0,0], [1,1,1,1,1,1,1,1,1,1], [2,2,2,2,2,2,2,2,2,2]])\r\n",
        "    hmm = HMM(num_states=3, num_words=3)\r\n",
        "    hmm.learn_unsupervised(corpus, 100)\r\n",
        "    print(\"After this test case, hmm.A should be the identity matrix\", np.eye(3))\r\n",
        "    print(\"This is your current value of hmm.A: \", np.round(hmm.A, 5))\r\n",
        "\r\n",
        "    print(\"After this test case, hmm.B should be some 3 by 3 permutation matrix\")\r\n",
        "    print(\"This is your current value of hmm.B: \", np.round(hmm.B, 5))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yFF2rKuuh6U"
      },
      "source": [
        "## Test Cases\r\n",
        "\r\n",
        "The following are test cases that are meant to help you debug your code. The code involves six test suites - an initialization test, a forward test, a backward test, a forward_backward test, a baum_welch_update test, and a final end_to_end test."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmewPFV2MPlS"
      },
      "source": [
        "## Test\r\n",
        "\r\n",
        "To actually run the test cases, run the cell below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFtzZ9W9MVKu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3f9d6a0e-54a1-4140-ceb0-12d60f36949c"
      },
      "source": [
        "init_test()\r\n",
        "forward_test()\r\n",
        "backward_test()\r\n",
        "forward_backward_test()\r\n",
        "baum_welch_update_test()\r\n",
        "end_to_end_test()\r\n",
        "\r\n",
        "\"\"\"\r\n",
        "Note: The end_to_end_test is not as robustg due to it using random starts. Try\r\n",
        "running the test case a few times to see if you get a good result at least a few\r\n",
        "times before deciding that your code is buggy.\r\n",
        "\"\"\""
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The result of the forward function should be [[ -2.96913  -3.43382]\n",
            " [ -4.66005  -9.19418]\n",
            " [ -7.35001  -7.89695]\n",
            " [ -9.65069  -9.95363]\n",
            " [-11.25815 -14.27392]\n",
            " [-18.14079 -14.4781 ]\n",
            " [-16.89275 -18.62696]\n",
            " [-19.45549 -20.17289]\n",
            " [-21.53772 -23.283  ]\n",
            " [-23.4927  -26.69119]\n",
            " [-25.84891 -26.73817]\n",
            " [-28.12237 -29.92402]]\n",
            "Your value of alpha is: [[ -2.96913  -3.43382]\n",
            " [ -4.66005  -9.19418]\n",
            " [ -7.35001  -7.89695]\n",
            " [ -9.65069  -9.95363]\n",
            " [-11.25815 -14.27392]\n",
            " [-18.14079 -14.4781 ]\n",
            " [-16.89275 -18.62697]\n",
            " [-19.45549 -20.17289]\n",
            " [-21.53772 -23.283  ]\n",
            " [-23.4927  -26.69119]\n",
            " [-25.84891 -26.73817]\n",
            " [-28.12237 -29.92402]]\n",
            "The result of the backward function should be [[-25.42937 -25.58918]\n",
            " [-23.32164 -23.19959]\n",
            " [-21.11007 -21.02033]\n",
            " [-18.82215 -18.94381]\n",
            " [-16.78523 -16.33951]\n",
            " [-13.42847 -13.51924]\n",
            " [-11.24815 -11.19161]\n",
            " [ -8.88679  -8.96441]\n",
            " [ -6.57374  -6.70985]\n",
            " [ -4.51873  -4.47419]\n",
            " [ -2.44529  -2.51463]\n",
            " [  0.        0.     ]]\n",
            "Your value of beta is: [[-25.42937 -25.58918]\n",
            " [-23.32164 -23.19959]\n",
            " [-21.11007 -21.02033]\n",
            " [-18.82215 -18.94381]\n",
            " [-16.78523 -16.33951]\n",
            " [-13.42847 -13.51924]\n",
            " [-11.24815 -11.19161]\n",
            " [ -8.88679  -8.96441]\n",
            " [ -6.57374  -6.70985]\n",
            " [ -4.51873  -4.47419]\n",
            " [ -2.44529  -2.51463]\n",
            " [  0.        0.     ]]\n",
            "The value of logprob should be: -27.9693\n",
            "Your value of logprob is: -27.96963\n",
            "The value of xi should be: [[[0.64523 0.00601]\n",
            "  [0.34278 0.00598]]\n",
            "\n",
            " [[0.60684 0.38117]\n",
            "  [0.00551 0.00648]]\n",
            "\n",
            " [[0.40595 0.2064 ]\n",
            "  [0.19863 0.18902]]\n",
            "\n",
            " [[0.5718  0.03278]\n",
            "  [0.35711 0.03831]]\n",
            "\n",
            " [[0.02625 0.90266]\n",
            "  [0.00109 0.07   ]]\n",
            "\n",
            " [[0.02482 0.00251]\n",
            "  [0.81777 0.15489]]\n",
            "\n",
            " [[0.59943 0.24316]\n",
            "  [0.08947 0.06793]]\n",
            "\n",
            " [[0.6143  0.07461]\n",
            "  [0.25347 0.05762]]\n",
            "\n",
            " [[0.8357  0.03207]\n",
            "  [0.12337 0.00886]]\n",
            "\n",
            " [[0.69872 0.26034]\n",
            "  [0.02412 0.01682]]\n",
            "\n",
            " [[0.63701 0.08583]\n",
            "  [0.22134 0.05582]]]\n",
            "Your value of xi is: [[[0.64523 0.00601]\n",
            "  [0.34278 0.00598]]\n",
            "\n",
            " [[0.60684 0.38117]\n",
            "  [0.00551 0.00648]]\n",
            "\n",
            " [[0.40595 0.2064 ]\n",
            "  [0.19863 0.18902]]\n",
            "\n",
            " [[0.5718  0.03278]\n",
            "  [0.35711 0.03831]]\n",
            "\n",
            " [[0.02625 0.90266]\n",
            "  [0.00109 0.07   ]]\n",
            "\n",
            " [[0.02482 0.00251]\n",
            "  [0.81777 0.15489]]\n",
            "\n",
            " [[0.59943 0.24316]\n",
            "  [0.08947 0.06793]]\n",
            "\n",
            " [[0.6143  0.07461]\n",
            "  [0.25347 0.05762]]\n",
            "\n",
            " [[0.8357  0.03207]\n",
            "  [0.12337 0.00886]]\n",
            "\n",
            " [[0.69872 0.26034]\n",
            "  [0.02412 0.01682]]\n",
            "\n",
            " [[0.63701 0.08583]\n",
            "  [0.22134 0.05582]]\n",
            "\n",
            " [[0.      0.     ]\n",
            "  [0.      0.     ]]]\n",
            "The value of gamma should be: [[0.65124 0.34876]\n",
            " [0.98802 0.01198]\n",
            " [0.61235 0.38765]\n",
            " [0.60458 0.39542]\n",
            " [0.92891 0.07109]\n",
            " [0.02733 0.97267]\n",
            " [0.8426  0.1574 ]\n",
            " [0.68891 0.31109]\n",
            " [0.86777 0.13223]\n",
            " [0.95906 0.04094]\n",
            " [0.72284 0.27716]\n",
            " [0.85835 0.14165]]\n",
            "Your value of gamma is: [[0.65124 0.34876]\n",
            " [0.98802 0.01198]\n",
            " [0.61235 0.38765]\n",
            " [0.60458 0.39542]\n",
            " [0.92891 0.07109]\n",
            " [0.02733 0.97267]\n",
            " [0.8426  0.1574 ]\n",
            " [0.68891 0.31109]\n",
            " [0.86777 0.13223]\n",
            " [0.95906 0.04094]\n",
            " [0.72284 0.27716]\n",
            " [0.85835 0.14165]]\n",
            "log-likelihood -96.33989919755487\n",
            "log-likelihood -61.83737464303461\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:71: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:81: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:164: RuntimeWarning: divide by zero encountered in log\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "log-likelihood -59.65907612785483\n",
            "log-likelihood -58.78006168233148\n",
            "log-likelihood -58.05638622904577\n",
            "log-likelihood -58.047573998672526\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:123: RuntimeWarning: divide by zero encountered in log\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "log-likelihood -58.04756024248874\n",
            "log-likelihood -58.04756014666195\n",
            "log-likelihood -58.04756014303575\n",
            "log-likelihood -58.04756014156317\n",
            "log-likelihood -58.04756014077336\n",
            "log-likelihood -58.047560140347095\n",
            "log-likelihood -58.04756014011701\n",
            "log-likelihood -58.04756013999282\n",
            "log-likelihood -58.047560139925785\n",
            "log-likelihood -58.04756013988958\n",
            "log-likelihood -58.04756013987006\n",
            "log-likelihood -58.04756013985951\n",
            "log-likelihood -58.04756013985383\n",
            "log-likelihood -58.04756013985076\n",
            "hmm.A should be [[0.      1.      0.      0.     ]\n",
            " [0.14122 0.      0.27099 0.58779]\n",
            " [0.20671 0.      0.      0.79329]\n",
            " [0.      0.90909 0.09091 0.     ]]\n",
            "Your implementation has hmm.A to be [[0.      1.      0.      0.     ]\n",
            " [0.14122 0.      0.27099 0.58779]\n",
            " [0.20671 0.      0.      0.79329]\n",
            " [0.      0.90909 0.09091 0.     ]]\n",
            "hmm.B should be [[0.      0.      0.      0.      0.      0.      0.      1.      0.\n",
            "  0.     ]\n",
            " [0.0625  0.      0.      0.5     0.      0.125   0.125   0.      0.125\n",
            "  0.0625 ]\n",
            " [0.      0.20671 0.      0.      0.79329 0.      0.      0.      0.\n",
            "  0.     ]\n",
            " [0.24667 0.      0.57555 0.      0.09556 0.      0.      0.      0.08222\n",
            "  0.     ]]\n",
            "Your implementation has hmm.B to be [[0.      0.      0.      0.      0.      0.      0.      1.      0.\n",
            "  0.     ]\n",
            " [0.0625  0.      0.      0.5     0.      0.125   0.125   0.      0.125\n",
            "  0.0625 ]\n",
            " [0.      0.20671 0.      0.      0.79329 0.      0.      0.      0.\n",
            "  0.     ]\n",
            " [0.24667 0.      0.57555 0.      0.09556 0.      0.      0.      0.08222\n",
            "  0.     ]]\n",
            "hmm.pi should be [1 0 0 0]\n",
            "Your implementation has hmm.pi to be [1. 0. 0. 0.]\n",
            "log-likelihood -64.29448539835923\n",
            "After this test case, hmm.A should either be approximately, [[0 1]\n",
            " [1 0]]\n",
            "This is your current value of hmm.A:  [[0. 1.]\n",
            " [1. 0.]]\n",
            "After this test case, hmm.B should either be approximately, [[0.  0.  0.5 0.5]\n",
            " [0.5 0.5 0.  0. ]]  or it should be  [[0.5 0.5 0.  0. ]\n",
            " [0.  0.  0.5 0.5]]\n",
            "This is your current value of hmm.B:  [[0.54167 0.45833 0.      0.     ]\n",
            " [0.      0.      0.52174 0.47826]]\n",
            "log-likelihood -33.578221749433645\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:165: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:246: RuntimeWarning: invalid value encountered in double_scalars\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:250: RuntimeWarning: invalid value encountered in double_scalars\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "log-likelihood -16.777820841440715\n",
            "log-likelihood -15.77248611608586\n",
            "log-likelihood -15.772486116083344\n",
            "log-likelihood -15.772486116083343\n",
            "log-likelihood -15.772486116083341\n",
            "log-likelihood -15.772486116083341\n",
            "log-likelihood -15.772486116083343\n",
            "log-likelihood -15.772486116083344\n",
            "log-likelihood -15.772486116083343\n",
            "After this test case, hmm.A should be the identity matrix [[1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]]\n",
            "This is your current value of hmm.A:  [[1.      0.      0.     ]\n",
            " [0.      0.1047  0.8953 ]\n",
            " [0.      0.58468 0.41532]]\n",
            "After this test case, hmm.B should be some 3 by 3 permutation matrix\n",
            "This is your current value of hmm.B:  [[1.  0.  0. ]\n",
            " [0.  0.5 0.5]\n",
            " [0.  0.5 0.5]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nNote: The end_to_end_test is not as robustg due to it using random starts. Try\\nrunning the test case a few times to see if you get a good result at least a few\\ntimes before deciding that your code is buggy.\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eF-l7WucpCBP"
      },
      "source": [
        "## Training\r\n",
        "\r\n",
        "Train a model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTWXUt15pDg4"
      },
      "source": [
        "tokenizer = lab_util.Tokenizer()\n",
        "tokenizer.fit(train_reviews)\n",
        "train_reviews_tk = tokenizer.tokenize(train_reviews)\n",
        "print(tokenizer.vocab_size)\n",
        "\n",
        "hmm = HMM(num_states=10, num_words=tokenizer.vocab_size)\n",
        "hmm.learn_unsupervised(train_reviews_tk, 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiCwE05xqXmI"
      },
      "source": [
        "Let's look at some of the words associated with each hidden state:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXhMoLUFqbn_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07fde43d-9f38-4827-9f9a-6db0f0176a19"
      },
      "source": [
        "for i in range(hmm.num_states):\n",
        "    most_probable = np.argsort(hmm.B[i, :])[:10]\n",
        "    print(f\"state {i}\")\n",
        "    for o in most_probable:\n",
        "        print(tokenizer.token_to_word[o], hmm.B[i, o])\n",
        "    print()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "state 0\n",
            "states 3.1126533519085174e-08\n",
            "ramen 5.5676898592441945e-08\n",
            "75 7.898861623322146e-08\n",
            "watered 2.656283188552594e-07\n",
            "chunks 3.72611444393929e-07\n",
            "sparkling 3.9483849173647634e-07\n",
            "anyway 4.5119796180180973e-07\n",
            "soaked 4.795532431975632e-07\n",
            "ice 5.285933133650525e-07\n",
            "wont 5.7574604262563e-07\n",
            "\n",
            "state 1\n",
            "wide 1.5247667858903364e-07\n",
            "companies 1.9194562658638927e-07\n",
            "shows 2.822460146045568e-07\n",
            "yellow 3.5496644457109357e-07\n",
            "measure 3.986421068729611e-07\n",
            "bodied 4.0022421217557886e-07\n",
            "support 5.618912749824626e-07\n",
            "sweetness 6.076348488701063e-07\n",
            "homemade 6.14790060145347e-07\n",
            "supply 6.807566447895067e-07\n",
            "\n",
            "state 2\n",
            "common 2.060077135398386e-08\n",
            "puck 9.8955626830216e-08\n",
            "talk 1.6479360901811192e-07\n",
            "wife 1.805859425492689e-07\n",
            "flax 2.0091580608788177e-07\n",
            "york 2.449951020895544e-07\n",
            "blueberries 3.753501949884209e-07\n",
            "cheese 4.1513537600984343e-07\n",
            "apart 4.5944605776258e-07\n",
            "important 4.958409694512951e-07\n",
            "\n",
            "state 3\n",
            "possible 1.3885248976967314e-07\n",
            "stated 1.8242212805906483e-07\n",
            "offer 3.364614389774708e-07\n",
            "lb 3.7009868917966656e-07\n",
            "bother 3.740896337900849e-07\n",
            "turns 4.953004834866852e-07\n",
            "sesame 5.03485219628404e-07\n",
            "share 5.310286660794997e-07\n",
            "pricing 5.464222573356471e-07\n",
            "full 5.610282639027703e-07\n",
            "\n",
            "state 4\n",
            "neither 7.68623259154897e-08\n",
            "filled 3.2489537570868267e-07\n",
            "tuna 3.3258819559863477e-07\n",
            "authentic 3.722763560975457e-07\n",
            "equal 3.8113561683174804e-07\n",
            "lemonade 4.2061817182736135e-07\n",
            "star 4.7158589366382065e-07\n",
            "wants 5.167430177626163e-07\n",
            "avoid 6.316799112824284e-07\n",
            "mg 7.014218156110767e-07\n",
            "\n",
            "state 5\n",
            "state 1.284879102401564e-07\n",
            "bubble 1.9018944329497684e-07\n",
            "appeal 2.032867351210606e-07\n",
            "running 3.2154267264263967e-07\n",
            "ball 5.487221068065553e-07\n",
            "reminded 5.657906982126247e-07\n",
            "pie 7.296685472358564e-07\n",
            "date 7.826196224002767e-07\n",
            "kid 8.055838287881533e-07\n",
            "followed 8.208873439617226e-07\n",
            "\n",
            "state 6\n",
            "play 2.920714267745198e-07\n",
            "34 3.5956964012438584e-07\n",
            "naturally 4.315693160065267e-07\n",
            "besides 5.824728880163609e-07\n",
            "dijon 9.022080707742095e-07\n",
            "barbeque 9.704170158899926e-07\n",
            "custard 1.2767256894310677e-06\n",
            "tiny 1.4034114571501134e-06\n",
            "delivered 1.4953920284007988e-06\n",
            "hair 1.5191413926007933e-06\n",
            "\n",
            "state 7\n",
            "r 4.3295520355425327e-08\n",
            "beware 1.9021427503313575e-07\n",
            "talk 3.3548870667794216e-07\n",
            "carrots 3.4899934599992935e-07\n",
            "american 5.093434484833086e-07\n",
            "moved 5.275592140829012e-07\n",
            "melitta 5.659010983870585e-07\n",
            "nutritious 6.622961641773782e-07\n",
            "term 7.220330421981864e-07\n",
            "improve 7.317942930101162e-07\n",
            "\n",
            "state 8\n",
            "death 1.4288288738549442e-07\n",
            "carbohydrate 2.985414566207438e-07\n",
            "crunch 3.226820285575695e-07\n",
            "raspberry 8.471984718291788e-07\n",
            "cheaper 9.648296233584822e-07\n",
            "thus 9.925863815881361e-07\n",
            "bigger 1.000041501372483e-06\n",
            "hate 1.1582680551677526e-06\n",
            "batches 1.3749099309001184e-06\n",
            "granted 1.584612380435659e-06\n",
            "\n",
            "state 9\n",
            "overpowering 5.3116049116620465e-08\n",
            "cal 2.4537733949617356e-07\n",
            "snacking 3.0198214174567624e-07\n",
            "lemonade 3.6354219668905184e-07\n",
            "times 4.234304957047809e-07\n",
            "biscotti 4.648201894741732e-07\n",
            "thrown 9.02921322007716e-07\n",
            "pringles 1.0584303773545863e-06\n",
            "fed 1.1454811210301157e-06\n",
            "road 1.15920611900035e-06\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAQ_PmASwdFz"
      },
      "source": [
        "We can also look at some samples from the model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tj1eT3s3wgFJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "232ab4b0-f9fd-4b27-95d2-4dfcdf7da0dd"
      },
      "source": [
        "for i in range(10):\n",
        "    print(tokenizer.de_tokenize([hmm.generate(10)]))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['crisp . to sell <unk> baby beans <unk> . ,']\n",
            "['as stick . of you delicious the but a you']\n",
            "['water this bad selling <unk> anyone very of it due']\n",
            "['know of 100 have , <unk> best with blend as']\n",
            "['able know and cheaper to enjoyed is not single surprised']\n",
            "[\"cracker . spent <unk> i've br <unk> to coffee ,\"]\n",
            "['salt but . for review you ones like i peanuts']\n",
            "[\"flavors they reviewers it 50 won't <unk> rice <unk> few\"]\n",
            "[\"it recipe the . br tolerate . . there's sandwich\"]\n",
            "[\"way <unk> didn't in readily added <unk> find many claim\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9Qk9adNr7lQ"
      },
      "source": [
        "Finally, let's repeat the classification experiment from Parts 1 and 2, using the _vector of expected hidden state counts_ as a sentence representation.\n",
        "\n",
        "(Warning! results may not be the same as in earlier versions of this experiment.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mL6JQXLJspyA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56e8fb45-4dc5-46f8-cc62-da79f92bb9de"
      },
      "source": [
        "def train_model(xs_featurized, ys):\n",
        "  import sklearn.linear_model\n",
        "  model = sklearn.linear_model.LogisticRegression()\n",
        "  model.fit(xs_featurized, ys)\n",
        "  return model\n",
        "\n",
        "def eval_model(model, xs_featurized, ys):\n",
        "  pred_ys = model.predict(xs_featurized)\n",
        "  print(\"test accuracy\", np.mean(pred_ys == ys))\n",
        "\n",
        "def training_experiment(name, featurizer, n_train):\n",
        "    print(f\"{name} features, {n_train} examples\")\n",
        "    train_xs = np.array([\n",
        "        hmm_featurizer(review) \n",
        "        for review in tokenizer.tokenize(train_reviews[:n_train])\n",
        "    ])\n",
        "    train_ys = train_labels[:n_train]\n",
        "    test_xs = np.array([\n",
        "        hmm_featurizer(review)\n",
        "        for review in tokenizer.tokenize(test_reviews)\n",
        "    ])\n",
        "    test_ys = test_labels\n",
        "    model = train_model(train_xs, train_ys)\n",
        "    eval_model(model, test_xs, test_ys)\n",
        "    print()\n",
        "\n",
        "def hmm_featurizer(review):\n",
        "    _, _, gamma = hmm.forward_backward(review)\n",
        "    return gamma.sum(axis=0)\n",
        "\n",
        "training_experiment(\"hmm\", hmm_featurizer, n_train=100)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hmm features, 100 examples\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:123: RuntimeWarning: divide by zero encountered in log\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "test accuracy 0.504\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6DI4otm0YHe"
      },
      "source": [
        "## Experiments for Part 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d73StCihj3_N",
        "outputId": "57b732c1-de5e-41ad-9a64-0935030c1be8"
      },
      "source": [
        "# (a)\n",
        "\n",
        "hmm = HMM(num_states=8,num_words=4)\n",
        "\n",
        "hmm.B = np.array([[1.0, 0.0, 0.0, 0.0],\n",
        "                  [0.0, 0.0, 1.0, 0.0],\n",
        "                  [1.0, 0.0, 0.0, 0.0],\n",
        "                  [0.0, 0.0, 0.0, 1.0],\n",
        "                  [0.0, 1.0, 0.0, 0.0],\n",
        "                  [0.0, 0.0, 1.0, 0.0],\n",
        "                  [0.0, 1.0, 0.0, 0.0],\n",
        "                  [0.0, 0.0, 0.0, 1.0]])\n",
        "\n",
        "hmm.A = np.array([[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
        "                  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
        "                  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
        "                  [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
        "                  [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0],\n",
        "                  [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
        "                  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
        "                  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]])\n",
        "    \n",
        "hmm.pi = np.array([0.25, 0.0, 0.25, 0.0, 0.25, 0.0, 0.25, 0.0])\n",
        "\n",
        "corpus = np.array([[0,2,0,2,0,2,0,2,0,2,0,2,0], [0,3,0,3,0,3,0,3,0,3,0,3] , [1,2,1,2,1,2,1,2,1,2,1,2] , [1,3,1,3,1,3,1,3,1,3]])\n",
        "hmm.learn_unsupervised(corpus, 10)\n",
        "\n",
        "for i in range(10):\n",
        "    print([hmm.generate(12)])"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:25: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:71: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:81: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:123: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:164: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:246: RuntimeWarning: invalid value encountered in double_scalars\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:250: RuntimeWarning: invalid value encountered in double_scalars\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "log-likelihood -5.545177444479562\n",
            "[[0, 3, 0, 3, 0, 3, 0, 3, 0, 3, 0, 3]]\n",
            "[[1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2]]\n",
            "[[0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2]]\n",
            "[[0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2]]\n",
            "[[0, 3, 0, 3, 0, 3, 0, 3, 0, 3, 0, 3]]\n",
            "[[0, 3, 0, 3, 0, 3, 0, 3, 0, 3, 0, 3]]\n",
            "[[0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2]]\n",
            "[[1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3]]\n",
            "[[0, 3, 0, 3, 0, 3, 0, 3, 0, 3, 0, 3]]\n",
            "[[0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sj65w6iMj5oQ"
      },
      "source": [
        "# (b)\n",
        "\n",
        "hmm = HMM(num_states=2, num_words=tokenizer.vocab_size)\n",
        "hmm.learn_unsupervised(train_reviews_tk, 10)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gg-oDVn7j5ue",
        "outputId": "57282b54-3d9f-41cb-c7fd-aed27081640b"
      },
      "source": [
        "for i in range(hmm.num_states):\n",
        "    most_probable = np.argsort(hmm.B[i, :])[:10]\n",
        "    print(f\"state {i}\")\n",
        "    for o in most_probable:\n",
        "        print(tokenizer.token_to_word[o], hmm.B[i, o])\n",
        "    print()"
      ],
      "execution_count": 344,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "state 0\n",
            "brewers 2.1576134035867762e-07\n",
            "wasted 4.590910697092427e-07\n",
            "supply 6.81768673954738e-07\n",
            "gummy 6.894822864827506e-07\n",
            "starting 7.038081493451297e-07\n",
            "watchers 9.385700292123929e-07\n",
            "cons 1.0843458349946028e-06\n",
            "clam 1.3275130925162067e-06\n",
            "alive 1.6246034620661128e-06\n",
            "rica 1.6621196323391283e-06\n",
            "\n",
            "state 1\n",
            "specifically 8.823358162344053e-08\n",
            "york 8.449136389790493e-07\n",
            "picture 9.550954392705242e-07\n",
            "classic 1.069369174615933e-06\n",
            "floor 1.1870732085541305e-06\n",
            "missing 1.2749401201574024e-06\n",
            "costa 1.4059376265785708e-06\n",
            "consistency 1.4399323515847144e-06\n",
            "bean 1.6671276422830663e-06\n",
            "mg 1.7301852453922772e-06\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pk1lxHfRj50T"
      },
      "source": [
        "hmm = HMM(num_states=10, num_words=tokenizer.vocab_size)\n",
        "hmm.learn_unsupervised(train_reviews_tk, 10)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HixKK3R1lWL8",
        "outputId": "958f4c94-6996-4122-c53a-b2c0b62e10e6"
      },
      "source": [
        "for i in range(hmm.num_states):\n",
        "    most_probable = np.argsort(hmm.B[i, :])[:10]\n",
        "    print(f\"state {i}\")\n",
        "    for o in most_probable:\n",
        "        print(tokenizer.token_to_word[o], hmm.B[i, o])\n",
        "    print()"
      ],
      "execution_count": 308,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "state 0\n",
            "hold 3.1692767845658185e-07\n",
            "piece 3.7320842677484414e-07\n",
            "meals 3.832428174569911e-07\n",
            "ordering 5.514287899882275e-07\n",
            "besides 5.851982333238589e-07\n",
            "decaffeinated 6.869341162627688e-07\n",
            "carried 7.238168477727419e-07\n",
            "version 7.879504452803143e-07\n",
            "therefore 8.578346259394604e-07\n",
            "okay 1.0260794750811618e-06\n",
            "\n",
            "state 1\n",
            "hardly 1.379604538870301e-08\n",
            "instant 3.6003081527778506e-07\n",
            "constantly 3.601173470155586e-07\n",
            "amazon's 4.880752114344526e-07\n",
            "guy 6.358949992651551e-07\n",
            "palatable 6.441858922017754e-07\n",
            "gatorade 6.502888513414543e-07\n",
            "forever 1.075177184993607e-06\n",
            "point 1.1667411817019264e-06\n",
            "unique 1.2256078580851735e-06\n",
            "\n",
            "state 2\n",
            "packages 2.2513328451349182e-08\n",
            "backyard 9.678987402616077e-08\n",
            "lightly 1.273592440885765e-07\n",
            "crush 1.7122761581226615e-07\n",
            "flakes 1.8336624775500517e-07\n",
            "buyer 2.1448159407158483e-07\n",
            "careful 2.1747838958950626e-07\n",
            "substitute 2.7476010196510193e-07\n",
            "individually 3.7533524336208485e-07\n",
            "consumed 3.9856736921414855e-07\n",
            "\n",
            "state 3\n",
            "hadn't 4.673765526814337e-08\n",
            "switched 1.688930926578187e-07\n",
            "thrilled 2.2357139496582128e-07\n",
            "allergy 2.3282180752031818e-07\n",
            "sit 2.782735598926367e-07\n",
            "starting 3.62945412003088e-07\n",
            "commercial 3.746223943926247e-07\n",
            "ridiculous 3.9703904927508015e-07\n",
            "fit 6.42881671300151e-07\n",
            "breath 6.767257312022435e-07\n",
            "\n",
            "state 4\n",
            "y 7.946713560592164e-08\n",
            "doubt 2.4291400175334814e-07\n",
            "rate 2.6612163412294726e-07\n",
            "kettle 4.7135409037987824e-07\n",
            "smoother 5.615497658712517e-07\n",
            "150 7.055770045046012e-07\n",
            "acerola 7.517513188415795e-07\n",
            "strange 8.563894193215232e-07\n",
            "brewers 8.976647514054301e-07\n",
            "paid 9.083656827820094e-07\n",
            "\n",
            "state 5\n",
            "living 1.2319862786143387e-07\n",
            "expectations 4.781337774359688e-07\n",
            "caramels 4.888490534529533e-07\n",
            "ruin 5.562306232728053e-07\n",
            "cases 5.946676267336209e-07\n",
            "sugary 6.64085026725976e-07\n",
            "fence 8.104434721756654e-07\n",
            "carries 1.1077141767218946e-06\n",
            "form 1.527494940149397e-06\n",
            "sip 1.5512157574480217e-06\n",
            "\n",
            "state 6\n",
            "barbeque 1.9488129681811442e-07\n",
            "suppose 2.278695421342421e-07\n",
            "call 2.2795688804458612e-07\n",
            "therefore 2.9683518474868835e-07\n",
            "expectations 4.1782165787892574e-07\n",
            "trust 4.2069415311875404e-07\n",
            "experienced 4.5998542211575134e-07\n",
            "beyond 4.949075159413245e-07\n",
            "sticky 6.847689768629378e-07\n",
            "major 6.89763290769713e-07\n",
            "\n",
            "state 7\n",
            "gatorade 5.828137640701416e-08\n",
            "dijon 9.224356684851867e-08\n",
            "blends 9.821667292458199e-08\n",
            "decaf 1.674850728874795e-07\n",
            "backyard 2.335137989758133e-07\n",
            "fridge 3.5180874863912504e-07\n",
            "barbecue 4.1457065005640516e-07\n",
            "son's 5.743284470616819e-07\n",
            "lemonade 5.791740565949632e-07\n",
            "folks 7.103893246262258e-07\n",
            "\n",
            "state 8\n",
            "cooking 2.6725819121244256e-07\n",
            "pringles 3.327307951568633e-07\n",
            "leaf 3.516269005919944e-07\n",
            "expecting 3.628095915090776e-07\n",
            "25 4.1320823637495405e-07\n",
            "throughout 4.337896468119932e-07\n",
            "aren't 4.4498566923786906e-07\n",
            "kettle 4.609493872377805e-07\n",
            "lack 6.191891926698855e-07\n",
            "rancid 6.801238831845535e-07\n",
            "\n",
            "state 9\n",
            "nutty 4.174989006767611e-08\n",
            "greta 1.806504879146928e-07\n",
            "grown 2.590129076002968e-07\n",
            "chance 3.092407986554919e-07\n",
            "sports 4.142381338558303e-07\n",
            "grams 4.4321193179707455e-07\n",
            "recipe 6.801295175112054e-07\n",
            "crush 6.905149108977517e-07\n",
            "floor 1.016254013998434e-06\n",
            "soon 1.2897581954417916e-06\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKcJ6978lWOw"
      },
      "source": [
        "hmm = HMM(num_states=100, num_words=tokenizer.vocab_size)\n",
        "hmm.learn_unsupervised(train_reviews_tk, 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bK-CYhorlWUl",
        "outputId": "a83db36f-800c-44f8-a52b-21622bdda490"
      },
      "source": [
        "for i in range(hmm.num_states):\n",
        "    most_probable = np.argsort(hmm.B[i, :])[:10]\n",
        "    print(f\"state {i}\")\n",
        "    for o in most_probable:\n",
        "        print(tokenizer.token_to_word[o], hmm.B[i, o])\n",
        "    print()"
      ],
      "execution_count": 310,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "state 0\n",
            "concerned 7.834324874966821e-07\n",
            "http 9.736727369970197e-07\n",
            "she 1.00838422700543e-06\n",
            "flax 1.706360580137425e-06\n",
            "say 1.7292599519737597e-06\n",
            "breath 2.048424775461733e-06\n",
            "sad 2.974206666336067e-06\n",
            "affordable 3.7242045490273987e-06\n",
            "cleaning 3.738589207538128e-06\n",
            "meal 4.222284252848407e-06\n",
            "\n",
            "state 1\n",
            "mean 1.1005367718001801e-07\n",
            "forget 2.2774155583861302e-07\n",
            "major 1.2828608568920328e-06\n",
            "before 1.3146743601225886e-06\n",
            "shows 2.1636252340084515e-06\n",
            "seemed 2.373701986497841e-06\n",
            "make 3.321781253893971e-06\n",
            "support 3.5586848923976144e-06\n",
            "mouth 3.817575094135159e-06\n",
            "over 4.056026084873231e-06\n",
            "\n",
            "state 2\n",
            "similar 2.7096778434621694e-07\n",
            "perfect 5.443023611312213e-07\n",
            "seemed 6.168712164311385e-07\n",
            "although 1.1821268421254969e-06\n",
            "truffles 1.2413704902493538e-06\n",
            "treats 1.9551209431559845e-06\n",
            "able 2.75696963625808e-06\n",
            "watch 2.9002734310761193e-06\n",
            "especially 3.3934385338790422e-06\n",
            "blend 3.415247725107145e-06\n",
            "\n",
            "state 3\n",
            "you've 2.5639895246226733e-07\n",
            "name 5.100975783159069e-07\n",
            "alternative 8.920324166163526e-07\n",
            "bar 9.704289535765533e-07\n",
            "kernels 1.1094438567052275e-06\n",
            "things 1.4823021246819793e-06\n",
            "deal 2.7873233082326933e-06\n",
            "front 2.874108485833167e-06\n",
            "oz 3.0390217470723284e-06\n",
            "left 3.4674007186027087e-06\n",
            "\n",
            "state 4\n",
            "holes 5.046037179025578e-07\n",
            "prepared 2.474333520260796e-06\n",
            "herbs 2.9525262895717316e-06\n",
            "you 3.253834622681113e-06\n",
            "mug 4.092459924288271e-06\n",
            "two 5.608448171882224e-06\n",
            "whenever 5.787487588895943e-06\n",
            "compared 6.158174062257399e-06\n",
            "didn't 6.780041762612409e-06\n",
            "him 6.8174111292132865e-06\n",
            "\n",
            "state 5\n",
            "total 6.661488122806266e-08\n",
            "happy 1.3402122973422575e-07\n",
            "wanting 3.842951907096777e-07\n",
            "fell 7.847793095800702e-07\n",
            "taste 1.2882770013101065e-06\n",
            "powdered 1.4460107109415473e-06\n",
            "br 2.8643160242274304e-06\n",
            "you've 4.420472410333607e-06\n",
            "boxes 4.542199418566447e-06\n",
            "intense 4.737437651509871e-06\n",
            "\n",
            "state 6\n",
            "mg 1.2388623412761243e-06\n",
            "today 2.7355519415315472e-06\n",
            "bit 2.7772077828124324e-06\n",
            "knows 3.363734646962922e-06\n",
            "set 3.5500915369593316e-06\n",
            "marketing 4.748135575779926e-06\n",
            "he's 5.361634675750312e-06\n",
            "living 5.829917514007942e-06\n",
            "advertised 6.9236290172157255e-06\n",
            "waste 7.196553276058881e-06\n",
            "\n",
            "state 7\n",
            "seen 2.989567181016671e-07\n",
            "customers 7.639660110558152e-07\n",
            "vegetables 1.3231810157600324e-06\n",
            "blue 3.0726487696243092e-06\n",
            "except 4.103409738426228e-06\n",
            "odd 4.749507981574887e-06\n",
            "organic 4.758104892885434e-06\n",
            "w 4.8743878066086596e-06\n",
            "extract 6.281315406081867e-06\n",
            "concentrated 6.312819709851682e-06\n",
            "\n",
            "state 8\n",
            "safe 1.0017930408294505e-06\n",
            "11 1.2405857225655924e-06\n",
            "daughter 1.2586484825334906e-06\n",
            "premium 1.4133418880206187e-06\n",
            "added 1.523728757465659e-06\n",
            "can 1.921138492118503e-06\n",
            "juice 2.2128121460753174e-06\n",
            "nutty 2.4844852190023716e-06\n",
            "soft 3.0319286339880768e-06\n",
            "happier 3.434029438616706e-06\n",
            "\n",
            "state 9\n",
            "maker 7.922831498698865e-07\n",
            "spices 1.5336065714105567e-06\n",
            "idea 1.8840225607988283e-06\n",
            "allergy 3.7166391888119996e-06\n",
            "down 5.2573924509153565e-06\n",
            "quantity 5.905256111298982e-06\n",
            "preference 6.541134232894529e-06\n",
            "awful 6.919036183570118e-06\n",
            "word 7.615964080098189e-06\n",
            "ago 7.881315675831113e-06\n",
            "\n",
            "state 10\n",
            "shortbread 7.343306828452341e-07\n",
            "peppermint 9.233477947119893e-07\n",
            "luck 1.1481514255772866e-06\n",
            "ate 1.498438797867318e-06\n",
            "line 2.4598412923507665e-06\n",
            "left 2.903814336503882e-06\n",
            "packages 3.0949094041480076e-06\n",
            "put 3.249053198384203e-06\n",
            "right 3.6807354747906166e-06\n",
            "daily 4.410419829114543e-06\n",
            "\n",
            "state 11\n",
            "avoid 3.9850363302811887e-07\n",
            "animals 5.321027075486369e-07\n",
            "gum 1.0200122199422589e-06\n",
            "vending 1.8962226724248108e-06\n",
            "stuff 2.066153117880397e-06\n",
            "mrs 2.7607953522820293e-06\n",
            "happens 3.4737353053950764e-06\n",
            "box 3.82826047151446e-06\n",
            "gf 4.00288960085807e-06\n",
            "grind 4.101021952507581e-06\n",
            "\n",
            "state 12\n",
            "vanilla 2.2569576077347704e-07\n",
            "out 3.312396722500712e-07\n",
            "carbohydrate 5.010029565303211e-07\n",
            "deep 5.576288238820029e-07\n",
            "lime 6.864046492324962e-07\n",
            "page 7.808040746695951e-07\n",
            "effort 3.702007007656648e-06\n",
            "jerky 4.197815612409231e-06\n",
            "salad 4.976960692792465e-06\n",
            "understand 5.164250804731185e-06\n",
            "\n",
            "state 13\n",
            "older 4.771556140809707e-07\n",
            "2 1.5724039157956349e-06\n",
            "grown 1.700511170230323e-06\n",
            "lid 1.873265659799007e-06\n",
            "contained 1.8835276028090139e-06\n",
            "caffeine 2.262045786849078e-06\n",
            "fructose 2.508800269186217e-06\n",
            "pocky 2.666335436014952e-06\n",
            "licorice 2.7008346537696665e-06\n",
            "u 4.456215019768353e-06\n",
            "\n",
            "state 14\n",
            "due 2.997919029992633e-07\n",
            "lot 3.1045654383932526e-07\n",
            "priced 8.612076641339152e-07\n",
            "receive 1.989422819884433e-06\n",
            "don't 2.144775609246488e-06\n",
            "general 2.7543431350105294e-06\n",
            "sodium 3.7938524443336455e-06\n",
            "only 4.762161453639324e-06\n",
            "show 5.157681989040914e-06\n",
            "number 5.5808875983847515e-06\n",
            "\n",
            "state 15\n",
            "summer 5.524882759855305e-07\n",
            "because 6.315112623297457e-07\n",
            "large 1.4629237345307251e-06\n",
            "95 2.0280892882776237e-06\n",
            "16 2.729460773156125e-06\n",
            "straight 4.398506239641123e-06\n",
            "grains 5.829398222980896e-06\n",
            "notice 6.2248514400740505e-06\n",
            "around 6.6111576463709574e-06\n",
            "missing 6.963781225648515e-06\n",
            "\n",
            "state 16\n",
            "dollars 1.8520365542260836e-07\n",
            "huge 1.0779520806701681e-06\n",
            "exactly 2.4384335166814565e-06\n",
            "splash 2.9802622938701353e-06\n",
            "went 3.378086039733405e-06\n",
            "u 4.521852390819738e-06\n",
            "order 4.546664978252038e-06\n",
            "sad 4.5681581650789565e-06\n",
            "fridge 5.082636168699845e-06\n",
            "custard 5.291769890764088e-06\n",
            "\n",
            "state 17\n",
            "weak 7.160407219122342e-07\n",
            "peach 1.438631656461391e-06\n",
            "co 3.370869994763418e-06\n",
            "keeping 4.859175626703775e-06\n",
            "and 4.885323139628456e-06\n",
            "my 6.326498681854539e-06\n",
            "air 6.6319430658831596e-06\n",
            "date 6.942279794832638e-06\n",
            "beverage 7.709164998143454e-06\n",
            "knew 7.921278864327601e-06\n",
            "\n",
            "state 18\n",
            "most 2.1110033872631817e-07\n",
            "premium 1.044591644465007e-06\n",
            "rating 2.4643437269903487e-06\n",
            "dishes 4.197753844535882e-06\n",
            "content 5.331940355134097e-06\n",
            "fat 5.456031179573646e-06\n",
            "awesome 6.398126460894438e-06\n",
            "difficult 6.677620055477007e-06\n",
            "readily 7.096518628680636e-06\n",
            "long 7.486290820799726e-06\n",
            "\n",
            "state 19\n",
            "includes 3.0594799147909716e-08\n",
            "filter 2.558532195005502e-06\n",
            "measure 3.833909945619657e-06\n",
            "information 3.928099877115793e-06\n",
            "pot 3.9326099074356255e-06\n",
            "roll 5.00169239063025e-06\n",
            "away 5.901130294648334e-06\n",
            "appeal 7.041161562525009e-06\n",
            "high 7.2912977244603384e-06\n",
            "sells 8.016579832015666e-06\n",
            "\n",
            "state 20\n",
            "moved 8.082885316926946e-07\n",
            "sugary 9.598269377262953e-07\n",
            "greatest 1.4814928998279877e-06\n",
            "folks 1.927987853728623e-06\n",
            "freeze 2.326902018447856e-06\n",
            "levels 2.649916103628517e-06\n",
            "next 2.7344396570750134e-06\n",
            "couple 4.06233403255064e-06\n",
            "allowed 4.311637439458263e-06\n",
            "research 4.95353781533152e-06\n",
            "\n",
            "state 21\n",
            "example 6.463540234536159e-08\n",
            "creamer 7.810325374565636e-08\n",
            "varieties 1.3116114309916865e-06\n",
            "lb 1.829251369964531e-06\n",
            "besides 1.8419822120851649e-06\n",
            "disposable 2.8794770694533094e-06\n",
            "star 3.1065765609108875e-06\n",
            "scratch 4.011912756980539e-06\n",
            "set 5.1678190598071924e-06\n",
            "decaffeinated 5.170816191443028e-06\n",
            "\n",
            "state 22\n",
            "none 1.7291905667078252e-07\n",
            "farms 8.197692569107871e-07\n",
            "inside 1.9293555064334482e-06\n",
            "other 1.9712679864141654e-06\n",
            "grounds 2.0565098193973367e-06\n",
            "the 2.1422973753671254e-06\n",
            "watchers 2.1511145728337993e-06\n",
            "dishes 2.536630856168776e-06\n",
            "general 2.5841844120628684e-06\n",
            "poor 3.851197562561944e-06\n",
            "\n",
            "state 23\n",
            "tend 4.699220036942361e-07\n",
            "huge 9.601894563073506e-07\n",
            "grains 2.782647061055805e-06\n",
            "foods 2.9769254090838867e-06\n",
            "uses 3.4281114183390334e-06\n",
            "plenty 3.568273553609063e-06\n",
            "club 4.193952093419087e-06\n",
            "missing 4.248887397783918e-06\n",
            "buy 5.235780922912939e-06\n",
            "measure 6.4031378248119045e-06\n",
            "\n",
            "state 24\n",
            "veggies 4.4306781034487937e-07\n",
            "serious 1.629749949584232e-06\n",
            "tangy 1.9897641900021607e-06\n",
            "line 2.1942731197509243e-06\n",
            "dessert 2.273743236710068e-06\n",
            "source 4.181815565601298e-06\n",
            "requires 4.30921707873274e-06\n",
            "stash 5.458120093803781e-06\n",
            "healthier 7.604141012603987e-06\n",
            "we're 8.619877284684171e-06\n",
            "\n",
            "state 25\n",
            "canned 1.436802514206927e-06\n",
            "night 1.5117824469543712e-06\n",
            "outstanding 2.106983759923837e-06\n",
            "individual 2.569208444794297e-06\n",
            "keeps 3.464036798739056e-06\n",
            "pack 4.379946213332211e-06\n",
            "beyond 4.650903966041338e-06\n",
            "basically 4.8129994143907246e-06\n",
            "double 5.555030873220781e-06\n",
            "numerous 5.950206175893431e-06\n",
            "\n",
            "state 26\n",
            "were 8.787309138117039e-09\n",
            "double 1.066051048476753e-06\n",
            "write 1.1013621994885243e-06\n",
            "irish 1.1207765226644926e-06\n",
            "satisfy 3.037156525344088e-06\n",
            "watery 4.0878336914844e-06\n",
            "the 4.356639026011477e-06\n",
            "going 4.9172814072811805e-06\n",
            "variety 5.225661601538015e-06\n",
            "problem 5.282885475741343e-06\n",
            "\n",
            "state 27\n",
            "finding 1.1018945430238658e-06\n",
            "dont 1.2362114278330808e-06\n",
            "person 1.6152114027920604e-06\n",
            "particularly 3.0670430706107597e-06\n",
            "candy 3.102392570847053e-06\n",
            "sour 3.1271250460631756e-06\n",
            "which 3.458066718921098e-06\n",
            "e 3.687926383449748e-06\n",
            "goodness 4.067884333783179e-06\n",
            "muffins 4.4583635632350606e-06\n",
            "\n",
            "state 28\n",
            "better 4.712455268291667e-08\n",
            "effects 1.1233770700182571e-06\n",
            "center 2.0723512132268704e-06\n",
            "muffin 2.528998722325759e-06\n",
            "vending 4.943148843029557e-06\n",
            "typical 5.7869743527995135e-06\n",
            "lobster 6.3886136256724416e-06\n",
            "nutrition 6.959350987181805e-06\n",
            "tasteless 7.421711758155565e-06\n",
            "ratio 7.434170696592494e-06\n",
            "\n",
            "state 29\n",
            "receive 2.541938465690301e-08\n",
            "unhealthy 3.664926327467188e-07\n",
            "easily 2.4874211079774224e-06\n",
            "digest 2.708708379203675e-06\n",
            "daily 3.591045088165075e-06\n",
            "vinegar 3.679369539589533e-06\n",
            "shaped 3.70061913037e-06\n",
            "lobster 3.871484168928659e-06\n",
            "feeding 4.330779424161406e-06\n",
            "pod 5.3830415735459296e-06\n",
            "\n",
            "state 30\n",
            "muffins 1.046109321644664e-06\n",
            "grain 1.3883415534576792e-06\n",
            "return 2.3824357644006377e-06\n",
            "introduced 2.726054221164004e-06\n",
            "heart 2.8309488211948598e-06\n",
            "glass 3.088517693342348e-06\n",
            "typically 3.933092182870207e-06\n",
            "junk 4.242826823084802e-06\n",
            "case 4.356525877024104e-06\n",
            "knew 4.467046088948346e-06\n",
            "\n",
            "state 31\n",
            "mini 4.916637776565528e-07\n",
            "pass 5.591516630911934e-07\n",
            "includes 8.742568324615477e-07\n",
            "like 8.901585882618579e-07\n",
            "actual 1.38186030098611e-06\n",
            "or 1.4800123520724996e-06\n",
            "comes 2.4548478548739435e-06\n",
            "across 2.7807272811424587e-06\n",
            "weather 3.478278895560289e-06\n",
            "followed 4.115248266374936e-06\n",
            "\n",
            "state 32\n",
            "salads 2.4963904636853105e-07\n",
            "mug 8.838822834089853e-07\n",
            "ones 9.465242253939016e-07\n",
            "keurig 1.8275881463550447e-06\n",
            "dead 1.8930654990878668e-06\n",
            "purchasing 2.7052752640574926e-06\n",
            "design 3.8547753184477395e-06\n",
            "a 4.375423482225508e-06\n",
            ". 4.848733211430529e-06\n",
            "shipping 5.394770474718879e-06\n",
            "\n",
            "state 33\n",
            "dollars 2.107525150065497e-07\n",
            "calories 3.5729244229092623e-07\n",
            "offer 3.693965613904094e-07\n",
            "straight 9.365139734678067e-07\n",
            "difference 1.6716780047244155e-06\n",
            "fake 1.7598216638274328e-06\n",
            "co 2.146728374040366e-06\n",
            "tolerate 2.3011736613700515e-06\n",
            "palatable 2.9861354001880063e-06\n",
            "chocolates 3.386236716058454e-06\n",
            "\n",
            "state 34\n",
            "labels 6.721632629071618e-07\n",
            "ship 7.420783699495862e-07\n",
            "delivered 7.682577433939785e-07\n",
            "happened 9.64297424698881e-07\n",
            "point 1.3608090509967208e-06\n",
            "purchasing 1.796817607969973e-06\n",
            "popchips 2.688103499729173e-06\n",
            "label 2.7992638285892623e-06\n",
            "jalapeno 3.0114028149350248e-06\n",
            "teaspoon 3.7419633093603533e-06\n",
            "\n",
            "state 35\n",
            "save 5.246962134248534e-07\n",
            "teeth 5.686866129964384e-07\n",
            "compared 9.407089476339202e-07\n",
            "months 1.0679702371847433e-06\n",
            "provide 1.6759167422669294e-06\n",
            "vegetable 2.1038944066341517e-06\n",
            "label 2.1814479438036275e-06\n",
            "fence 2.346239798612995e-06\n",
            "dirty 2.3816654297867893e-06\n",
            "fairly 2.7218481451585997e-06\n",
            "\n",
            "state 36\n",
            "idea 1.6175383028926707e-07\n",
            "junk 1.6950841272205286e-06\n",
            "! 1.8764766267465101e-06\n",
            "dogs 2.3409730279241187e-06\n",
            "vegetable 2.478039347179492e-06\n",
            "early 3.1271018800289018e-06\n",
            "sad 4.07894353280792e-06\n",
            "although 4.267705202056481e-06\n",
            "alot 4.295119517931229e-06\n",
            "sauces 4.380591769458062e-06\n",
            "\n",
            "state 37\n",
            "noted 6.928577033296584e-07\n",
            "wondering 8.496799030945214e-07\n",
            "she 8.962211570779405e-07\n",
            "leak 1.1125505008390162e-06\n",
            "grain 1.2074845214642549e-06\n",
            "carbonation 1.445206699116855e-06\n",
            "metallic 1.6381525135628477e-06\n",
            "milder 1.76276580108082e-06\n",
            "shipped 2.649527223958179e-06\n",
            "want 3.031344444649797e-06\n",
            "\n",
            "state 38\n",
            "stuff 3.854754287643994e-07\n",
            "center 1.5421380740996611e-06\n",
            "convenient 1.7187157066960266e-06\n",
            "at 1.9417127233294485e-06\n",
            "purchased 2.325047246542773e-06\n",
            "n 2.7676458787727195e-06\n",
            "sweetened 3.165036861838517e-06\n",
            "tell 3.263603800901301e-06\n",
            "varieties 3.5058653224122437e-06\n",
            "17 3.602734015686649e-06\n",
            "\n",
            "state 39\n",
            "enjoying 1.9775751479663367e-08\n",
            "natural 9.836186362115873e-08\n",
            "years 5.026994771349188e-07\n",
            "point 1.7240784827133642e-06\n",
            "realize 3.885168039992254e-06\n",
            "bother 4.8159500981894925e-06\n",
            "made 5.577845811449232e-06\n",
            "themselves 5.821130678843503e-06\n",
            "140 7.392160410761447e-06\n",
            "wrapped 7.5073134451230574e-06\n",
            "\n",
            "state 40\n",
            "minute 7.572579841594872e-08\n",
            "wondering 1.7576247845311725e-07\n",
            "huge 4.933745966870778e-07\n",
            "health 1.2027822162436768e-06\n",
            "pod 2.0603895900140547e-06\n",
            "prepared 2.175688572162694e-06\n",
            "helped 2.774321984189478e-06\n",
            "expensive 3.2943110589070055e-06\n",
            "happier 3.5203911782294864e-06\n",
            "share 3.9565540599347846e-06\n",
            "\n",
            "state 41\n",
            "plus 5.929237125172248e-07\n",
            "types 1.0863987602133513e-06\n",
            "sugary 1.2041159722328385e-06\n",
            "products 2.739058238717754e-06\n",
            "human 4.698396888100667e-06\n",
            "throwing 4.87319304624485e-06\n",
            "black 5.1646836456806805e-06\n",
            "incredible 5.286986927677979e-06\n",
            "cats 5.379276644496751e-06\n",
            "ketchup 6.206634996063428e-06\n",
            "\n",
            "state 42\n",
            "rose 4.006595725326739e-08\n",
            "ago 1.1431517337240785e-06\n",
            "tried 2.181622850335721e-06\n",
            "yogurt 2.8777756261253127e-06\n",
            "watch 3.1676912188178195e-06\n",
            "jar 3.8531816612584e-06\n",
            "addicted 4.351363395193405e-06\n",
            "chocolates 4.6074402373412114e-06\n",
            "world 5.393170472966835e-06\n",
            "nuts 5.854560863464962e-06\n",
            "\n",
            "state 43\n",
            "which 6.030873627588336e-09\n",
            "expect 2.2779893977049977e-07\n",
            "easier 6.512792344801152e-07\n",
            "french 7.477316419024658e-07\n",
            "c 1.2726092549538919e-06\n",
            "peanuts 2.024578119303195e-06\n",
            "satisfy 2.0424514366951764e-06\n",
            "after 2.6975175604681896e-06\n",
            "bottom 2.775536429825687e-06\n",
            "husband 2.8634331239221914e-06\n",
            "\n",
            "state 44\n",
            "crunch 3.175070882484557e-07\n",
            "140 5.79052613545549e-07\n",
            "he 7.533931006306913e-07\n",
            "satisfied 9.200866266622814e-07\n",
            "european 1.1704546588648656e-06\n",
            "hate 1.192457861132373e-06\n",
            "grains 1.848900026988147e-06\n",
            "locally 2.9838083248946737e-06\n",
            "occasionally 3.356554541980541e-06\n",
            "stronger 4.524020697231767e-06\n",
            "\n",
            "state 45\n",
            "aware 1.4089518869929515e-06\n",
            "chemicals 1.7256224097625386e-06\n",
            "restaurant 2.3840732689104844e-06\n",
            "live 2.6676568945167392e-06\n",
            "70 3.497424470577729e-06\n",
            "mixing 3.5573890046452857e-06\n",
            "were 4.0000900496527454e-06\n",
            "knew 4.770536733604613e-06\n",
            "pieces 4.907920863693911e-06\n",
            "another 5.7002927247758125e-06\n",
            "\n",
            "state 46\n",
            "w 3.963456424334888e-07\n",
            "glad 9.039086825128171e-07\n",
            "suggest 1.503618380453756e-06\n",
            "carbonation 2.4809184180581987e-06\n",
            "doubt 2.943098893286045e-06\n",
            "sugar 3.6473467726126344e-06\n",
            "iams 3.707404553956886e-06\n",
            "us 4.829111532969794e-06\n",
            "really 5.469346891375653e-06\n",
            "ask 5.559717366819302e-06\n",
            "\n",
            "state 47\n",
            "bottles 5.299144448977889e-08\n",
            "warm 2.7451789605560906e-06\n",
            "kitchen 4.072447016062335e-06\n",
            "someone 5.800276009623078e-06\n",
            "finally 5.977804774362651e-06\n",
            "baked 6.783125002275561e-06\n",
            "cheese 6.853455155674727e-06\n",
            "quality 6.92655768299011e-06\n",
            "fridge 6.944064686200667e-06\n",
            "nuts 7.071486203931033e-06\n",
            "\n",
            "state 48\n",
            "seem 1.6838689822868113e-06\n",
            "limited 3.428285486846061e-06\n",
            "look 4.240716392615045e-06\n",
            "drinks 4.390596191447644e-06\n",
            "expect 5.1718594148535745e-06\n",
            "beat 5.7055018814768925e-06\n",
            "trans 6.004367287322152e-06\n",
            "teeth 6.079115692299984e-06\n",
            "off 6.300222375239386e-06\n",
            "processed 6.803167124845468e-06\n",
            "\n",
            "state 49\n",
            "packaging 7.942718025513556e-07\n",
            "there's 8.306868126961918e-07\n",
            "weird 1.0260205770715442e-06\n",
            "hint 1.6693925944270343e-06\n",
            "found 1.8872835139419546e-06\n",
            "coconut 3.6766489812661824e-06\n",
            "pudding 4.048293308831904e-06\n",
            "threw 4.124367995489168e-06\n",
            "bbq 5.16918009427144e-06\n",
            "we 5.979505868331403e-06\n",
            "\n",
            "state 50\n",
            "sit 9.681569571339885e-08\n",
            "felidae 7.077373744616845e-07\n",
            "appreciate 7.359416711969565e-07\n",
            "spent 1.2989547933892589e-06\n",
            "yuck 1.9418997812315178e-06\n",
            "having 2.529741738032731e-06\n",
            "threw 3.613219353053287e-06\n",
            "customer 4.41058183721504e-06\n",
            "sad 6.228328195014424e-06\n",
            "original 6.818684533002962e-06\n",
            "\n",
            "state 51\n",
            "nor 2.2035740984490464e-07\n",
            "crisp 2.2132597431995893e-07\n",
            "oils 1.2498259421755253e-06\n",
            "damaged 2.0130301303433608e-06\n",
            "likes 2.1512228487862055e-06\n",
            "its 2.2241507708742517e-06\n",
            "thrown 3.2346066738939864e-06\n",
            "day 4.175459299480925e-06\n",
            "carbs 4.868339568006991e-06\n",
            "sold 5.99455421143483e-06\n",
            "\n",
            "state 52\n",
            "turkey 3.001880600801983e-07\n",
            "look 9.269555853765846e-07\n",
            "supermarket 1.2161133143015896e-06\n",
            "health 1.8410812658704576e-06\n",
            "tangy 2.1350032472936345e-06\n",
            "stale 3.314735609206376e-06\n",
            "pudding 3.8103650630829414e-06\n",
            "spent 4.3632005864972806e-06\n",
            "colored 4.420982284232549e-06\n",
            "show 5.2999968820252945e-06\n",
            "\n",
            "state 53\n",
            "box 6.673917368658524e-07\n",
            "watchers 8.101183282674364e-07\n",
            "o 1.1863797906809278e-06\n",
            "get 1.768780028710919e-06\n",
            "thick 2.364777660573561e-06\n",
            "cacao 2.386349240905157e-06\n",
            "href 2.4519806984342785e-06\n",
            "pack 2.926534574101113e-06\n",
            "problems 3.0267271580297662e-06\n",
            "traditional 3.1002304038357397e-06\n",
            "\n",
            "state 54\n",
            "sitting 7.09224541221626e-07\n",
            "weeks 1.101715803712676e-06\n",
            "mostly 1.6334911820637978e-06\n",
            "pods 1.9651267398427696e-06\n",
            "spot 2.073274347374324e-06\n",
            "stuck 4.242419414106195e-06\n",
            "hope 5.087702246889635e-06\n",
            "shame 5.190136077660509e-06\n",
            "chicken 5.476111658530542e-06\n",
            "quantity 5.630703801767701e-06\n",
            "\n",
            "state 55\n",
            "much 9.457676695215889e-08\n",
            "spring 8.792791623764712e-07\n",
            "satisfy 1.540222905051768e-06\n",
            "death 2.1930171667628796e-06\n",
            "nectar 2.388384679600559e-06\n",
            "trick 2.8060663207717177e-06\n",
            "tomatoes 3.4354815318547062e-06\n",
            "totally 4.05161583628993e-06\n",
            "grey 4.361667688647067e-06\n",
            "often 4.538911111824548e-06\n",
            "\n",
            "state 56\n",
            "clearly 5.853476514643566e-07\n",
            "internet 1.06901376500492e-06\n",
            "she 1.1887624984886932e-06\n",
            "year 2.3540288650725317e-06\n",
            "vet 2.6716810279867545e-06\n",
            "size 2.6761216284948584e-06\n",
            "sadly 3.917024982275153e-06\n",
            "grams 4.049634455371922e-06\n",
            "finished 6.569274850640313e-06\n",
            "lover 6.576533007366609e-06\n",
            "\n",
            "state 57\n",
            "then 4.252816438497818e-07\n",
            "cheese 1.1970168443702053e-06\n",
            "beer 2.5388383632052506e-06\n",
            "soups 2.650314371314049e-06\n",
            "soda 3.454683818184355e-06\n",
            "covered 3.477093634534951e-06\n",
            "room 3.695639965604625e-06\n",
            "priced 3.847275511116308e-06\n",
            "wheat 4.821488083257938e-06\n",
            "herbs 5.277914087255722e-06\n",
            "\n",
            "state 58\n",
            "apple 9.917233981647298e-07\n",
            "toast 1.1961306935677917e-06\n",
            "holds 1.8277772002504168e-06\n",
            "dop 2.044222925474827e-06\n",
            "95 2.1106156740218256e-06\n",
            "thought 2.43679142779602e-06\n",
            "worry 2.6167723680444127e-06\n",
            "drinking 2.9084244476176253e-06\n",
            "disease 3.326473983354042e-06\n",
            "bar 3.7891602662472067e-06\n",
            "\n",
            "state 59\n",
            "training 1.3156193962451561e-06\n",
            "bag 2.030362219109102e-06\n",
            "you 2.5112493337673567e-06\n",
            "allergy 2.849769464094328e-06\n",
            "come 3.262519830328917e-06\n",
            "stuff 4.569254827028515e-06\n",
            "chocolate 5.140991444258389e-06\n",
            "glass 5.235938704498312e-06\n",
            "pricey 5.237672082256391e-06\n",
            "13 5.302385859182962e-06\n",
            "\n",
            "state 60\n",
            "flavor 2.4577720851476914e-07\n",
            "piece 1.1914110341290113e-06\n",
            "companies 1.2663808091009045e-06\n",
            "seriously 1.3527028850243668e-06\n",
            "opened 1.4545986418749142e-06\n",
            "80 1.525859739364757e-06\n",
            "containers 1.8444727696190806e-06\n",
            "potassium 4.030614170904399e-06\n",
            "you 4.436847613615225e-06\n",
            "fill 5.627549378395747e-06\n",
            "\n",
            "state 61\n",
            "consistency 2.480949626448497e-07\n",
            "friend 1.7973166989511362e-06\n",
            "texture 2.9634236970441094e-06\n",
            "dishes 3.0709844558637817e-06\n",
            "page 3.10046929211355e-06\n",
            "mine 3.205233370142846e-06\n",
            "give 3.402865021332888e-06\n",
            "o 3.962638236149256e-06\n",
            "choices 4.198927670770386e-06\n",
            "eat 4.80989539444624e-06\n",
            "\n",
            "state 62\n",
            "air 8.14046494514783e-07\n",
            "any 1.1930681767980338e-06\n",
            "one 1.1943283782072128e-06\n",
            "superior 1.2036744372906832e-06\n",
            "goes 1.6736429333696593e-06\n",
            "daughter 1.9743901215721828e-06\n",
            "spinach 2.2742328735379255e-06\n",
            "treats 2.569880407747066e-06\n",
            "negative 4.545699904909512e-06\n",
            "working 5.4398027622351445e-06\n",
            "\n",
            "state 63\n",
            "done 5.716146422183296e-07\n",
            "chunks 8.419797029998483e-07\n",
            "chocolates 2.8657784842582333e-06\n",
            "family 3.0708200355333417e-06\n",
            "couldn't 3.5442406197461902e-06\n",
            "about 3.7471913095904514e-06\n",
            "worked 3.7532573135004573e-06\n",
            "chocolate 4.033849594719174e-06\n",
            "clearly 4.705388855459589e-06\n",
            "change 4.960440979280709e-06\n",
            "\n",
            "state 64\n",
            "hazelnut 1.1263002737052663e-06\n",
            "sell 1.1660273175079858e-06\n",
            "realize 1.9471772875839844e-06\n",
            "chip 1.978728292946954e-06\n",
            "crunchy 2.488624196951589e-06\n",
            "experienced 2.649431386734229e-06\n",
            "gave 2.797044983317046e-06\n",
            "boxes 3.01903659623408e-06\n",
            "well 3.300958550147106e-06\n",
            "while 4.2701998276980565e-06\n",
            "\n",
            "state 65\n",
            "content 6.856195711466992e-07\n",
            "figured 7.641372514702132e-07\n",
            "bag 1.7035461644371012e-06\n",
            "bother 1.7485123516008791e-06\n",
            "year 2.3907760503297827e-06\n",
            "such 2.559580001861598e-06\n",
            "mahogany 5.011152727946435e-06\n",
            "other 5.247726698256564e-06\n",
            "sells 6.243807910022842e-06\n",
            "contain 6.770375552403972e-06\n",
            "\n",
            "state 66\n",
            "mention 9.627952725235215e-08\n",
            "popchips 2.945345181625439e-07\n",
            "labels 8.488904025484922e-07\n",
            "individual 1.201361043133739e-06\n",
            "become 1.3868738663987072e-06\n",
            "t 1.7471097169368193e-06\n",
            "reviews 3.0344567067032828e-06\n",
            "donut 3.374776720932718e-06\n",
            "on 3.562674224152372e-06\n",
            "store 4.030470159039687e-06\n",
            "\n",
            "state 67\n",
            "18 2.1007314259659587e-07\n",
            "stated 4.4698302972768857e-07\n",
            "have 9.94376263629313e-07\n",
            "maker 1.1877682055075687e-06\n",
            "heart 1.8504058450324905e-06\n",
            "touch 1.8998024631156502e-06\n",
            "butter 1.9923855885385884e-06\n",
            "purchasing 2.35170576158124e-06\n",
            "per 2.8083812895068995e-06\n",
            "u 3.269719580680201e-06\n",
            "\n",
            "state 68\n",
            "hazelnut 1.2191766282899934e-07\n",
            "seasoning 6.723229272278464e-07\n",
            "blend 7.827209597733246e-07\n",
            "gluten 1.1133729220749142e-06\n",
            "mail 1.8251281204617634e-06\n",
            "vomiting 1.953188486515033e-06\n",
            "done 2.6978764817531827e-06\n",
            "bring 2.730707554378852e-06\n",
            "never 3.494871119387019e-06\n",
            "won't 4.370144927233682e-06\n",
            "\n",
            "state 69\n",
            "crunch 8.136808181656909e-07\n",
            "wet 1.541677310617737e-06\n",
            "research 2.623507291067445e-06\n",
            "points 3.0439185794822467e-06\n",
            "hard 3.880460019460302e-06\n",
            "mountain 4.378765170289132e-06\n",
            "overall 4.747200851247689e-06\n",
            "online 5.560192999573844e-06\n",
            "natural 5.90171440614911e-06\n",
            "yeah 6.440741423453147e-06\n",
            "\n",
            "state 70\n",
            "he's 7.318452663067498e-07\n",
            "readily 7.944735842361071e-07\n",
            "individually 1.8950181177883804e-06\n",
            "doing 1.9942525625455457e-06\n",
            "consistent 2.1501661946485415e-06\n",
            "need 2.246038006758605e-06\n",
            "burn 2.454303681770732e-06\n",
            "appears 2.624748366498045e-06\n",
            "traditional 2.6659618682066122e-06\n",
            "mccann's 3.267446078365101e-06\n",
            "\n",
            "state 71\n",
            "packs 1.3423608024030384e-07\n",
            "learned 1.3465196845582351e-06\n",
            "sealed 1.8461549803250152e-06\n",
            "pack 2.0168849264830115e-06\n",
            "pounds 2.8157024711351214e-06\n",
            "stars 3.6985385372687595e-06\n",
            "whatever 5.709132785730207e-06\n",
            "liver 7.427801658454094e-06\n",
            "enjoy 7.751930214966799e-06\n",
            "pops 8.088082445587207e-06\n",
            "\n",
            "state 72\n",
            "watered 8.627596871669492e-07\n",
            "worse 3.4818729660813046e-06\n",
            "open 4.194961534772101e-06\n",
            "honey 4.56595663546665e-06\n",
            "stock 5.620940439406442e-06\n",
            "within 5.633505125193517e-06\n",
            "juices 6.454183983089876e-06\n",
            "same 7.098882367652892e-06\n",
            "organic 7.6400554977428e-06\n",
            "grams 8.44854064336972e-06\n",
            "\n",
            "state 73\n",
            "milder 2.919440665547928e-07\n",
            "75 6.706731120573597e-07\n",
            "freeze 1.4623183841797685e-06\n",
            "palatable 1.573432150941074e-06\n",
            "certain 2.624964884862304e-06\n",
            "berry 3.3827547546940406e-06\n",
            "candy 4.023330696800679e-06\n",
            "warning 4.8634393614083654e-06\n",
            "before 4.9690086493585775e-06\n",
            "shape 5.32118793702263e-06\n",
            "\n",
            "state 74\n",
            "across 2.0562158010924858e-06\n",
            "pack 2.4540647465260826e-06\n",
            "pleasantly 2.813000615239761e-06\n",
            "34 2.973124089680239e-06\n",
            "spicy 3.5284075120173406e-06\n",
            "salad 3.817368228674267e-06\n",
            "leak 4.678054079890386e-06\n",
            "flavour 4.999459053499631e-06\n",
            "interesting 5.127906014433038e-06\n",
            "wonder 5.130508447830233e-06\n",
            "\n",
            "state 75\n",
            "damage 5.1463774501257195e-08\n",
            "fish 9.980499566901384e-08\n",
            "concentrates 1.671823996978027e-07\n",
            "love 5.307596531421255e-07\n",
            "mom 5.752024306714795e-07\n",
            "price 1.929303776243825e-06\n",
            "particular 4.57922947817705e-06\n",
            "carry 5.037366969166223e-06\n",
            "ate 5.21736552579627e-06\n",
            "getting 5.50388274076265e-06\n",
            "\n",
            "state 76\n",
            "cool 1.7286754596841986e-07\n",
            "real 1.2957837890329643e-06\n",
            "g 1.937493016604372e-06\n",
            "in 2.066004682503464e-06\n",
            "nothing 2.5168149555180296e-06\n",
            "dip 2.646415579979747e-06\n",
            "messy 2.723777769850312e-06\n",
            "thanks 2.8821562687684125e-06\n",
            "down 2.885892190032619e-06\n",
            "mango 2.9749927966728694e-06\n",
            "\n",
            "state 77\n",
            "mine 2.199044873834041e-06\n",
            "their 2.306194515127186e-06\n",
            "sell 2.357113954459435e-06\n",
            "prefer 2.4405088219944814e-06\n",
            "york 2.497351857445494e-06\n",
            "soup 2.5015167127273685e-06\n",
            "batches 3.1580711395568343e-06\n",
            "addicted 3.4966420475924143e-06\n",
            "dinner 4.2148077643011936e-06\n",
            "fantastic 4.381942213032314e-06\n",
            "\n",
            "state 78\n",
            "product 3.862177990994401e-07\n",
            "how 4.169980839019849e-07\n",
            "things 7.719816669028541e-07\n",
            "spend 8.596405709826961e-07\n",
            "happened 1.2413560381690539e-06\n",
            "sauce 1.2954812726441459e-06\n",
            "waste 1.6333293204291039e-06\n",
            "aware 2.227648516173803e-06\n",
            "being 2.5614021336355874e-06\n",
            "savory 2.649781822287807e-06\n",
            "\n",
            "state 79\n",
            "muffins 3.436996696616646e-08\n",
            "do 5.334550915340609e-07\n",
            "skin 8.895080024956691e-07\n",
            "going 1.4090112124035557e-06\n",
            "found 1.6176320742649338e-06\n",
            "personal 2.0491563093590903e-06\n",
            "basil 2.1898466014855694e-06\n",
            "sodium 2.8550890324157378e-06\n",
            "aftertaste 3.663715882631197e-06\n",
            "iced 4.1670073612674125e-06\n",
            "\n",
            "state 80\n",
            "i'll 4.6394720019371895e-07\n",
            "green 5.068172101217585e-07\n",
            "they 6.381546174579509e-07\n",
            "trans 9.64209880730547e-07\n",
            "home 1.8505423117965745e-06\n",
            "these 2.2997404235136437e-06\n",
            "favorites 2.650182618877315e-06\n",
            "onion 3.40576904562612e-06\n",
            "sounds 3.6404787167557037e-06\n",
            "previously 4.00810624747389e-06\n",
            "\n",
            "state 81\n",
            "literally 3.065005551631676e-07\n",
            "paste 1.0769539276288572e-06\n",
            "current 1.95926903524305e-06\n",
            "describe 2.0217069799572244e-06\n",
            "plenty 2.4792089881877524e-06\n",
            "container 2.537592225966447e-06\n",
            "these 2.5950755726260216e-06\n",
            "break 2.6578881987356297e-06\n",
            "supposed 2.8944057531808693e-06\n",
            "internet 3.4798978029044226e-06\n",
            "\n",
            "state 82\n",
            "hard 2.4605549966544054e-06\n",
            "eggs 2.5341438327795674e-06\n",
            "poured 2.7590576056847544e-06\n",
            "cents 2.868474639389996e-06\n",
            "reviews 5.0095953117811184e-06\n",
            "delicious 6.667523988611512e-06\n",
            "huge 7.426347695134455e-06\n",
            "fresh 7.818920015384324e-06\n",
            "etc 8.202850301842246e-06\n",
            "nose 8.283617081575205e-06\n",
            "\n",
            "state 83\n",
            "look 1.0973585371083333e-07\n",
            "email 2.300875462086119e-07\n",
            "smell 9.259727525329823e-07\n",
            "herbs 9.428950235742662e-07\n",
            "fell 9.726134221363566e-07\n",
            "grab 1.578746959454062e-06\n",
            "kitchen 1.9392575433941884e-06\n",
            "shape 2.009343403099381e-06\n",
            "ruin 3.056194173605024e-06\n",
            "herb 3.080448898276436e-06\n",
            "\n",
            "state 84\n",
            "facts 1.9069709319938025e-07\n",
            "complete 3.299762893957614e-07\n",
            "thank 7.003349735955141e-07\n",
            "shape 8.295334875201853e-07\n",
            "club 1.1544088050411607e-06\n",
            "grind 2.239494384704623e-06\n",
            "spices 2.460139572534184e-06\n",
            "often 3.446147535121e-06\n",
            "besides 3.924879116390068e-06\n",
            "saw 3.9956732091804725e-06\n",
            "\n",
            "state 85\n",
            "tight 4.591083893969629e-07\n",
            "pods 9.427506341770882e-07\n",
            "drinks 1.7361505676472807e-06\n",
            "noted 2.2892231790497072e-06\n",
            "somewhere 2.3505231729474923e-06\n",
            "wont 2.527071723545698e-06\n",
            "homemade 3.8592420633876455e-06\n",
            "fried 4.000611619465713e-06\n",
            "waste 4.290048392733637e-06\n",
            "across 6.175415063555305e-06\n",
            "\n",
            "state 86\n",
            "lime 3.2794651796620846e-07\n",
            "dairy 6.066540338648866e-07\n",
            "maple 2.737585519410249e-06\n",
            "zero 3.5689479241992302e-06\n",
            "drinker 3.7291708031056825e-06\n",
            "shape 4.542899141447933e-06\n",
            "incredible 5.888062901778173e-06\n",
            "happened 5.993123478983328e-06\n",
            "says 6.092956516236625e-06\n",
            "fed 6.637335994871755e-06\n",
            "\n",
            "state 87\n",
            "following 1.5196538343577813e-06\n",
            "actually 2.074677937308872e-06\n",
            "talk 2.520493682895696e-06\n",
            "microwave 2.9455480298380375e-06\n",
            "carried 3.157194920698773e-06\n",
            "pie 3.2024690083921263e-06\n",
            "sweet 3.7606842577641336e-06\n",
            "sour 3.845363813128499e-06\n",
            "manufacturer 3.872594438494222e-06\n",
            "sampler 4.6133612479585655e-06\n",
            "\n",
            "state 88\n",
            "larger 3.4131325950062564e-07\n",
            "flakes 1.620264288637355e-06\n",
            "wild 2.236263024611479e-06\n",
            "couldn't 4.549057191946067e-06\n",
            "smell 4.5922374752616e-06\n",
            "individual 5.4292768934091065e-06\n",
            "design 5.691311004752187e-06\n",
            "products 6.051883726647632e-06\n",
            "changed 7.269022204104379e-06\n",
            "seem 7.90838279723159e-06\n",
            "\n",
            "state 89\n",
            "lids 2.000567695709602e-07\n",
            "changed 1.047696019307959e-06\n",
            "dish 1.6582182596441937e-06\n",
            "learned 1.6633410048318909e-06\n",
            "number 2.5040578003288183e-06\n",
            "layer 2.6213144315607508e-06\n",
            "weak 2.6574947436969896e-06\n",
            "making 3.162676977051339e-06\n",
            "body 4.332298893464206e-06\n",
            "science 4.412932726016425e-06\n",
            "\n",
            "state 90\n",
            "splash 2.978772430670789e-07\n",
            "traditional 3.1127855478564626e-07\n",
            "classic 6.451989642741353e-07\n",
            "carbonated 9.710064294517712e-07\n",
            "enjoyed 1.305222787536065e-06\n",
            "dishes 1.717055211057943e-06\n",
            "right 2.3953076480365236e-06\n",
            "by 3.6942295544282183e-06\n",
            "needs 4.596896663315391e-06\n",
            "olive 4.75716251604075e-06\n",
            "\n",
            "state 91\n",
            "knew 2.6359643605214715e-07\n",
            "chemical 1.0167648777230446e-06\n",
            "gold 2.3400511287507965e-06\n",
            "enough 2.4168171085941577e-06\n",
            "already 2.737694970707059e-06\n",
            "foods 2.8561694663374993e-06\n",
            "glad 3.2365731133066685e-06\n",
            "larger 3.840744902344452e-06\n",
            "full 3.918348179018118e-06\n",
            "complete 4.463789758881768e-06\n",
            "\n",
            "state 92\n",
            "nutty 3.004647142307609e-07\n",
            "hair 3.2914746622655974e-07\n",
            "berry 4.3136574552772794e-07\n",
            "fell 8.47761653459685e-07\n",
            "heard 8.498017614231659e-07\n",
            "break 1.3594674092474242e-06\n",
            "packing 2.0715299834291234e-06\n",
            "absolutely 2.1001026453371837e-06\n",
            "vegetables 3.043982092011984e-06\n",
            "oven 3.4585312664833845e-06\n",
            "\n",
            "state 93\n",
            "close 1.1163551837826885e-06\n",
            "tend 1.3365892669429682e-06\n",
            "soft 2.4577745449318607e-06\n",
            "outside 2.755875954497382e-06\n",
            "satisfy 3.4257797960643917e-06\n",
            "any 3.6027498512968024e-06\n",
            "line 3.7972284716162303e-06\n",
            "training 4.169065137796794e-06\n",
            "can 4.652398898420968e-06\n",
            "smoother 5.740988146219136e-06\n",
            "\n",
            "state 94\n",
            "an 6.300434922546538e-08\n",
            "concentrates 1.185168577424843e-06\n",
            "site 1.7168889415783383e-06\n",
            "amount 2.8010724002103983e-06\n",
            "snack 3.215548604497204e-06\n",
            "occasionally 3.5575537840889407e-06\n",
            "machine 4.272687621942176e-06\n",
            "sweeter 4.319506228206946e-06\n",
            "longer 4.808987080951071e-06\n",
            "hydrogenated 5.15049945540524e-06\n",
            "\n",
            "state 95\n",
            "expensive 6.936853239123266e-07\n",
            "gf 7.499171401914815e-07\n",
            "wanting 1.2847369283202916e-06\n",
            "as 1.4380806380573576e-06\n",
            "flavored 1.6721002416672265e-06\n",
            "adding 3.11315286348008e-06\n",
            "late 4.296096427139482e-06\n",
            "seems 4.682545678426055e-06\n",
            "fair 5.41114917205388e-06\n",
            "gourmet 5.756027810804722e-06\n",
            "\n",
            "state 96\n",
            "poor 2.2005079153984806e-07\n",
            "overpriced 5.082212818974922e-07\n",
            "rose 1.113718138546756e-06\n",
            "table 1.4685776042616742e-06\n",
            "whenever 1.7533610817725753e-06\n",
            "empty 2.3448250520352105e-06\n",
            "dry 2.982398369483285e-06\n",
            "moist 3.1770935727235284e-06\n",
            "crumbs 3.2161708462975624e-06\n",
            "smelled 3.2314198389736464e-06\n",
            "\n",
            "state 97\n",
            "update 2.1737816497339733e-07\n",
            "veggies 5.029599338781965e-07\n",
            "calcium 2.780852396927963e-06\n",
            "vitamins 4.2054998768345364e-06\n",
            "berry 4.770426158483111e-06\n",
            "3 5.186261161544682e-06\n",
            "candies 5.31028646129784e-06\n",
            "mentioned 6.282677503119402e-06\n",
            "seed 6.316497246755025e-06\n",
            "yellow 6.4611529836973464e-06\n",
            "\n",
            "state 98\n",
            "even 1.3311029139489976e-07\n",
            "find 6.367788276309626e-07\n",
            "some 1.94060823341289e-06\n",
            "rip 2.228248186143854e-06\n",
            "farms 2.7871082969871186e-06\n",
            "locally 3.623355320466114e-06\n",
            "honey 3.921224545572426e-06\n",
            "longer 3.941333072031055e-06\n",
            "aren't 4.1249216281534224e-06\n",
            "chewy 4.135995515375329e-06\n",
            "\n",
            "state 99\n",
            "birthday 1.8522135821477697e-07\n",
            "through 2.696327816462244e-07\n",
            "nutritional 3.261975754317438e-07\n",
            "dry 4.2328018778892903e-07\n",
            "sauce 9.852070671048085e-07\n",
            "99 2.7431618552699334e-06\n",
            "i 3.9132109287879294e-06\n",
            "r 4.5920006016231115e-06\n",
            "coffees 5.1738637964740285e-06\n",
            "com 5.287338333573196e-06\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wp0tdoJBPIMd"
      },
      "source": [
        "# (c)\n",
        "\n",
        "hmm = HMM(num_states=10, num_words=tokenizer.vocab_size)\n",
        "hmm.learn_unsupervised(train_reviews_tk, 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8JZk5HilWXh"
      },
      "source": [
        "# Your code here!\n",
        "training_experiment(\"hmm\", hmm_featurizer, n_train=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9yuYlKglWaL",
        "outputId": "7f3f7950-d9f9-46b4-81b9-68319b94cccc"
      },
      "source": [
        "# Your code here!\n",
        "training_experiment(\"hmm\", hmm_featurizer, n_train=500)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hmm features, 500 examples\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:123: RuntimeWarning: divide by zero encountered in log\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "test accuracy 0.6\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovbrCIUT0NGy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43c81345-f621-4545-d17c-5337d9ec2029"
      },
      "source": [
        "# Your code here!\n",
        "training_experiment(\"hmm\", hmm_featurizer, n_train=1000)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hmm features, 1000 examples\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:123: RuntimeWarning: divide by zero encountered in log\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "test accuracy 0.602\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7sKuyZDL1kN",
        "outputId": "9d47c9f1-8af1-44f3-9872-0466bf496c38"
      },
      "source": [
        "# Your code here!\n",
        "training_experiment(\"hmm\", hmm_featurizer, n_train=2000)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hmm features, 2000 examples\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:123: RuntimeWarning: divide by zero encountered in log\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "test accuracy 0.63\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZizXaNPYr5n",
        "outputId": "e38046c9-a09a-4d7f-a7fc-8377bdb9bbd5"
      },
      "source": [
        "# Your code here!\n",
        "training_experiment(\"hmm\", hmm_featurizer, n_train=5000)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hmm features, 5000 examples\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:123: RuntimeWarning: divide by zero encountered in log\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "test accuracy 0.632\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjVkhJI1Yr_A"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHi7wRY2YsBy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}